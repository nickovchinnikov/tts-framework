{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TTS-Framework docs","text":"<p>This is the documentation for the TTS-Framework. Here you can find the technical description of the project and the API reference. Also, here you can find the code specs, the project structure and the development guidelines.</p>"},{"location":"#a-comprehensive-review-of-opensource-text-to-speech-tts-models","title":"A comprehensive review of opensource Text-to-Speech (TTS) Models","text":"<p>Basically here you can find everything you need to know about the project.</p> <p>You can read about this mkdocs-material here: mkdocs-material</p>"},{"location":"#wip-modified-version-of-delightfultts-and-univnet-codec","title":"[WIP] Modified version of DelightfulTTS and UnivNet codec.","text":""},{"location":"#references","title":"References","text":""},{"location":"#development-docs","title":"Development docs","text":"<p>Description of the training process. Docs, ideas and examples for the training process. </p>"},{"location":"#models","title":"Models","text":""},{"location":"#tts-models","title":"TTS Models","text":""},{"location":"#delightfultts","title":"DelightfulTTS","text":"<p>The DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021</p>"},{"location":"#vocoder","title":"Vocoder","text":""},{"location":"#univnet","title":"Univnet","text":"<p>UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</p> <p>Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch. </p>"},{"location":"#training","title":"Training","text":"<p>Training code documentation.</p>"},{"location":"readme/","title":"About mkdocs-material","text":""},{"location":"readme/#how-this-documentation-is-built","title":"How this documentation is built","text":"<p>The documetation is built with MkDocs and the Material for MkDocs theme.</p>"},{"location":"readme/#how-to-run-the-docs-locally","title":"How to run the docs locally:","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"readme/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"review/","title":"A comprehensive review of opensource Text-to-Speech (TTS) Models","text":""},{"location":"review/#tts-overview-little-theory-and-math","title":"TTS Overview, Little Theory and Math","text":""},{"location":"review/#training-text-to-speech-models-a-journey-from-zero-to","title":"Training Text-to-Speech Models: A Journey from Zero to...","text":"<p>Developing a high-quality text-to-speech (TTS) system is a complex task that requires extensive training of machine learning models. While successful TTS models can revolutionize how we interact with technology, enabling natural-sounding speech synthesis for various applications, the path to achieving such models is often paved with challenges and setbacks. Training a TTS model from scratch is a challenging process that involves numerous steps, from data preparation and preprocessing to model architecture selection, hyperparameter tuning, and iterative refinement. Even with state-of-the-art techniques and powerful computational resources, it's not uncommon for initial training attempts to fall short of expectations, yielding suboptimal results or encountering convergence issues. By carefully analyzing the shortcomings of unsuccessful models and identifying the root causes of their underperformance, researchers and practitioners can gain invaluable insights into the intricacies of TTS model training. These lessons can then inform and refine the training process, leading to more robust and high-performing models. In this article, we embark on a comprehensive journey, exploring the intricate world of TTS models.</p>"},{"location":"review/#how-it-all-started","title":"How it all started?","text":"<p>At Peech, we're dedicated to making Text to Speech accessible to everyone - individuals and publishers alike. Our innovative technology converts web articles, e-books, and any written content into engaging audiobooks. This is especially beneficial for individuals with dyslexia, ADHD, or vision impairments, as well as anyone who prefers listening to reading. The heart of our app lies in its text-to-speech technology. I've recently joined the impressive speech team with the ambition of advancing our capabilities in AI and Machine Learning. To achieve this, we've embarked on a research journey focused on training models in this domain. I'm excited to share with you the results I've achieved in our research and model training. We have done a good job, and I'd like to present the accomplishments to you.</p> <p>Audio is a very complicated data structure, just take a look for a 1 sec waveform...</p> Audio exhibits patterns at multiple time scales. Source: Google DeepMind."},{"location":"review/#one-to-many-mapping-problem-in-tts","title":"One-to-Many Mapping Problem in TTS","text":"<p>In TTS, the goal is to generate a speech waveform \\(y\\) from a given input text \\(x\\). This can be represented as a mapping function \\(f\\) such that:</p> \\[y = f(x)\\] <p>However, the mapping from text to speech is not unique, as there can be multiple valid speech outputs \\(y\\) for the same input text \\(x\\). This is because speech is a complex signal that encodes various types of information beyond just the phonetic content, such as pitch, duration, speaker characteristics, prosody, emotion, and more.</p> <p>Let's denote these additional factors as a set of variation parameters \\(v\\). Then, the mapping function \\(f\\) can be rewritten as:</p> \\[y = f(x, v)\\] <p>This means that for a given input text \\(x\\), the speech output \\(y\\) can vary depending on the values of the variation parameters \\(v\\).</p> <p>For example, consider the input text <code>x = \"Hello, how are you?\"</code>. Depending on the variation parameters \\(v\\), we can have different speech outputs:</p> <ul> <li> <p>If \\(v\\) represents pitch contour, we can have a speech output with a rising pitch at the end (questioning tone) or a falling pitch (statement tone).</p> </li> <li> <p>If \\(v\\) represents speaker identity, we can have speech outputs from different speakers, each with their unique voice characteristics.</p> </li> <li> <p>If \\(v\\) represents emotion, we can have speech outputs conveying different emotions, such as happiness, sadness, or anger. Mathematically, we can represent the variation parameters \\(v\\) as a vector of different factors:</p> </li> </ul> \\[v = [v_{\\text{pitch}}, v_{\\text{duration}}, v_{\\text{speaker}}, v_{\\text{prosody}}, v_{\\text{emotion}}, \\dots]\\] <p>The challenge in TTS is to model the mapping function \\(f\\) in such a way that it can generate appropriate speech outputs \\(y\\) for a given input text \\(x\\) and variation parameters \\(v\\). This is known as the one-to-many mapping problem, as there can be multiple valid speech outputs for the same input text, depending on the variation factors.</p> <p>There are two main approaches to modeling the specific aspects of the variation parameters \\(v\\):</p> <ol> <li> <p>Modeling Individual Factors: In this approach, each variation factor is modeled independently, without considering the interdependencies and interactions between different factors. For example, a model might focus solely on predicting pitch contours or duration values, treating them as separate components. While this approach can capture specific aspects of the variation parameters, it fails to model the variation information in a comprehensive and systematic way. The interdependencies between different factors, such as the relationship between pitch and prosody or the influence of speaker characteristics on duration, are not accounted for.</p> </li> <li> <p>Unified Frameworks for Modeling Multiple Factors: Recent research has proposed unified frameworks that aim to model multiple variation factors simultaneously, capturing their complementary nature and enabling more expressive and faithful speech synthesis. In this approach, the TTS model is designed to jointly model and generate multiple variation factors, considering their interdependencies and interactions. For instance, a unified framework might incorporate modules for predicting pitch, duration, and speaker characteristics simultaneously, while also accounting for their mutual influences. Mathematically, this can be represented as a mapping function \\(f\\) that takes the input text \\(x\\) and generates the speech output \\(y\\) by considering the combined effect of multiple variation parameters \\(v\\):</p> </li> </ol> \\[y=f(x,v_{\\text{pitch}},v_{\\text{duration}},v_{\\text{speaker}},v_{\\text{prosody}},v_{\\text{emotion}}, \\dots)\\] <p>By modeling the variation parameters in a unified and comprehensive manner, these frameworks aim to capture the complex relationships between different factors, enabling more expressive and faithful speech synthesis that better reflects the nuances and variations present in natural speech. The unified approach to modeling variation parameters in TTS systems has gained traction in recent years, as it addresses the limitations of modeling individual factors independently and enables the generation of more natural and expressive speech outputs.</p>"},{"location":"review/#two-sides-of-a-coin-ar-vs-nar","title":"Two sides of a coin: AR vs NAR","text":"<p>Any structured data can be transformed into a sequence. For instance, speech can be represented as a sequence of waveforms. Neural sequence generation methods can be used to generate such sequences, and there are two main categories of models for this purpose: autoregressive (AR) and non-autoregressive (non-AR) sequence generation methods. I believe, that any sequence can be generated using neural sequence generation methods (but not sure).</p>"},{"location":"review/#autoregressive-models","title":"Autoregressive models","text":"<p>Autoregressive (AR) sequence generation involves generating a sequence one token at a time in an autoregressive manner. In an AR model, the current value in the sequence is predicted based on the previous values and an error term (often referred to as white noise, which represents the unpredictable or random component).</p> <p>The definition from wikipedia: autoregressive model:</p> <p>In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation.</p> <p>The AR model's ability to capture sequential dependencies allows it to generate speech with natural-sounding variations, for example - making it expressive. This is because the model can learn to predict the next value in the sequence based on the context provided by the previous values.</p> <p>Autoregressive model of order \\(n\\), denoted as \\(\\text{AR}(n)\\), can be defined as:</p> \\[x_t = b + \\varphi_1x_{t-1} + \\varphi_2x_{t-2} + \\dots + \\varphi_px_{t-n} + \\epsilon_t\\] <p>Using sum notation this can be written as:</p> \\[x_t = b + \\sum_{i=1}^n \\varphi_i x_{t-i} + \\varepsilon_t\\] <p>where \\(b\\) is the bias term, \\(x_t\\) is the current value, \\(x_{t-1}, x_{t-2}, \\ldots, x_{t-n}\\) or \\(x_{t-i}\\) are the \\(n\\) previous values, \\(\\varphi_1, \\varphi_2, \\dots , \\varphi_n\\) are the model parameters and \\(\\varepsilon_t\\) is the error term or the white noise.</p> <p>During training, the model learns the optimal parameters \\(\\varphi_1, \\varphi_2, \\ldots, \\varphi_n\\) by minimizing a loss function, such as the Mean Squared Error (MSE):</p> \\[L(\\varphi) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x_i})^2\\] <p>where \\(x_i\\) is the true value, \\(\\hat{x_i}\\) is the predicted value, and \\(n\\) is the total number of time steps.</p>"},{"location":"review/#probabilistic-perspective","title":"Probabilistic Perspective","text":"<p>To define AR Sequence Generation in a probabilistic manner, we can use the chain rule to decompose the joint probability of the sequence into a product of conditional probabilities.</p> <p>AR sequence generation can be formulated using the chain rule of probability. Let \\(x = (x_1, x_2, \\ldots, x_n)\\) be the true sequence of length \\(n\\).</p> \\[P(x) = P(x_1, x_2, ..., x_n)\\] <p>The joint probability of the sequence can be decomposed into a product of conditional probabilities:</p> \\[P(x) = P(x_1, x_2, ..., x_n) = P(x_1)P(x_2 | x_1)P(x_3 | x_1, x_2) \\dots P(x_n | x_1, x_2 \\dots, x_{n-1})\\] <p>Using prod notation this can be written as:</p> \\[P(x) = \\prod_{i=1}^nP(x_i|x_1,x_2,\\dots,x_{i-1})\\] <p>We can simplify the result to:</p> \\[P(x) = \\prod_{i=1}^nP(x_i|\\mathbf{x}_{&lt;i})\\]"},{"location":"review/#inference","title":"Inference","text":"<p>During inference, the model generates the sequence one token at a time, using the previously predicted tokens as input:</p> \\[\\hat{x}_t = f(\\hat{x}_{t-1}, \\dots, \\hat{x}_{t-n})\\] <p>where \\(f\\) is the AR model, and \\(\\hat{x}_{t-1}, \\hat{x}_{t-2}, \\ldots, \\hat{x}_{t-n}\\) are the previously predicted tokens. </p> <p>Using sum notation and AR model definition this can be written as:</p> \\[\\hat{x}_t = b + \\sum_{i=1}^n \\varphi_i \\hat{x}_{t-i}\\]"},{"location":"review/#ar-model-schema","title":"AR model schema","text":"<p>Note: oversimplified schema, shouldn't be considered seriously. SOS - start of the sequence, EOS - end of the sequence.</p> <p></p>"},{"location":"review/#the-best-example-of-the-ar-world-transformer-llm","title":"The best example of the AR world: Transformer &amp; LLM","text":"<p>Transformers, which are the backbone of many state-of-the-art natural language processing (NLP) and speech models, including Large Language Models (LLMs), use a mechanism called self-attention to model relationships between all pairs of tokens in a sequence. Self-attention allows the model to capture long-range dependencies and weigh the importance of each token relative to others, enabling it to focus on the most relevant tokens when generating the next token. The attention mechanism in Transformers can be thought of as a weighted sum of the input tokens, where the weights represent the importance or relevance of each token for the current prediction. This allows the model to selectively focus on the most relevant tokens when generating the output.</p> <p>Mathematically, the attention mechanism can be represented as \\(QKV\\) attention:</p> \\[\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\] <p>where:</p> <ul> <li>\\(Q\\) (Query) represents the current token or context for which we want to find relevant information.</li> <li>\\(K\\) (Key) and \\(V\\) (Value) represent the input tokens and their corresponding values or representations.</li> <li>\\(d_k\\) is the dimensionality of the key vectors, and the scaling factor \\(\\sqrt{d_k}\\) is used for numerical stability.</li> <li>The softmax function is applied to the scaled dot-product of the query and key vectors, resulting in a set of weights that sum to 1. These weights determine the relative importance of each input token for the current prediction.</li> <li>The weighted sum of the value vectors, \\(V\\), is then computed using the attention weights, producing the final output of the attention mechanism.</li> </ul>"},{"location":"review/#schema","title":"Schema:","text":"<p>The self-attention mechanism in Transformers allows the model to capture dependencies between tokens in a sequence, regardless of their position or distance. This has made Transformers highly effective for various natural language processing tasks, including language modeling, machine translation, and text generation.</p>"},{"location":"review/#cons-of-autoregressive-ar-models","title":"Cons of Autoregressive (AR) models","text":"<p>While the autoregressive nature of AR models allows them to capture sequential dependencies and generate expressive sequences, it also introduces a significant drawback: slow inference time, especially for long sequences.</p>"},{"location":"review/#time-complexity-analysis","title":"Time Complexity Analysis","text":"<p>The time complexity of an algorithm or model refers to how the computational time required for execution scales with the input size. In the case of AR models, the input size is the sequence length, denoted as \\(n\\). During inference, AR models generate sequences one token at a time, using the previously generated tokens to predict the next token. This sequential generation process means that the model needs to process each token in the sequence, resulting in a time complexity of \\(O(n)\\). In other words, the inference time grows linearly with the sequence length. This linear time complexity can become a significant bottleneck in real-time scenarios or when dealing with long sequences, such as generating long texts, speech utterances, or high-resolution images. Even with modern hardware and parallelization techniques, the sequential nature of AR models can make them computationally expensive and slow for certain applications. The slow inference time of AR models can be particularly problematic in the following scenarios:</p> <ol> <li> <p>Real-time speech synthesis: In applications like virtual assistants or text-to-speech systems, the ability to generate speech in real-time is crucial for a seamless user experience. AR models may struggle to keep up with the required speed, leading to noticeable delays or latency.</p> </li> <li> <p>Long-form text generation: When generating long texts, such as articles, stories, or reports, the inference time of AR models can become prohibitively slow, making them impractical for certain use cases. While AR models excel at capturing sequential dependencies and generating expressive sequences, their slow inference time can be a significant limitation in certain applications, especially those requiring real-time or low-latency performance or involving long sequences.</p> </li> </ol> <p>One of the inherent challenges in autoregressive (AR) models is the unavoidable loss of quality for longer sequences. This issue is visually represented in the following plot:</p> <p></p> <p>As you can see, the dispersion or spread of the predicted values increases as the sequence length grows. This phenomenon occurs by design in AR models, and it's crucial to understand the underlying reasons behind it.</p>"},{"location":"review/#harmful-biases-in-autoregressive-ar-generation","title":"Harmful Biases in Autoregressive (AR) Generation","text":"<p>Autoregressive (AR) models are prone to several harmful biases that can negatively impact their performance and the quality of the generated sequences. These biases can arise from various factors, including the training process, the model architecture, and the nature of the data itself. Let's explore the reasons behind these, why do we have such kind of troubles?</p>"},{"location":"review/#error-propagation","title":"Error Propagation","text":"<p>Error propagation occurs when errors in the predicted tokens accumulate and affect the subsequent tokens, leading to a compounding effect, where the errors in the early tokens propagate to the later tokens. This can cause the generated sequence to deviate significantly from the true sequence, especially for longer sequences.</p> <p>For example, the model predicts a sequence of length \\(n\\), denoted as</p> \\[\\mathbf{\\hat{x}_n} = (\\hat{x}_{t-1}, \\hat{x}_{t-2}, \\dots, \\hat{x}_{t-n})\\] <p>Each predicted token \\(\\hat{x}_t\\) is computed based on the previous predicted tokens:</p> \\[\\hat{x}_t = \\varphi_1\\hat{x}_{t-1} + \\varphi_2\\hat{x}_{t-2} + \\dots + \\varphi_p\\hat{x}_{t-n} + b = \\sum_{i=1}^n \\varphi_i\\hat{x}_{t-i} + b\\] <p>Suppose the model predicts a token \\(\\hat{x}_{t-i}\\) with an error \\(\\epsilon_{t-i}\\).  The next token will be predicted based on the erroneous token \\(\\hat{x}_{t-i}\\), leading to a new error. This process continues, and the errors accumulate, leading to a significant deviation from the true sequence.</p> <p>We can define this error propagation as:</p> \\[\\hat{x}_t = (\\varphi_1\\hat{x}_{t-1} + \\epsilon_{t-1}) + (\\varphi_2\\hat{x}_{t-2} + \\epsilon_{t-2}) + \\dots + (\\varphi_p\\hat{x}_{t-n} + \\epsilon_{t-n}) + b\\] <p>Adding the growing error as a separated sum helps to visualize the harmful impact of the error propagation term:</p> \\[\\hat{x}_t = \\sum_{i=1}^n \\varphi_i\\hat{x}_{t-i} + \\sum_{i=1}^n \\epsilon_{t-i} + b\\] <p>The second term, \\(\\sum_{i=1}^n \\epsilon_{t-i}\\), represents the accumulated error propagation term, which grows as the sequence length increases.</p> <p>Error-propagation term: </p> \\[\\text{err} = \\sum_{i=1}^n \\epsilon_{t-i}\\] <p>The worst-case scenario occurs when the error happens on the first token in the sequence, \\(\\hat{x}_1\\). In this case, the error propagates and accumulates through the entire sequence, leading to the maximum deviation from the true sequence.</p> <p>In the context of text-to-speech synthesis, error propagation can lead to the generation of random noise or artifacts in the synthesized audio, making it difficult to avoid and potentially resulting in an unnatural or distorted output.</p>"},{"location":"review/#label-bias","title":"Label Bias","text":"<p>Label bias occurs when the normalization constraint over the vocabulary items at each decoding step in AR models leads to learning miscalibrated distributions over tokens and sequences. This can happen because the model is forced to assign probabilities that sum to 1 over the entire vocabulary, even if some tokens or sequences are highly unlikely or impossible in the given context.</p> <p>Mathematically, the probability distribution over the vocabulary items at time step \\(t\\) is typically computed using a softmax function:</p> \\[p(x_t | x_{t-1}, \\dots, x_{t-n}) = \\text{softmax}(W x_{t-1} + \\dots + W x_{t-n} + b)\\] <p>where \\(p(x_t | x_{t-1}, ..., x_{t-n})\\) is the probability distribution over the vocabulary items, \\(W\\) is the weight matrix, and \\(b\\) is the bias term.</p> <p>This normalization constraint can lead to a bias towards certain tokens or sequences, even if they are unlikely or irrelevant in the given context. For example, consider a language model trained on a corpus of news articles. If the model encounters a rare or unseen word during inference, it may still assign a non-zero probability to that word due to the normalization constraint, even though the word is highly unlikely in the context of news articles.</p> <p>The consequences of label bias can be significant, especially in cases where the data distribution is highly skewed or contains rare or unseen events. It can lead to the generation of nonsensical or irrelevant tokens, which can degrade the overall quality and coherence of the generated sequences. This bias can be particularly problematic in applications such as machine translation, text summarization, or dialogue systems, where accurate and context-appropriate generation is crucial.</p>"},{"location":"review/#order-bias","title":"Order Bias","text":"<p>Order bias occurs when the left-to-right generation order imposed by AR models is not the most natural or optimal order for the given task or data. In some cases, the data may prefer a different generation order or require considering the entire context simultaneously, rather than generating tokens sequentially.</p> <p>For example, in text-to-speech synthesis, the natural flow of speech may not follow a strict left-to-right order. The pitch, intonation, or emphasis of a sentence can be determined by the context of the entire sentence, rather than the individual words. If the model is trained to generate the audio sequence in a left-to-right order, it may fail to capture these nuances, leading to an unnatural or strange-sounding output.</p>"},{"location":"review/#non-autoregressive-models-nar","title":"Non-autoregressive models (NAR)","text":"<p>Non-autoregressive (NAR) sequence generation is an alternative approach to autoregressive (AR) models, where the entire output sequence is generated in parallel, without relying on previously generated tokens. Unlike AR models, which generate the sequence one token at a time in an autoregressive manner, NAR models do not have any sequential dependencies during the generation process. The key advantage of NAR models is their computational efficiency and speed. By generating the entire sequence in a single pass, NAR models can significantly reduce the inference time compared to AR models, which can be particularly beneficial for applications that require real-time or low-latency performance.</p>"},{"location":"review/#time-complexity-analysis_1","title":"Time Complexity Analysis","text":"<p>The time complexity of NAR models is constant, denoted as \\(O(1)\\), as the generation of the entire sequence is performed in a single pass, regardless of the sequence length. This is in contrast to AR models, which have a linear time complexity or \\(O(n)\\).</p>"},{"location":"review/#definition","title":"Definition","text":"<p>Here is how we can define the non-autoregressive model:</p> \\[x_t = f(x, t)\\] <p>where \\(f\\) is the NAR model, \\(x\\) is the input sequence, and \\(t\\) is the time step.</p> <p>In a non-autoregressive model, the output sequence is generated in a single pass, without any sequential dependencies. This can be represented mathematically as:</p> \\[\\hat{x} = f(x)\\] <p>where \\(x\\) is the input sequence, \\(f\\) is the NAR model and \\(\\hat{x}\\) is the output sequence.</p> <p>To define NAR Sequence Generation in a probabilistic manner, we can use the product rule to decompose the joint probability of the sequence into a product of independent probabilities.</p> <p>NAR Sequence Generation as a Probabilistic Model: Let's denote the sequence of tokens as \\(x = (x_1, x_2, \\dots, x_n)\\), where \\(n\\) is the sequence length. We can model the joint probability of the sequence using the product rule:</p> \\[P(x) = P(x_1)P(x_2) \\dots P(x_n) = \\prod_{i=1}^nP(x_i)\\] <p>Note that in a non-autoregressive model, the probability of each token is independent of the previous tokens, in contrast to AR models, which model the conditional probabilities using the chain rule.</p>"},{"location":"review/#nar-model-schema","title":"NAR model schema","text":"<p>Note: over-oversimplified schema, shouldn't be considered seriously at all. Shows that every token is independent.</p> <p></p>"},{"location":"review/#cons-of-nar-models","title":"Cons of NAR models","text":"<p>NAR models generate each token independently, without considering the context of the previous tokens, models do not capture sequential dependencies between tokens, which can result in generated sequences that do not follow the natural flow of language. The independence assumption in the output space ignores the real dependency between target tokens. NAR models are not well-suited to capture hierarchical structures in language, such as syntax and semantics.</p> <p>Based on the nature NAR models may generate repetitive phonemes or sounds, leading to a lack of naturalness and coherence in the synthesized speech, models may generate shorter or broken utterances, failing to capture the full meaning or context of the input text. NAR models may generate speech that sounds unnatural or robotic, indicating a lack of understanding of the acoustic properties of speech, and also may struggle to capture long-range dependencies between tokens, leading to a lack of coherence and fluency in the generated sequence.</p>"},{"location":"review/#text-to-speech-non-autoregressive-models","title":"Text-to-Speech Non-Autoregressive models","text":"<p>Despite their limitations, Non-Autoregressive (NAR) models are still widely used in Text-to-Speech (TTS) systems. One of the main advantages of NAR models is their ability to generate speech much faster than Autoregressive (AR) models, thanks to their reduced computational requirements. This makes them well-suited for real-time TTS applications where speed is crucial. While NAR models may not produce speech that is as natural-sounding as AR models, they can still generate high-quality speech that is sufficient for many use cases.</p>"},{"location":"review/#side-by-side-ar-nar","title":"Side-by-side AR / NAR","text":"<p>To enhance the quality of the synthesized audio in non-autoregressive (NAR) text-to-speech models, researchers have explored the idea of incorporating additional modules or blocks to predict various speech parameters. By explicitly modeling speech parameters, such as pitch, energy, and other prosodic features, the models can generate more expressive and natural-sounding speech.</p> <p>For instance, in the FastPitch model, a dedicated pitch predictor module is introduced to generate pitch contours in parallel, which are then used to condition the mel-spectrogram generation.</p>"},{"location":"review/#schema-of-fastspeech","title":"Schema of FastSpeech:","text":"<p>While introducing additional modules and conditioning the generation process on various speech parameters can improve the quality of the synthesized audio, it is important to strike a balance between model complexity and computational efficiency. The non-autoregressive nature of these models is designed to achieve faster inference speeds compared to autoregressive models, and adding too many complex modules or dependencies could potentially negate this advantage.</p>"},{"location":"review/#mostly-adequate-tts-models-review","title":"Mostly adequate TTS models review","text":"<p>In the following sections, I will review the most interesting implementations and key ideas in the field of non-autoregressive (NAR) text-to-speech models. It's important to note that not all models have published research papers or open-source code available. However, I will endeavor to uncover and present the relevant information and insights from the available sources.</p>"},{"location":"review/#ar-and-hybrid-models","title":"AR and hybrid models","text":""},{"location":"review/#sunoai-bark","title":"SunoAI Bark","text":"<p>My research has begun with the discovery of an incredible open-source solution that can produce incredible results: SunoAI Bark</p> <p>Bark is obviously AR model, but how it works? I found this great repo: audio-webui and author tried to found out how bark works, but:</p> <p>The author is researching voice cloning using the SunoAI Bark model and identified three different models used in the generation process: Semantics, Coarse, and Fine. The author notes that the current method of generating semantic tokens is problematic, as it generates tokens based on the input text rather than the actual audio.</p> <p>The author proposes three potential way for creation of speaker files for voice cloning:</p> <ol> <li> <p>Find out how Bark generates semantic tokens (may not be publicly available).</p> </li> <li> <p>Train a neural network to convert audio files to semantic tokens (time-consuming, but effective).</p> </li> <li> <p>Use a speech converter to change the input audio without editing semantics (not perfect, but better than current methods).</p> </li> </ol> <p>The author has implemented Method 3 using coqui-ai/TTS and achieved decent voice cloning results, but with limitations. </p> <p>Results: Decent voice cloning, not near perfect though. Better than previous methods, but struggles with some voices and accents. These issues lie in the transfer step.</p> <p>They also explored Method 1, but were unable to find a suitable model. </p> <p>Pre: It looks like bark uses AudioLM for the semantic tokens! I'm not sure if they use a different model though. I'll have to test that. But if they don't use a pre-trained model. I can do step 2. Results: No, it doesn't look like i can find a model, i did succeed in creating same-size vector embeddings, but the vectors use the wrong tokens.</p> <p>Method 2, which involves creating a quantizer based on data, has shown promising results, achieving convincing voice cloning with some limitations.</p> <p>Pre: What if instead of training a whole model, i only create a quantizer based on a bunch of data? Results: Successful voice cloning, can be very convincing. Still some limitations. But better than anything I've seen done with bark before.</p> <p>I can't tell you more, please, share any details if you find better explanations of the Bark. AR nature of the model is obvious, that's for sure. I faced with the limitations from the first touch - model can't generate audio more than 13-14 sec length. Several issues about this problem are here:</p> <ul> <li> <p>Limited to 13 seconds?</p> </li> <li> <p>Arbitrarily long text</p> </li> </ul> <p>Comment about it from issue #36 Inquiry on Maximum input character text prompt Length:</p> <p>right now the output is limited by the context window of the model (1024) which equates to about 14s. So text should be around that duration (meaning ~2-3 sentences). For longer texts you can either do them one at a time (and use the same history prompt to continue the same voice) or feed the first generation as the history for the second. i know that that's still a bit inconvenient. will try to add better support for that in the next couple of days</p> <p>You can implement the following approach, split sentences and synthesize the audio chunks:</p> <pre><code>from bark import generate_audio,preload_models\nfrom scipy.io.wavfile import write as write_wav\nimport numpy as np\nimport nltk\n\nnltk.download('punkt')\npreload_models()\n\nlong_string = \"\"\"\nBark is a transformer-based text-to-audio model created by [Suno](https://suno.ai/). Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference.\n\"\"\"\n\nsentences = nltk.sent_tokenize(long_string)\n\n# Set up sample rate\nSAMPLE_RATE = 22050\nHISTORY_PROMPT = \"en_speaker_6\"\n\nchunks = ['']\ntoken_counter = 0\n\nfor sentence in sentences:\n    current_tokens = len(nltk.Text(sentence))\n    if token_counter + current_tokens &lt;= 250:\n        token_counter = token_counter + current_tokens\n        chunks[-1] = chunks[-1] + \" \" + sentence\n    else:\n        chunks.append(sentence)\n        token_counter = current_tokens\n\n# Generate audio for each prompt\naudio_arrays = []\nfor prompt in chunks:\n    audio_array = generate_audio(prompt,history_prompt=HISTORY_PROMPT)\n    audio_arrays.append(audio_array)\n\n# Combine the audio files\ncombined_audio = np.concatenate(audio_arrays)\n\n# Write the combined audio to a file\nwrite_wav(\"combined_audio.wav\", SAMPLE_RATE, combined_audio)\n</code></pre> <p>Also you can use batching: <pre><code>from IPython.display import Audio\n\nfrom transformers import AutoProcessor\nfrom transformers import BarkModel\n\nmodel = BarkModel.from_pretrained(\"suno/bark-small\")\nprocessor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n\ntext_prompt = [\n\u00a0 \u00a0 \"Let's try generating speech, with Bark, a text-to-speech model\",\n\u00a0 \u00a0 \"Wow, batching is so great!\",\n\u00a0 \u00a0 \"I love Hugging Face, it's so cool.\"\n]\n\ninputs = processor(text_prompt).to(device)\n\nwith torch.inference_mode():\n\u00a0 # samples are generated all at once\n\u00a0 speech_output = model.generate(**inputs, do_sample = True, fine_temperature = 0.4, coarse_temperature = 0.8)\n\n# let's listen to the output samples!\n\nAudio(speech_output[2].cpu().numpy(), rate=sampling_rate)\n</code></pre></p> <p>But the butch generation creates a new problem. The quality of the audio is dramatically reduced, and it generates strange noises, much more often than in the case of generation in order. I found a great description in the comment: output quality seems worse if you pack it like that One more problem that I found is empty sounds if your text chunks are different sizes. Sometimes it's a random noise like background voices etc. So, you need to split the text carefully, it's very important. Bark occasionally hallucinates, which can lead to unexpected and inaccurate results. Unfortunately, it is impossible to detect this issue during the generation process, making it challenging to identify and correct. Furthermore, even during the review process, hallucinations can be difficult to spot, as they may be subtle or blend in with the rest of the generated content. This can result in incorrect or misleading information being presented as factual, which can have serious consequences in certain applications.</p>"},{"location":"review/#note","title":"\ud83d\udcd3 Note","text":"<p>I'm still unsure how bark works. I am trying to figure out how it works though. My current knowledge goes here.</p>"},{"location":"review/#sunoai-summary","title":"SunoAI summary","text":"<p>From what I understand, the issue of long context is not a priority for the Bark team and is largely being overlooked in the issues discussion. The only solution they've proposed is to use batches, but this approach has its limitations and doesn't work well. Moreover, due to the text context, it can't be processed accurately. Specifically, a small context of 2-3 sentences tends to disregard what happened in the previous passage, which can significantly impact the intonation and tempo of a speaker. Bark occasionally hallucinates, which can lead to unexpected and inaccurate results.</p>"},{"location":"review/#vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers","title":"VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers","text":"<p>Samples on the demo page.</p> <p>VALL-E is a novel language model approach for text-to-speech (TTS) synthesis that leverages large-scale, diverse, and multi-speaker speech data. Unlike traditional cascaded TTS systems, VALL-E treats TTS as a conditional language modeling task, using discrete audio codec codes as intermediate representations. This approach enables VALL-E to synthesize high-quality, personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.</p> <p>Key Components:</p> <ol> <li> <p>Neural Codec Model: VALL-E uses a pre-trained neural audio codec model, EnCodec, to convert audio waveforms into discrete acoustic codes. This model is trained on a large dataset of speech and can reconstruct high-quality waveforms even for unseen speakers.</p> </li> <li> <p>Language Model: VALL-E is trained as a language model, using the discrete acoustic codes as input and output. This allows the model to learn the patterns and structures of speech, enabling it to generate coherent and natural-sounding speech.</p> </li> <li> <p>In-Context Learning: VALL-E has strong in-context learning capabilities, similar to GPT-3, which enables it to adapt to new speakers and acoustic environments with minimal additional training data.</p> </li> </ol>"},{"location":"review/#the-problem-formulation-is-valid-for-bark-as-well-except-dimensions","title":"The Problem Formulation is valid for Bark as well (except dimensions):","text":"<p>Problem Formulation: Regarding TTS as Conditional Codec Language Modeling</p> <p>Given a dataset \\(D = \\{x_i, y_i\\}\\), where \\(y\\) is an audio sample and \\(x = \\{x_0, x_1, \\dots, x_L\\}\\) is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as \\(\\text{Encodec}(y) = C^{T\\times8}\\), where \\(C\\) represents the two-dimensional acoustic code matrix, and \\(T\\) is the downsampled utterance length. The row vector of each acoustic code matrix \\(c_{t,:}\\) represents the eight codes for frame \\(t\\) and the column vector of each acoustic code matrix \\(c_{:,j}\\) represents the code sequence from the \\(j\\)-th codebook, where \\(j \\in \\{1, \\dots, 8\\}\\). After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as \\(\\text{Decodec}(C) \\approx \\hat{y}\\).</p> <p></p> <p>Based on the paper, it appears that the proposed model combines the principles of Autoregressive (AR) and Non-Linear Autoregressive (NAR) models:</p> <p>The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed. On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction. On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from \\(O(T)\\) to \\(O(1)\\). Overall, the prediction of \\(C\\) can be modeled as: \\(p(C|x, \\tilde{C}; \\theta) = p(c_{:,1}| \\tilde{C}_{:,1}, X; \\theta_{\\text{AR}}) \\prod_{x=2}^8 p(c_{:,j}|c_{:,&lt;j}, x, \\tilde{C}; \\theta_{\\text{NAR}})\\)</p>"},{"location":"review/#vall-e-implementations","title":"VALL-E implementations","text":"<p>Note that there is currently no official implementation of VALL-E available. Even ETA for the official release is unknown VALL-E ETA?. The official repository currently contains an empty space, awaiting further updates. However, for those interested in experimenting with VALL-E, an unofficial PyTorch implementation is available at unofficial PyTorch implementation of VALL-E. This implementation can be trained on a single GPU and is widely used in the community.</p> <p>After eight hours of training, the model can synthesize natural speech!</p> <p>code branch <code>position</code> PR #46 egs <code>https://github.com/lifeiteng/vall-e/tree/position/egs/ljspeech</code> synthesised speech https://github.com/lifeiteng/vall-e/tree/position/egs/ljspeech/demos The next step is to verify <code>in-context learning capabilities</code> of VALL-E on a large dataset.</p> <p>And I found one more training story: After 100 epochs training, the model can synthesize natural speech on LibriTTS</p> <p>I trained vall-e on LibriTTS about 100 epochs (took almost 4 days on 8 A100 GPUs) and I obtained plausible synthesized audio.</p> <p>Here is a demo.</p> <p>[1] prompt : prompt_link  synthesized audio : synt_link</p> <p>[2] prompt : prompt_link  ground truth : gt_link  synthesized audio : synt_link</p> <p>[3] prompt : prompt_link  synthesized audio : synt_link</p> <p>[4] prompt : prompt_link  ground truth : gt_link  synthesized audio : synt_link</p> <p>The model I trained has worse quality than the original vall-e because of the dataset amount. However, it has promising quality in clean audio. I'm not sure whether I can share my pre-trained LibriTTS model. If I can, I would like to share the pre-trained LibriTTS model.</p> <p>Model weights:</p> <p>Sorry for late reply. This is the model that I trained.  google drive link : link</p> <p>One more unofficial PyTorch implementation of VALL-E, based on the EnCodec tokenizer.</p>"},{"location":"review/#vall-e-summary","title":"VALL-E summary","text":"<p>So, VALL-E is an AR or NAR model? VALL-E is a hybrid model that leverages the strengths of both AR and NAR architectures.</p> <p>In section 6 Conclusion, Limitations, and Future Work you can see one important note:</p> <p>Synthesis robustness: We observe that some words may be unclear, missed, or duplicated in speech synthesis. It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue. The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling. In the future, we would like to leverage these techniques to solve the issue.</p> <p>It's interesting to note that the authors acknowledge this limitation and plan to explore techniques from non-autoregressive models or modified attention mechanisms to address this issue in future work. Even state-of-the-art AR models struggle with limitations like disordered attention alignments, leading to synthesis robustness issues.</p>"},{"location":"review/#styledtts-2-towards-human-level-text-to-speech-through-style-diffusion-and-adversarial-training-with-large-speech-language-models","title":"StyledTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models","text":"<p>Github repo StyleTTS 2 is a state-of-the-art text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. By modeling styles as a latent random variable through diffusion models, StyleTTS 2 is able to generate the most suitable style for the input text in diverse speech synthesis. The use of large pre-trained SLMs, such as WavLM, as discriminators with differentiable duration modeling enables end-to-end training and improves speech naturalness.</p>"},{"location":"review/#style-diffusion","title":"Style Diffusion","text":"<p>StyleTTS 2 models speech \\(x\\) as a conditional distribution \\(p(x|t) = \\int p(x|t, s)p(s|t) \\mathrm{d}s\\), where \\(t\\) represents the phonetic content, and \\(s\\) is a latent variable representing the generalized speech style. This style variable \\(s\\) is sampled using Efficient Diffusion Models (EDM), which follow the combined probability flow and time-varying Langevin dynamics. By modeling styles as a latent random variable through diffusion models, StyleTTS 2 can generate the most suitable style for the input text, enabling diverse speech synthesis.</p>"},{"location":"review/#adversarial-training-with-large-slms","title":"Adversarial Training with Large SLMs","text":"<p>StyleTTS 2 employs large pre-trained SLMs, such as WavLM, as discriminators during training. This adversarial training, combined with the proposed differentiable duration modeling, results in improved speech naturalness.</p> <p></p>"},{"location":"review/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Differentiable Upsampling and Fast Style Diffusion: StyleTTS 2 introduces a differentiable upsampling method that allows for generating speech samples during training in a fully differentiable manner, similar to inference. These generated samples are used to optimize the loss function involving the large pre-trained SLM, enabling end-to-end training of all components.</p> </li> <li> <p>Efficient Style Sampling: To enable fast style sampling during inference, StyleTTS 2 uses the ancestral DPM-2 solver instead of the 2nd-order Heun method, allowing for fast and diverse sampling without significantly impacting inference speed.</p> </li> <li> <p>Multispeaker Modeling: For multispeaker settings, StyleTTS 2 models \\(p(s|t, c)\\), where \\(c\\) is a speaker embedding obtained from a reference audio of the target speaker. This speaker embedding is injected into the transformer model using adaptive layer normalization.</p> </li> </ol>"},{"location":"review/#known-issues","title":"Known Issues","text":"<p>It's worth noting that StyleTTS 2 has a known issue where loud white noise can occur on short texts when the <code>embedding_scale</code> parameter is set to a value greater than 1 (see GitHub issue #46).</p>"},{"location":"review/#hybrid-ar-nar-approach-perspective","title":"Hybrid AR-NAR Approach perspective","text":"<p>StyleTTS 2 presents a novel and promising approach to text-to-speech synthesis by combining the strengths of both autoregressive (AR) and non-autoregressive (NAR) models. This hybrid architecture leverages the power of large pre-trained SLMs, which are typically autoregressive models, while incorporating the diversity and expressiveness offered by diffusion models, which are non-autoregressive. By combining the pre-trained SLM with the style diffusion process, StyleTTS 2 effectively creates a hybrid architecture that combines the strengths of both AR and NAR models. The SLM provides a strong foundation for capturing sequential dependencies and generating high-quality speech, while the diffusion process introduces diversity and expressiveness, enabling the generation of natural-sounding speech with appropriate styles and characteristics. The combination of style diffusion and adversarial training with large SLMs has proven to be a powerful approach for generating natural and expressive speech.</p>"},{"location":"review/#non-ar-models","title":"Non AR models","text":""},{"location":"review/#fastpitch-parallel-text-to-speech-with-pitch-prediction","title":"FastPitch: Parallel Text-to-Speech with Pitch Prediction","text":"<p>Implementation NVIDIA/NeMo: FastPitch and Mixer-TTS Training</p> <p>Traditional autoregressive TTS models, which generate speech one step at a time, suffer from slow inference speeds, making them unsuitable for real-time applications. FastPitch, proposed by NVIDIA in 2020, is a fully-parallel text-to-speech model that addresses these challenges by introducing parallel pitch prediction and conditioning the mel-spectrogram generation on the predicted pitch contours. It is based on FastSpeech and composed mainly of two feed-forward Transformer (FFTr) stacks. The first one operates in the resolution of input tokens, the second one in the resolution of the output frames. Let \\(x = (x_1, \\dots, x_n)\\) be the sequence of input lexical units, and \\(y = (y_1, \\dots, y_t)\\) be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation \\(h = \\text{FFTr}(x)\\). The hidden representation \\(h\\) is used to make predictions about the duration and average pitch of every character with a 1-D CNN</p> <p></p> \\[d = \\text{DurationPredictor}(h), \\hat{p} = \\text{PitchPredictor}(h)\\] <p>where \\(\\hat{d} \\in \\mathbb{N}^n\\) and \\(\\hat{p} \\in \\mathbb{R}^n\\). Next, the pitch is projected to match the dimensionality of the hidden representation \\(h \\in \\mathbb{R}^{n\u00d7d}\\) and added to \\(h\\). The resulting sum \\(g\\) is discretely up-sampled and passed to the output FFTr, which produces the output mel-spectrogram sequence:</p> <p>\\(g = h + \\text{PitchEmbedding}(p)\\) \\(\\hat{y} = \\text{FFTr}([\\underbrace{g_1, \\dots, g_1}_{d_1}, \\dots \\underbrace{g_n, \\dots, g_n}_{d_n} ])\\)</p> <p>Ground truth \\(p\\) and \\(d\\) are used during training, and predicted \\(\\hat{p}\\) and \\(\\hat{d}\\) are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities \\(\\mathcal{L} = ||\\hat{y} \u2212 y||^2_2 + \u03b1|| \\hat{p} \u2212 p ||^2_2 + \u03b3||\\hat{d} \u2212 d||^2_2\\)</p>"},{"location":"review/#key-contributions_1","title":"Key Contributions","text":"<p>Parallel Pitch Prediction: One of the main innovations of FastPitch is its ability to predict the fundamental frequency (F0) contours, which represent the pitch variations in speech, directly from the input text in a fully-parallel manner. Unlike previous TTS models that predicted pitch contours autoregressively, FastPitch employs a feed-forward architecture (FFTr) and a dedicated pitch predictor to generate the pitch contours in a single pass. This parallel pitch prediction eliminates the need for autoregressive processing, leading to faster inference times.</p> <p>Conditioning on Pitch Contours: FastPitch conditions the mel-spectrogram generation, which represents the spectral characteristics of speech, on the predicted pitch contours. By explicitly modeling and incorporating the pitch information, FastPitch can capture and reproduce the natural variations in pitch and intonation present in human speech, resulting in more natural-sounding synthesized audio.</p> <p>Feed-Forward Architecture: FastPitch is a feed-forward model, which means that it does not rely on recurrent neural networks (RNNs) or attention mechanisms. This architecture choice contributes to the model's efficiency and parallelizability.</p> <p>Improved Speech Quality: By explicitly modeling pitch contours and conditioning the mel-spectrogram generation on them, FastPitch achieves improved speech quality compared to its predecessor, FastSpeech. The synthesized speech exhibits more natural-sounding pitch variations and intonation patterns.</p>"},{"location":"review/#limitations-and-future-work","title":"Limitations and Future Work","text":"<p>While FastPitch represents a significant advancement in parallel text-to-speech synthesis, the authors acknowledge some limitations and suggest future research directions:</p> <p>Prosody Modeling: Although FastPitch improves pitch prediction, further work is needed to model other aspects of prosody, such as duration and energy contours, in a parallel and efficient manner.</p> <p>Multi-Speaker Adaptation: The FastPitch model presented in the paper is trained on a single-speaker dataset. Extending the model to handle multiple speakers and enable efficient speaker adaptation remains an open challenge.</p> <p>Deployment and Inference Optimization: While FastPitch offers faster inference times compared to autoregressive models, further optimization and deployment strategies could be explored to enable real-time applications and on-device inference.</p> <p>Overall, FastPitch is a notable contribution to the field of text-to-speech synthesis, introducing parallel pitch prediction and demonstrating improved speech quality while maintaining computational efficiency. However, there is still room for further research and development to address the remaining limitations and expand the model's capabilities.</p>"},{"location":"review/#delightfultts-the-microsoft-speech-synthesis-system-for-blizzard-challenge-2021","title":"DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021","text":"<p>DelightfulTTS is a text-to-speech (TTS) system developed by Microsoft for the Blizzard Challenge 2021, a prestigious annual evaluation of TTS systems. This system aims to generate high-quality and expressive speech while maintaining computational efficiency.</p>"},{"location":"review/#architecture","title":"Architecture","text":"<p>DelightfulTTS is based on a non-autoregressive (NAR) architecture, which allows for parallel generation of mel-spectrograms, enabling faster inference compared to traditional autoregressive models. The acoustic model still faces the challenge of the one-to-many mapping problem, where multiple speech variations can correspond to the same input text. To address this issue, DelightfulTTS systematically models variation information from both explicit and implicit perspectives:</p> <ol> <li> <p>For explicit factors like speaker ID and language ID, lookup embeddings are used during training and inference.</p> </li> <li> <p>For explicit factors like pitch and duration, the values are extracted from paired text-speech data during training, and dedicated predictors are employed to estimate these values during inference.</p> </li> <li> <p>For implicit factors like utterance-level and phoneme-level prosody, reference encoders are used to extract the corresponding values during training, while separate predictors are employed to predict these values during inference.</p> </li> </ol> <p>The acoustic model in DelightfulTTS is built upon non-autoregressive generation models like FastSpeech, incorporating an improved Conformer module to better capture local and global dependencies in the mel-spectrogram representation. By explicitly modeling variation information from multiple perspectives and leveraging non-autoregressive architectures, DelightfulTTS aims to generate high-quality and expressive speech while addressing the one-to-many mapping challenge inherent in text-to-speech synthesis.</p>"},{"location":"review/#schema_1","title":"Schema:","text":""},{"location":"review/#conformer-architecture","title":"Conformer Architecture","text":"<p>The Conformer architecture is a variant of the Transformer model that integrates both convolutional neural networks (CNNs) and self-attention mechanisms. It was originally proposed for end-to-end speech recognition tasks and later adopted for TTS systems due to its ability to capture both local and global dependencies in the input sequences. In the context of TTS, the Conformer architecture is typically used in the acoustic model, which is responsible for generating the mel-spectrogram representation of speech from the input text. Mathematically, the Conformer block can be represented as follows: Let \\(X\\) be the input sequence. The key components of the Conformer block used in the acoustic model are:</p> <ol> <li> <p>Convolutional Feed-Forward Module: This module applies a 1D convolutional layer to the input sequence, capturing local dependencies. \\(X' = \\text{Conv1D}(X) + X\\)</p> </li> <li> <p>Depthwise Convolution Module: This module applies a depthwise 1D convolutional layer, further enhancing the modeling of local correlations. \\(X'' = \\text{DepthwiseConv1D}(X') + X'\\)</p> </li> <li> <p>Self-Attention Module: This module employs multi-headed self-attention, allowing the model to capture global dependencies and long-range interactions within the input sequence. \\(X''' = \\text{MultiHeadAttention}(X'') + X''\\)</p> </li> <li> <p>Second Convolutional Feed-Forward Module: Another convolutional feed-forward module is applied after the self-attention module, further processing the output. \\(Y = \\text{Conv1D}(X''') + X'''\\)</p> </li> </ol> <p>Here, \\(\\text{Conv1D}\\), \\(\\text{DepthwiseConv1D}\\), and \\(\\text{MultiHeadAttention}\\) represent the convolutional, depthwise convolutional, and multi-headed self-attention operations, respectively. The Conformer architecture combines the strengths of CNNs and self-attention, allowing the model to capture both local and global dependencies in the input sequence. This is particularly important for TTS tasks, where the model needs to generate mel-spectrograms of varying lengths while capturing the prosodic and acoustic characteristics of speech. In the DelightfulTTS model, the authors likely employed the Conformer architecture in the acoustic model to benefit from its ability to model both local and global dependencies, leading to improved prosody and audio fidelity in the synthesized speech.</p>"},{"location":"review/#variation-information-modeling","title":"Variation Information Modeling","text":"<p>Text-to-speech (TTS) is a typical one-to-many mapping problem where there could be multiple varying speech outputs (e.g., different pitch, duration, speaker, prosody, emotion, etc.) for a given text input. It is critical to model these variation information in speech to improve the expressiveness and fidelity of synthesized speech. While previous works have tried different methods to model the information, they focus on a specific aspect and cannot model in a comprehensive and systematic way. In this paper, considering that different variation information can be complementary to each other, the authors propose a unified way to model them in the proposed variance adaptor (as shown in Figure 1c).</p>"},{"location":"review/#categorization-of-variation-information","title":"Categorization of Variation Information","text":"<p>Observing that some variation information can be obtained implicitly (e.g., pitch can be extracted by some tools) or explicitly (e.g., utterance-level prosody can only be learned by the model), the authors categorize all the information they model as follows:</p> <ol> <li> <p>Explicit Modeling: Language ID, Speaker ID, Pitch, Duration</p> </li> <li> <p>Implicit Modeling: Utterance-level prosody, Phoneme-level prosody</p> </li> </ol> <p>For speaker and language ID, lookup embeddings are used in training and inference. For pitch and duration, the values are extracted from paired text-speech data in training, and two predictors are used to predict the values in inference. For utterance-level and phoneme-level prosody, two reference encoders are used to extract the values in training , and two separate predictors are used to predict the values in inference. The two reference encoders are both made up of convolution and RNN layers. Utterance-level prosody vector is obtained by the last RNN hidden and a style token layer. Phoneme-level prosody vectors are obtained by using the outputs of the phoneme encoder (phoneme-level) as a query to attend to the outputs of the mel-spectrogram reference encoder (frame-level). Different from, the authors do not use VAE but directly use the latent representation as the phoneme-level vector for training stability . The utterance-level prosody predictor contains a GRU layer followed by a bottleneck module to predict the prosody vector. The phoneme-level prosody predictor takes both the outputs of the text encoder and the utterance-level prosody vector as input. With the help of the utterance-level prosody vector, the authors do not need an autoregressive prosody predictor as in for faster inference. By unifying explicit and implicit information in different granularities (language-level, speaker-level, utterance-level, phoneme-level) in the variance adaptor, the authors aim to achieve better expressiveness in prosody and controllability in pitch and duration.</p>"},{"location":"review/#key-contributions_2","title":"Key Contributions","text":"<ul> <li> <p>Non-Autoregressive Architecture: DelightfulTTS employs a non-autoregressive architecture, enabling faster inference compared to autoregressive models while maintaining high speech quality.</p> </li> <li> <p>Explicit Prosody Modeling: The system explicitly models pitch and energy contours, in addition to duration, to capture the prosodic variations in speech, leading to more natural and expressive synthesized speech.</p> </li> <li> <p>Multi-Speaker Capabilities: DelightfulTTS supports multi-speaker synthesis by conditioning the generation process on speaker embeddings, enabling efficient speaker adaptation.</p> </li> </ul>"},{"location":"review/#limitations-and-future-work_1","title":"Limitations and Future Work","text":"<p>While DelightfulTTS represents a significant advancement in non-autoregressive TTS, the authors acknowledge some limitations and suggest future research directions:</p> <ul> <li> <p>Robustness to Long Inputs: The system's performance may degrade for very long input sequences, and further work is needed to improve robustness and consistency for such cases.</p> </li> <li> <p>Fine-Grained Prosody Control: While DelightfulTTS models pitch, energy, and duration, additional work could explore more fine-grained control over prosodic aspects, such as emphasis and phrasing.</p> </li> <li> <p>Deployment and Optimization: Further optimization and deployment strategies could be explored to enable real-time applications and on-device inference, leveraging the non-autoregressive architecture's potential for efficiency.</p> </li> </ul> <p>Overall, DelightfulTTS is a notable contribution to the field of text-to-speech synthesis, demonstrating the potential of non-autoregressive architectures and explicit prosody modeling for generating high-quality and expressive speech. The system's performance in the Blizzard Challenge 2021 highlights its competitiveness and the progress made in this domain.</p>"},{"location":"review/#tts-review-summary","title":"TTS Review summary","text":"<p>Based on my research, I haven't found a clear convergence between the areas of autoregressive (AR) and non-autoregressive (NAR) text-to-speech models in terms of combining or composing them into a single unified model. The key requirements for NAR models are sustainability, the ability to guarantee stability in the generated audio sequences, balanced quality, and fast generation speed, which is a significant advantage of the NAR architecture. Both the FastPitch and DelightfulTTS models meet these criteria. However, I prefer the DelightfulTTS model because it is more complex compared to FastPitch and can potentially provide more diverse results. The DelightfulTTS model's complexity and diversity could be further enhanced by incorporating ideas from the StyledTTS2 model, which uses diffusion as an AR variator to make the speech more unique and expressive based on the context. While the diffusion-based approach in StyledTTS2 can introduce expressiveness, the NAR nature of DelightfulTTS can contribute to the stability of the generated speech. Therefore, a potential direction could be to explore a hybrid approach that combines the strengths of DelightfulTTS's NAR architecture with the expressive capabilities of the diffusion-based variator from StyledTTS2. Such a hybrid model could leverage the stability and efficiency of the NAR framework while incorporating the contextual expressiveness and diversity offered by the diffusion-based variator. However, it's important to note that integrating these two approaches into a single unified model may pose challenges, and further research is needed to investigate the feasibility and effectiveness of such a hybrid approach.</p>"},{"location":"review/#delightfultts-implementation-and-training","title":"DelightfulTTS implementation and training","text":"<p>DelightfulTTS Github repo</p> <p>After conducting research, I decided to implement the DelightfulTTS model. I didn't find many details in the paper about the implementation and hyperparameters, but I found the Comprehensive-Transformer-TTS repository and was heavily influenced by this implementation. For example, you can check the Conformer implementation: Comprehensive-Transformer-TTS/model/transformers/conformer.py</p> <p>I also found another great implementation on GitHub: dunky11/voicesmith</p> <p>VoiceSmith makes it possible to train and infer on both single and multispeaker models without any coding experience. It fine-tunes a pretty solid text to speech pipeline based on a modified version of DelightfulTTS and UnivNet on your dataset.</p> <p>And there's also a coqui-ai PR Add Delightful-TTS model implemetation.</p>"},{"location":"review/#pulp-friction","title":"Pulp Friction","text":"<p>I chose to use my own training code due to the flexibility it offers in the training process. While PyTorch Lightning is an excellent training framework that simplifies multi-GPU training and incorporates many best practices, custom implementations come with their own set of advantages and disadvantages.</p>"},{"location":"review/#how-to-train-our-model","title":"How to Train Our Model","text":"<p>Using PyTorch Lightning to train a model is very straightforward. My training script allows you to train on a single machine with either one or multiple GPUs.</p> <p>I tried to support the hyperparameters and configurations in one place. You can find the configuration in the <code>models/config</code> directory. <code>PreprocessingConfig</code> is the basic piece of configuration where you can set the configuration for preprocessing audio used for training.</p> <p><code>PreprocessingConfigUnivNet</code> and <code>PreprocessingConfigHifiGAN</code> have different <code>n_mel_channels</code>, and these two configurations are not compatible because this parameter fundamentally defines the architecture of the <code>AcousticModel</code>.</p> <p>For the <code>PreprocessingConfigHifiGAN</code> , we have sampling rates of 22050 or 44100. You can see the post-initialization action that sets the parameters of <code>stft</code> based on this parameter:</p> <pre><code>def __post_init__(self):\n    r\"\"\"It modifies the 'stft' attribute based on the 'sampling_rate' attribute.\n    If 'sampling_rate' is 44100, 'stft' is set with specific values for this rate.\n    If 'sampling_rate' is not 22050 or 44100, a ValueError is raised.\n\n    Raises:\n        ValueError: If 'sampling_rate' is not 22050 or 44100.\n    \"\"\"\n    if self.sampling_rate == 44100:\n        self.stft = STFTConfig(\n            filter_length=2048,\n            hop_length=512,\n            win_length=2048,\n            n_mel_channels=80,\n            mel_fmin=20,\n            mel_fmax=11025,\n        )\n    if self.sampling_rate not in [22050, 44100]:\n        raise ValueError(\"Sampling rate must be 22050 or 44100\")\n</code></pre> <p>To set up the cluster, you need to define the following environment variables:</p> <pre><code># Node runk in the cluster\nnode_rank = 0\n# Num nodes in the cluster\nnum_nodes = 1\n\nos.environ[\"WORLD_SIZE\"] = f\"{num_nodes}\"\nos.environ[\"NODE_RANK\"] = f\"{node_rank}\"\n\n# Setup of the training cluster port\nos.environ[\"MASTER_PORT\"] = \"12355\"\n\n# Change the IP address to the IP address of the master node\nos.environ[\"MASTER_ADDR\"] = \"10.148.0.6\"\n</code></pre> <p>You can also choose the logs directory and checkpoints paths:</p> <pre><code>default_root_dir = \"logs\"\nckpt_acoustic = \"./checkpoints/epoch=301-step=124630.ckpt\"\nckpt_vocoder = \"./checkpoints/vocoder.ckpt\"\n</code></pre> <p>Here's my Trainer code. I use the <code>DDPStrategy</code> because it's the only strategy that works in my case. Other useful parameters are:</p> <pre><code>trainer = Trainer(\n    accelerator=\"cuda\",  # Use GPU acceleration\n    devices=-1,  # Use all available GPUs\n    num_nodes=num_nodes,\n    strategy=DDPStrategy(\n        gradient_as_bucket_view=True,  # Optimize gradient communication\n        find_unused_parameters=True,  # Ignore unused parameters\n    ),\n    default_root_dir=default_root_dir,  # Directory for logs\n    enable_checkpointing=True,  # Enable checkpoint saving\n    accumulate_grad_batches=5,  # Accumulate gradients for 5 batches\n    max_epochs=-1,  # Train indefinitely (until manually stopped)\n    log_every_n_steps=10,  # Log every 10 steps\n    gradient_clip_val=0.5,  # Clip gradients to a maximum value of 0.5\n)\n</code></pre> <p>Import the preprocessing configuration that you want to train and choose the type of training. For example:</p> <pre><code>from models.config import PreprocessingConfigUnivNet as PreprocessingConfig\n\n# ...\n\npreprocessing_config = PreprocessingConfig(\"multilingual\")\nmodel = DelightfulTTS(preprocessing_config)\n\ntuner = Tuner(trainer)\n\ntrain_dataloader = model.train_dataloader(\n    # NOTE: Preload the cached dataset into the RAM\n    # Use only if you have enough RAM to cache your entire dataset\n    cache_dir=\"/dev/shm/\",\n    cache=True,\n)\n\ntrainer.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    # Resume training states from the checkpoint file\n    ckpt_path=ckpt_acoustic,\n)\n</code></pre> <p>The entry point to the project is inside the train.py file, which serves as the foundation of the training process.</p>"},{"location":"review/#inference-code","title":"Inference code","text":"<p>For inference, you can use the code examples from <code>demo/demo.py</code>. You have three possible options for combining the acoustic model and vocoder:</p> <ul> <li>DelightfulTTS + UnivNet, with a sample rate of 22050 Hz</li> <li>DelightfulTTS + HiFi-GAN, with a sample rate of 44100 Hz</li> <li>FastPitch + HiFi-GAN, with a sample rate of 44100 Hz</li> </ul> <p>You can experiment with these TTS models and find the best fit for your use case.</p>"},{"location":"review/#dataset-code","title":"Dataset code","text":"<p>For managing datasets, I utilize the lhotse library. This library allows you to wrap your audio dataset, prepare cutset files, and then filter or select specific subsets of the data. For example:</p> <pre><code># Duration lambda\nself.dur_filter = (\n    lambda duration: duration &gt;= self.min_seconds\n    and duration &lt;= self.max_seconds\n)\n\n# Filter the HiFiTTS cutset to only include the selected speakers\nself.cutset_hifi = self.cutset_hifi.filter(\n    lambda cut: isinstance(cut, MonoCut)\n    and str(cut.supervisions[0].speaker) in self.selected_speakers_hi_fi_ids_\n    and self.dur_filter(cut.duration),\n).to_eager()\n</code></pre> <p>With lhotse, you can filter your dataset based on various criteria, such as speaker identities or audio duration. Notably, you don't need to read the audio metadata for this step, as the metadata is already present in the cutset file, which can be prepared beforehand. The lhotse library provides a wide range of functions and utilities for managing and processing speech datasets efficiently. I highly recommend exploring its capabilities to streamline your dataset preparation and filtering tasks.</p> <p>I have prepared the following ready-to-use datasets:</p> <ol> <li> <p>HifiLibriDataset: This dataset code combines the Hi-Fi Multi-Speaker English TTS Dataset and LibriTTS-R. You can choose specific speakers from these datasets or filter the audio based on various parameters. The dataset code can cache data to the filesystem or RAM if needed. This dataset can be used to train the DelightfulTTS model.</p> </li> <li> <p>HifiGanDataset: This dataset code is designed for the HiFi-GAN model. The data is prepared in a specific way, as you can see in the <code>__getitem__</code> method. With this dataset implementation, you can train the HiFi-GAN vocoder.</p> </li> <li> <p>LIBRITTS_R: This code is a modified version of the Pytorch LIBRITTS dataset.</p> </li> </ol> <p>Additionally, you can explore the dataset code I prepared for various experiments within this project.</p>"},{"location":"review/#docs","title":"Docs","text":"<p>I firmly believe that a good project starts with comprehensive documentation, and good code is built upon a solid foundation of test cases. With this in mind, I have made concerted efforts to maintain consistent documentation and ensure thorough test coverage for my code. The repository serves as a comprehensive resource where you can explore the implementation details, review the documentation, and examine the test cases that ensure the code's reliability and correctness.</p> <p>You can find all the documentation inside the <code>docs</code> directory, run the docs locally with <code>mkdocs serve</code></p> <p>Also here you can the docs online</p>"},{"location":"review/#acoustic-model","title":"Acoustic model","text":"<p>The acoustic model is responsible for generating mel-spectrograms from input phoneme sequences, speaker identities, and language identities.</p> <p>The core of your acoustic model is the AcousticModel class, which inherits from PyTorch's <code>nn.Module</code>. This class contains several sub-modules and components that work together to generate the mel-spectrogram output.</p> <ol> <li> <p>Encoder: The encoder is a Conformer module that processes the input phoneme sequences. It takes the phoneme embeddings, speaker embeddings, language embeddings, and positional encodings as input and generates a hidden representation.</p> </li> <li> <p>Prosody Modeling:</p> </li> <li> <p>Utterance-Level Prosody: The model includes an UtteranceLevelProsodyEncoder and a PhonemeProsodyPredictor (for utterance-level prosody) to capture and predict the prosodic features at the utterance level.</p> </li> <li> <p>Phoneme-Level Prosody: Similarly, there is a PhonemeLevelProsodyEncoder and a PhonemeProsodyPredictor (for phoneme-level prosody) to model the prosodic features at the phoneme level.</p> </li> <li> <p>Variance Adaptor:</p> </li> <li> <p>Pitch Adaptor: The PitchAdaptorConv module is responsible for adding pitch embeddings to the encoder output, based on the predicted or target pitch values.</p> </li> <li> <p>Energy Adaptor: The EnergyAdaptor module adds energy embeddings to the encoder output, based on the predicted or target energy values.</p> </li> <li> <p>Length Adaptor: The LengthAdaptor module upsamples the encoder output to match the length of the target mel-spectrogram, using the predicted or target duration values.</p> </li> <li> <p>Decoder: The decoder is another Conformer module that takes the output from the variance adaptor modules and generates the final mel-spectrogram prediction.</p> </li> <li> <p>Embeddings: The model includes learnable embeddings for phonemes, speakers, and languages, which are combined and used as input to the encoder.</p> </li> <li> <p>Aligner: The Aligner module is used during training to compute the attention logits and alignments between the encoder output and the target mel-spectrogram.</p> </li> </ol> <p>The <code>forward_train</code> method defines the forward pass during training, where it takes the input phoneme sequences, speaker identities, language identities, mel-spectrograms, pitches, energies, and attention priors (if available). It computes the mel-spectrogram prediction, as well as the predicted pitch, energy, and duration values, which are used for computing the training loss. The forward method defines the forward pass during inference, where it takes the input phoneme sequences, speaker identities, language identities, and duration control value, and generates the mel-spectrogram prediction. Additionally, there are methods for freezing and unfreezing the model parameters, as well as preparing the model for export by removing unnecessary components (e.g., prosody encoders) that are not needed during inference. Implementation follows the DelightfulTTS architecture and incorporates various components for modeling prosody, pitch, energy, and duration, while leveraging the Conformer modules for the encoder and decoder.</p>"},{"location":"review/#results","title":"Results","text":""},{"location":"review/#total_loss","title":"<code>total_loss</code>","text":""},{"location":"review/#mel_loss","title":"<code>mel_loss</code>","text":"<p>Check our online demo</p> <p>Training the DelightfulTTS model is a computationally intensive task due to its large size and numerous components. Achieving stable and consistent results requires significant computational resources and time. In my experience, using 8 Nvidia V100 GPUs resulted in extremely slow progress, with visible improvements taking weeks to manifest. However, upgrading to 8 Nvidia A100 GPUs significantly accelerated the training process, allowing visible progress to be observed within days. For optimal performance and efficiency, the ideal hardware configuration would be a cluster with 16 Nvidia A100 GPUs. The computational demands of the DelightfulTTS model highlight the importance of leveraging state-of-the-art hardware accelerators, such as the Nvidia A100 GPUs, to enable practical training times and facilitate the development of high-quality text-to-speech systems.</p>"},{"location":"review/#future-work","title":"Future work","text":"<p>The combination of autoregressive (AR) and non-autoregressive (NAR) architectures is a promising direction in the text-to-speech (TTS) field. Diffusion models can introduce sufficient variation, and the approach taken by StyledTTS2 is particularly appealing. A potential avenue for future exploration could be to integrate the FastPitch model, known for its speed and stability, with the diffusion block from StyledTTS2. This hybrid approach could leverage the strengths of both models. The NAR nature of FastPitch would contribute to the overall speed and stability of the system, while the diffusion-based component from StyledTTS2 could introduce diversity and expressiveness to the generated speech. By combining the efficient and robust FastPitch model with the expressive capabilities of the diffusion-based variator from StyledTTS2, this hybrid architecture could potentially deliver diverse and natural-sounding speech while maintaining computational efficiency and stability. However, integrating these two distinct approaches into a unified model may pose challenges, and further research would be necessary to investigate the feasibility and effectiveness of such a hybrid solution.</p>"},{"location":"dev/change_log/","title":"Change Log","text":""},{"location":"dev/change_log/#major-changes","title":"Major Changes","text":""},{"location":"dev/change_log/#1-text-preprocessing-is-changed","title":"1 Text preprocessing is changed.","text":"<p>Dunky11 created a not-so-good way for text preprocessing. Maybe his solutions are appropriate for the previous releases of NeMo, but now it works stable. I can't find the described scenarios: Dunky11 quote:</p> <p>Nemo's text normalizer unfortunately produces a large amount of false positives. For example it normalizes 'medic' into 'm e d i c' or 'yeah' into 'y e a h'. To reduce the amount of false positives we will do a check for unusual symbols or words inside the text and only normalize if necessary. I checked the described cases and it works fine. Tried to find an issue, but wasn't lucky. Maybe I did it wrong. I have the code, that wrapped the <code>NeMo</code> and added several more preprocessing features, that absolutely required, like char mapping. You can find docs here: NormalizeText</p>"},{"location":"dev/change_log/#2-the-phonemizer-process-g2p-and-tokenization-process-are-changed","title":"2 The phonemizer process (G2P) and tokenization process are changed.","text":"<p>I tried to build the same tokenization as Dunky11, but failed, because of the vocab. It's not possible to reproduce the same vocab in the same order, and the vocab that I have missesed some <code>IPA</code> tokens. Change of the vocab order == lost all the progress that was made from the training steps. So, it makes no sense to build my own tokenization that lost benefits during the training process. I decided to use tokenizations from DeepPhonemizer, maybe Dunky11 didn't find it, I don't understand why he's built his own solution.</p> <p>Maybe because of the <code>[SILENCE]</code> token from here:</p> <pre><code>for sentence in sentence_tokenizer.tokenize(text):\n    symbol_ids = []\n    sentence = text_normalizer(sentence)\n    for word in word_tokenizer.tokenize(sentence):\n        word = word.lower()\n        if word.strip() == \"\":\n            continue\n        elif word in [\".\", \"?\", \"!\"]:\n            symbol_ids.append(self.symbol2id[word])\n        elif word in [\",\", \";\"]:\n            symbol_ids.append(self.symbol2id[\"SILENCE\"])\n</code></pre>"},{"location":"dev/change_log/#3-training-framework-instead-of-tricky-training-spaghetti","title":"3 Training framework instead of tricky training spaghetti","text":"<p>PyTorch Lightning instead of wild hacks.</p>"},{"location":"dev/problems_and_fixes/","title":"Problems and Fixes","text":""},{"location":"dev/problems_and_fixes/#fixes","title":"Fixes","text":"<p>During the development process, a huge amount of job has been done. For example, the <code>stft</code> code changed to the latest way of conversion from <code>complex</code> to <code>real</code>. For example:</p> <pre><code># Compute the short-time Fourier transform of the input waveform\nx = torch.stft(\n    x,\n    n_fft=n_fft,\n    hop_length=hop_length,\n    win_length=win_length,\n    center=False,\n    return_complex=True,\n    # Add window parameter to prevent the signal leak\n    window=torch.ones(win_length, device=x.device),\n)  # [B, F, TT, 2]\n\n# Convert to real as the additional step\nx = torch.view_as_real(x)\n</code></pre> <p>It's a tested fix, and works stable.</p> <p>Minor fixes, and <code>TODO</code> that I can't fix now, but wanted to fix for the future.</p> <p>The ideas behind the model and architecture are mostly the same. The training code is completely different, there is a base of dunky11 and the architecture is completely new.</p>"},{"location":"dev/problems_and_fixes/#changes","title":"Changes","text":""},{"location":"dev/problems_and_fixes/#pitch_adaptor2-and-pitches_stat","title":"pitch_adaptor2 and pitches_stat","text":"<p>Instead of <code>pitch_adaptor</code> that used the <code>stats.json</code> (which is unknown) that looks like critical issue for me, I have a new version of <code>PitchAdaptor</code> that receive an argument <code>pitch_range</code>.</p> <p>Inside the <code>AcousticModule</code> I have the <code>pitches_stat</code>, parameter, it's required for the audio generation and <code>PitchAdaptor. Without the new weights that keep-in-track</code>pitches_stat``, it's not possible to generate the waveform or mel-spectrogram.</p>"},{"location":"dev/problems_and_fixes/#latest-changes-in-pitchadaptor-now-i-have-pitchadaptorconv","title":"Latest changes in PitchAdaptor, now I have PitchAdaptorConv!","text":"<p><code>PitchAdaptorConv</code> is much more effective and during the training I found out the best performance and quality compare to the <code>PitchAdaptor</code> based on <code>Embeddings</code> idea.</p>"},{"location":"dev/problems_and_fixes/#problems","title":"Problems","text":""},{"location":"dev/problems_and_fixes/#fixme-step-param","title":"FIXME: Step param!","text":"<p>I have a <code>step</code> parameter, it requires the future investigation. Maybe I need to add this param to the model step with <code>self.register_buffer</code>. It's required for the FastSpeech2LossGen Possible training issue!</p>"},{"location":"dev/problems_and_fixes/#done-fixed-with-selfglobal_step","title":"Done: fixed with <code>self.global_step</code>","text":""},{"location":"dev/readme/","title":"References","text":""},{"location":"dev/readme/#development-process","title":"Development process","text":"<p>Docs for the training code are here: Modules</p> <p>Here I describe only the training process, approaches, and technical details of the implementation. What is changed compared to the original implementation dunky11/voicesmith</p> <p>And crucial ideas behind the training and architecture. Description of the problems and fixes.</p>"},{"location":"dev/readme/#change-log","title":"Change log","text":"<p>Difference with original implementation.</p>"},{"location":"dev/readme/#problems-and-fixes","title":"Problems and fixes","text":"<p>Fixes and problem in the system.</p>"},{"location":"dev/readme/#training","title":"Training","text":"<p>Training docs and examples.</p>"},{"location":"dev/training/","title":"Training process","text":""},{"location":"dev/training/#lightning-checkpoints","title":"Lightning checkpoints","text":""},{"location":"dev/training/#more-details-are-here-transfer-learning","title":"More details are here: Transfer Learning","text":"<p>The new checkpoints, generated by <code>lightning</code>, are very easy to save and load:</p> <pre><code>trainer = Trainer(\n    # Save checkpoints to the `default_root_dir` directory\n    default_root_dir=\"checkpoints/acoustic\",\n    limit_train_batches=2,\n    max_epochs=1,\n    accelerator=\"cuda\",\n)\n\n# Training process...\nresult = trainer.fit(model=module, train_dataloaders=train_dataloader)\n\n# ...\n\n# Restore from the checkpoint\nacoustic_module = AcousticModule.load_from_checkpoint(\n    \"./checkpoints/am_pitche_stats.ckpt\",\n)\n\nvocoder_module = VocoderModule.load_from_checkpoint(\n    \"./checkpoints/vocoder.ckpt\",\n)\n</code></pre>"},{"location":"dev/lightning/readme/","title":"Lightning docs summary","text":""},{"location":"dev/lightning/readme/#basic","title":"Basic:","text":""},{"location":"dev/lightning/readme/#hyperparameters-from-the-cli-and-argparse","title":"Hyperparameters from the CLI and argparse","text":""},{"location":"dev/lightning/readme/#debug","title":"Debug","text":""},{"location":"dev/lightning/readme/#early-stopping","title":"Early Stopping","text":""},{"location":"dev/lightning/readme/#find-bottleneck","title":"Find bottleneck","text":""},{"location":"dev/lightning/readme/#saving-loading-checkpoints","title":"Saving Loading Checkpoints","text":""},{"location":"dev/lightning/readme/#visualize-experiments","title":"Visualize experiments","text":""},{"location":"dev/lightning/readme/#intermediate","title":"Intermediate","text":""},{"location":"dev/lightning/readme/#hardware-agnostic-training","title":"Hardware agnostic training","text":""},{"location":"dev/lightning/readme/#gpu-training","title":"GPU training","text":""},{"location":"dev/lightning/readme/#lightning-data-module","title":"Lightning Data Module","text":""},{"location":"dev/lightning/readme/#configure-hyperparameters-from-the-cli","title":"Configure hyperparameters from the CLI","text":""},{"location":"dev/lightning/readme/#customize-checkpointing-behavior","title":"Customize checkpointing behavior","text":""},{"location":"dev/lightning/readme/#track-and-visualize-experiments","title":"Track and Visualize Experiments","text":""},{"location":"dev/lightning/readme/#sota-scaling-techniques-n-bit-precision","title":"SOTA scaling techniques (N-Bit Precision)","text":""},{"location":"dev/lightning/readme/#important-effective-training-techniques-acc-gradcliping","title":"Important! Effective Training Techniques (Acc grad,cliping)","text":""},{"location":"dev/lightning/readme/#deploy-models-into-production","title":"Deploy models into production","text":""},{"location":"dev/lightning/readme/#advanced","title":"Advanced","text":""},{"location":"dev/lightning/readme/#configure-hyperparameters-from-the-cli-advanced","title":"Configure hyperparameters from the CLI (Advanced)","text":""},{"location":"dev/lightning/readme/#customize-training-loop","title":"Customize training loop","text":""},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/","title":"Configure hyperparameters from the CLI","text":""},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#configure-hyperparameters-from-the-cli-advanced","title":"Configure hyperparameters from the CLI (Advanced)","text":""},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#more-details-are-here-configure-hyperparameters-from-the-cli-advanced","title":"More details are here: Configure hyperparameters from the CLI (Advanced)","text":""},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#run-using-a-config-file","title":"Run using a config file","text":"<p>To run the CLI using a yaml config, do:</p> <pre><code>python main.py fit --config config.yaml\n</code></pre> <p>Individual arguments can be given to override options in the config file:</p> <pre><code>python main.py fit --config config.yaml --trainer.max_epochs 100\n</code></pre>"},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#automatic-save-of-config","title":"Automatic save of config","text":"<p>To ease experiment reporting and reproducibility, by default <code>LightningCLI</code> automatically saves the full YAML configuration in the log directory. After multiple fit runs with different hyperparameters, each one will have in its respective log directory a <code>config.yaml</code> file. These files can be used to trivially reproduce an experiment, e.g.:</p> <pre><code>python main.py fit --config lightning_logs/version_7/config.yaml\n</code></pre> <p>The automatic saving of the config is done by the special callback <code>SaveConfigCallback</code>. This callback is automatically added to the Trainer. To disable the save of the config, instantiate <code>LightningCLI</code> with <code>save_config_callback=None</code>.</p> <p>To change the file name of the saved configs to e.g. <code>name.yaml</code>, do:</p> <pre><code>cli = LightningCLI(..., save_config_kwargs={\"config_filename\": \"name.yaml\"})\n</code></pre> <p>It is also possible to extend the <code>SaveConfigCallback</code> class, for instance to additionally save the config in a logger. An example of this is:</p> <pre><code>class LoggerSaveConfigCallback(SaveConfigCallback):\n    def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -&gt; None:\n        if isinstance(trainer.logger, Logger):\n            config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\n            trainer.logger.log_hyperparams({\"config\": config})\n\n\ncli = LightningCLI(..., save_config_callback=LoggerSaveConfigCallback)\n</code></pre>"},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#prepare-a-config-file-for-the-cli","title":"Prepare a config file for the CLI","text":"<p>The <code>--help</code> option of the CLIs can be used to learn which configuration options are available and how to use them. However, writing a config from scratch can be time-consuming and error-prone. To alleviate this, the CLIs have the <code>--print_config</code> argument, which prints to stdout the configuration without running the command.</p> <p>For a CLI implemented as <code>LightningCLI(DemoModel, BoringDataModule)</code>, executing:</p> <pre><code>python main.py fit --print_config\n</code></pre> <p>generates a config with all default values like the following:</p> <pre><code>seed_everything: null\ntrainer:\n  logger: true\n  ...\nmodel:\n  out_dim: 10\n  learning_rate: 0.02\ndata:\n  data_dir: ./\nckpt_path: null\n</code></pre> <p>A standard procedure to run experiments can be:</p> <pre><code># Print a configuration to have as reference\npython main.py fit --print_config &gt; config.yaml\n# Modify the config to your liking - you can remove all default arguments\nnano config.yaml\n# Fit your model using the edited configuration\npython main.py fit --config config.yaml\n</code></pre>"},{"location":"dev/lightning/advanced/hyperparameters_from_the_cli/#customize-arguments","title":"Customize arguments","text":""},{"location":"dev/lightning/advanced/own_your_loop/","title":"Customize training loop","text":""},{"location":"dev/lightning/advanced/own_your_loop/#customize-training-loop","title":"Customize training loop","text":""},{"location":"dev/lightning/advanced/own_your_loop/#more-details-are-here-own-your-loop-advanced","title":"More details are here: Own your loop (advanced)","text":""},{"location":"dev/lightning/advanced/own_your_loop/#manual-optimization","title":"Manual Optimization","text":"<p>For advanced research topics like reinforcement learning, sparse coding, or GAN research, it may be desirable to manually manage the optimization process, especially when dealing with multiple optimizers at the same time.</p> <p>In this mode, Lightning will handle only accelerator, precision and strategy logic. The users are left with <code>optimizer.zero_grad()</code>, gradient accumulation, optimizer toggling, etc.</p> <p>To manually optimize, do the following:</p> <ul> <li>Set <code>self.automatic_optimization=False</code> in your <code>LightningModule</code>'s <code>__init__</code>.</li> <li>Use the following functions and call them manually:<ul> <li><code>self.optimizers()</code> to access your optimizers (one or multiple)</li> <li><code>optimizer.zero_grad()</code> to clear the gradients from the previous training step</li> <li><code>self.manual_backward(loss)</code> instead of <code>loss.backward()</code></li> <li><code>optimizer.step()</code> to update your model parameters</li> <li><code>self.toggle_optimizer()</code> and <code>self.untoggle_optimizer()</code> if needed</li> </ul> </li> </ul> <p>Here is a minimal example of manual optimization.</p> <pre><code>from lightning.pytorch import LightningModule\n\n\nclass MyModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Important: This property activates manual optimization.\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n</code></pre>"},{"location":"dev/lightning/advanced/own_your_loop/#access-your-own-optimizer","title":"Access your Own Optimizer","text":"<p>The provided optimizer is a <code>LightningOptimizer</code> object wrapping your own optimizer configured in your <code>configure_optimizers()</code>. You can access your own optimizer with <code>optimizer.optimizer</code>. However, if you use your own optimizer to perform a step, Lightning won't be able to support accelerators, precision and profiling for you.</p> <pre><code>class Model(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n        ...\n\n    def training_step(self, batch, batch_idx):\n        optimizer = self.optimizers()\n\n        # `optimizer` is a `LightningOptimizer` wrapping the optimizer.\n        # To access it, do the following.\n        # However, it won't work on TPU, AMP, etc...\n        optimizer = optimizer.optimizer\n        ...\n</code></pre>"},{"location":"dev/lightning/advanced/own_your_loop/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>You can accumulate gradients over batches similarly to <code>accumulate_grad_batches</code> argument in <code>Trainer</code> for automatic optimization. To perform gradient accumulation with one optimizer after every <code>N</code> steps, you can do as such.</p> <pre><code>def __init__(self):\n    super().__init__()\n    self.automatic_optimization = False\n\n\ndef training_step(self, batch, batch_idx):\n    opt = self.optimizers()\n\n    # scale losses by 1/N (for N batches of gradient accumulation)\n    loss = self.compute_loss(batch) / N\n    self.manual_backward(loss)\n\n    # accumulate gradients of N batches\n    if (batch_idx + 1) % N == 0:\n        opt.step()\n        opt.zero_grad()\n</code></pre>"},{"location":"dev/lightning/advanced/own_your_loop/#gradient-clipping","title":"Gradient Clipping","text":"<p>You can clip optimizer gradients during manual optimization similar to passing the <code>gradient_clip_val</code> and <code>gradient_clip_algorithm</code> argument in Trainer during automatic optimization. To perform gradient clipping with one optimizer with manual optimization, you can do as such.</p> <pre><code>from lightning.pytorch import LightningModule\n\n\nclass SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n\n        # compute loss\n        loss = self.compute_loss(batch)\n\n        opt.zero_grad()\n        self.manual_backward(loss)\n\n        # clip gradients\n        self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n\n        opt.step()\n</code></pre> <p>Note that <code>configure_gradient_clipping()</code> won't be called in Manual Optimization. Instead consider using self. <code>clip_gradients()</code> manually like in the example above.</p> <p>Note that <code>configure_gradient_clipping()</code> won\u2019t be called in Manual Optimization. Instead consider using self. <code>clip_gradients()</code> manually like in the example above.</p>"},{"location":"dev/lightning/advanced/own_your_loop/#use-multiple-optimizers-like-gans","title":"Use Multiple Optimizers (like GANs)","text":"<p>Here is an example training a simple GAN with multiple optimizers using manual optimization.</p> <pre><code>import torch\nfrom torch import Tensor\nfrom lightning.pytorch import LightningModule\n\n\nclass SimpleGAN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.G = Generator()\n        self.D = Discriminator()\n\n        # Important: This property activates manual optimization.\n        self.automatic_optimization = False\n\n    def sample_z(self, n) -&gt; Tensor:\n        sample = self._Z.sample((n,))\n        return sample\n\n    def sample_G(self, n) -&gt; Tensor:\n        z = self.sample_z(n)\n        return self.G(z)\n\n    def training_step(self, batch, batch_idx):\n        # Implementation follows the PyTorch tutorial:\n        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n        g_opt, d_opt = self.optimizers()\n\n        X, _ = batch\n        batch_size = X.shape[0]\n\n        real_label = torch.ones((batch_size, 1), device=self.device)\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\n\n        g_X = self.sample_G(batch_size)\n\n        ##########################\n        # Optimize Discriminator #\n        ##########################\n        d_x = self.D(X)\n        errD_real = self.criterion(d_x, real_label)\n\n        d_z = self.D(g_X.detach())\n        errD_fake = self.criterion(d_z, fake_label)\n\n        errD = errD_real + errD_fake\n\n        d_opt.zero_grad()\n        self.manual_backward(errD)\n        d_opt.step()\n\n        ######################\n        # Optimize Generator #\n        ######################\n        d_z = self.D(g_X)\n        errG = self.criterion(d_z, real_label)\n\n        g_opt.zero_grad()\n        self.manual_backward(errG)\n        g_opt.step()\n\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\n    def configure_optimizers(self):\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n        return g_opt, d_opt\n</code></pre>"},{"location":"dev/lightning/advanced/own_your_loop/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Every optimizer you use can be paired with any Learning Rate Scheduler. Please see the documentation of <code>configure_optimizers()</code> for all the available options</p> <p>You can call <code>lr_scheduler.step()</code> at arbitrary intervals. Use <code>self.lr_schedulers()</code> in your <code>LightningModule</code> to access any learning rate schedulers defined in your <code>configure_optimizers()</code>.</p>"},{"location":"dev/lightning/advanced/own_your_loop/#more-details-are-here-own-your-loop-advanced_1","title":"More details are here: Own your loop (advanced)","text":""},{"location":"dev/lightning/basic/argparse_and_cli/","title":"Argparse and Hyperparams","text":""},{"location":"dev/lightning/basic/argparse_and_cli/#hyperparameters-from-the-cli","title":"Hyperparameters from the CLI","text":""},{"location":"dev/lightning/basic/argparse_and_cli/#more-details-are-here-configure-hyperparameters-from-the-cli","title":"More details are here: Configure hyperparameters from the CLI","text":"<p>The <code>ArgumentParser</code> is a built-in feature in Python that let's you build CLI programs. You can use it to make hyperparameters and other training settings available from the command line:</p> <pre><code>from argparse import ArgumentParser\n\nparser = ArgumentParser()\n\n# Trainer arguments\nparser.add_argument(\"--devices\", type=int, default=2)\n\n# Hyperparameters for the model\nparser.add_argument(\"--layer_1_dim\", type=int, default=128)\n\n# Parse the user inputs and defaults (returns a argparse.Namespace)\nargs = parser.parse_args()\n\n# Use the parsed arguments in your program\ntrainer = Trainer(devices=args.devices)\nmodel = MyModel(layer_1_dim=args.layer_1_dim)\n</code></pre> <p>This allows you to call your program like so:</p> <pre><code>python trainer.py --layer_1_dim 64 --devices 1\n</code></pre>"},{"location":"dev/lightning/basic/argparse_and_cli/#docs-and-examples","title":"Docs and examples:","text":""},{"location":"dev/lightning/basic/argparse_and_cli/#argparse-parser-for-command-line-options-arguments-and-sub-commands","title":"argparse \u2014 Parser for command-line options, arguments and sub-commands","text":""},{"location":"dev/lightning/basic/argparse_and_cli/#command-line-option-and-argument-parsing-using-argparse-in-python","title":"Command-Line Option and Argument Parsing using argparse in Python","text":""},{"location":"dev/lightning/basic/argparse_and_cli/#build-command-line-interfaces-with-pythons-argparse","title":"Build Command-Line Interfaces With Python's argparse","text":""},{"location":"dev/lightning/basic/debug/","title":"Debug","text":""},{"location":"dev/lightning/basic/debug/#how-to-debug-lightning","title":"How to debug Lightning","text":""},{"location":"dev/lightning/basic/debug/#more-details-are-here-debug","title":"More details are here: Debug","text":""},{"location":"dev/lightning/basic/debug/#breakpoint-better-use-debugger","title":"Breakpoint (better use debugger)","text":"<pre><code>def function_to_debug():\n    x = 2\n\n    # set breakpoint\n    import pdb\n\n    pdb.set_trace()\n    y = x**2\n</code></pre>"},{"location":"dev/lightning/basic/debug/#run-all-your-model-code-once-quickly","title":"Run all your model code once quickly","text":"<p>The <code>fast_dev_run</code> argument in the trainer runs 5 batch of training, validation, test and prediction data through your trainer to see if there are any bugs:</p> <pre><code>Trainer(fast_dev_run=True)\n</code></pre> <p>To change how many batches to use, change the argument to an integer. Here we run 7 batches of each:</p> <pre><code>Trainer(fast_dev_run=7)\n</code></pre>"},{"location":"dev/lightning/basic/debug/#shorten-the-epoch-length","title":"Shorten the epoch length","text":"<pre><code># use only 10% of training data and 1% of val data\ntrainer = Trainer(limit_train_batches=0.1, limit_val_batches=0.01)\n\n# use 10 batches of train and 5 batches of val\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=5)\n</code></pre>"},{"location":"dev/lightning/basic/debug/#sanity-check","title":"Sanity Check","text":"<p>Lightning runs 2 steps of validation in the beginning of training. This avoids crashing in the validation loop sometime deep into a lengthy training loop.</p> <pre><code>trainer = Trainer(num_sanity_val_steps=2)\n</code></pre>"},{"location":"dev/lightning/basic/debug/#print-input-output-layer-dimensions","title":"Print input output layer dimensions","text":"<p>Whenever the <code>.fit()</code> function gets called, the Trainer will print the weights summary for the LightningModule.</p> <pre><code>trainer.fit(...)\n</code></pre> <p>this generate a table like:</p> <pre><code>  | Name  | Type        | Params\n----------------------------------\n0 | net   | Sequential  | 132 K\n1 | net.0 | Linear      | 131 K\n2 | net.1 | BatchNorm1d | 1.0 K\n</code></pre> <p>To add the child modules to the summary add a <code>ModelSummary</code>:</p> <pre><code>from lightning.pytorch.callbacks import ModelSummary\n\ntrainer = Trainer(callbacks=[ModelSummary(max_depth=-1)])\n</code></pre> <p>To print the model summary if <code>.fit()</code> is not called:</p> <pre><code>from lightning.pytorch.utilities.model_summary import ModelSummary\n\nmodel = LitModel()\nsummary = ModelSummary(model, max_depth=-1)\nprint(summary)\n</code></pre> <p>To turn off the autosummary use:</p> <pre><code>Trainer(enable_model_summary=False)\n</code></pre>"},{"location":"dev/lightning/basic/early_stopping/","title":"Early Stopping","text":""},{"location":"dev/lightning/basic/early_stopping/#early-stopping","title":"Early Stopping","text":""},{"location":"dev/lightning/basic/early_stopping/#more-details-are-here-earlystopping","title":"More details are here: EarlyStopping","text":"<p>You can stop and skip the rest of the current epoch early by overriding <code>on_train_batch_start()</code> to return <code>-1</code> when some condition is met.</p>"},{"location":"dev/lightning/basic/early_stopping/#earlystopping-callback","title":"EarlyStopping Callback","text":"<p>The <code>EarlyStopping</code> callback can be used to monitor a metric and stop the training when no improvement is observed.</p> <pre><code>from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\n# ...\ntrainer = Trainer(\n    # Save checkpoints to the `default_root_dir` directory\n    default_root_dir=\"checkpoints/acoustic\",\n    limit_train_batches=2,\n    max_epochs=1,\n    accelerator=\"cuda\",\n    # Need to define the criterias\n    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]\n)\n</code></pre>"},{"location":"dev/lightning/basic/find_bottlenecks/","title":"Find Bottlenecks","text":""},{"location":"dev/lightning/basic/find_bottlenecks/#find-training-loop-bottlenecks","title":"Find training loop bottlenecks","text":""},{"location":"dev/lightning/basic/find_bottlenecks/#more-details-are-here-find-bottlenecks","title":"More details are here: Find bottlenecks","text":"<p>The most basic profile measures all the key methods across <code>Callbacks</code>, <code>DataModules</code> and the <code>LightningModule</code> in the training loop.</p> <pre><code>trainer = Trainer(profiler=\"simple\")\n</code></pre> <p>Once the <code>.fit()</code> function has completed, you\u2019ll see an output like this:</p> <pre><code>FIT Profiler Report\n\n-----------------------------------------------------------------------------------------------\n|  Action                                          |  Mean duration (s)     |  Total time (s) |\n-----------------------------------------------------------------------------------------------\n|  [LightningModule]BoringModel.prepare_data       |  10.0001               |  20.00          |\n|  run_training_epoch                              |  6.1558                |  6.1558         |\n|  run_training_batch                              |  0.0022506             |  0.015754       |\n|  [LightningModule]BoringModel.optimizer_step     |  0.0017477             |  0.012234       |\n|  [LightningModule]BoringModel.val_dataloader     |  0.00024388            |  0.00024388     |\n|  on_train_batch_start                            |  0.00014637            |  0.0010246      |\n|  [LightningModule]BoringModel.teardown           |  2.15e-06              |  2.15e-06       |\n|  [LightningModule]BoringModel.on_train_start     |  1.644e-06             |  1.644e-06      |\n|  [LightningModule]BoringModel.on_train_end       |  1.516e-06             |  1.516e-06      |\n|  [LightningModule]BoringModel.on_fit_end         |  1.426e-06             |  1.426e-06      |\n|  [LightningModule]BoringModel.setup              |  1.403e-06             |  1.403e-06      |\n|  [LightningModule]BoringModel.on_fit_start       |  1.226e-06             |  1.226e-06      |\n-----------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"dev/lightning/basic/find_bottlenecks/#profile-the-time-within-every-function","title":"Profile the time within every function","text":"<pre><code>trainer = Trainer(profiler=\"advanced\")\n</code></pre> <p>Once the <code>.fit()</code> function has completed, you\u2019ll see an output like this:</p> <pre><code>Profiler Report\n\nProfile stats for: get_train_batch\n        4869394 function calls (4863767 primitive calls) in 18.893 seconds\nOrdered by: cumulative time\nList reduced from 76 to 10 due to restriction &lt;10&gt;\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n    1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n    1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n    1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n    1875     0.084    0.000   18.290    0.010 fetch.py:44(&lt;listcomp&gt;)\n    60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n    60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n    60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n    60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n    60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n</code></pre> <p>If the profiler report becomes too long, you can stream the report to a file:</p> <pre><code>from lightning.pytorch.profilers import AdvancedProfiler\n\nprofiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\ntrainer = Trainer(profiler=profiler)\n</code></pre>"},{"location":"dev/lightning/basic/find_bottlenecks/#measure-accelerator-usage","title":"Measure accelerator usage","text":"<p>Another helpful technique to detect bottlenecks is to ensure that you\u2019re using the full capacity of your accelerator (GPU/TPU/IPU/HPU). This can be measured with the <code>DeviceStatsMonitor</code>:</p> <pre><code>from lightning.pytorch.callbacks import DeviceStatsMonitor\n\ntrainer = Trainer(callbacks=[DeviceStatsMonitor()])\n</code></pre> <p>CPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set <code>DeviceStatsMonitor(cpu_stats=True)</code>. To disable logging CPU metrics, you can specify <code>DeviceStatsMonitor(cpu_stats=False)</code>.</p>"},{"location":"dev/lightning/basic/find_bottlenecks/#find-bottlenecks-in-your-code-intermediate","title":"Find bottlenecks in your code (intermediate)","text":""},{"location":"dev/lightning/basic/find_bottlenecks/#more-details-are-here-find-bottlenecks-in-your-code-intermediate","title":"More details are here: Find bottlenecks in your code (intermediate)","text":""},{"location":"dev/lightning/basic/find_bottlenecks/#profile-pytorch-operations","title":"Profile pytorch operations","text":"<p>To understand the cost of each PyTorch operation, use the <code>PyTorchProfiler</code> built on top of the PyTorch profiler.</p> <pre><code>from lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler()\ntrainer = Trainer(profiler=profiler)\n</code></pre> <p>The profiler will generate an output like this:</p> <pre><code>Profiler Report\n\nProfile stats for: training_step\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nt                      62.10%           1.044ms          62.77%           1.055ms          1.055ms\naddmm                  32.32%           543.135us        32.69%           549.362us        549.362us\nmse_loss               1.35%            22.657us         3.58%            60.105us         60.105us\nmean                   0.22%            3.694us          2.05%            34.523us         34.523us\ndiv_                   0.64%            10.756us         1.90%            32.001us         16.000us\nones_like              0.21%            3.461us          0.81%            13.669us         13.669us\nsum_out                0.45%            7.638us          0.74%            12.432us         12.432us\ntranspose              0.23%            3.786us          0.68%            11.393us         11.393us\nas_strided             0.60%            10.060us         0.60%            10.060us         3.353us\nto                     0.18%            3.059us          0.44%            7.464us          7.464us\nempty_like             0.14%            2.387us          0.41%            6.859us          6.859us\nempty_strided          0.38%            6.351us          0.38%            6.351us          3.175us\nfill_                  0.28%            4.782us          0.33%            5.566us          2.783us\nexpand                 0.20%            3.336us          0.28%            4.743us          4.743us\nempty                  0.27%            4.456us          0.27%            4.456us          2.228us\ncopy_                  0.15%            2.526us          0.15%            2.526us          2.526us\nbroadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us\nsize                   0.06%            0.967us          0.06%            0.967us          0.484us\nis_complex             0.06%            0.961us          0.06%            0.961us          0.481us\nstride                 0.03%            0.517us          0.03%            0.517us          0.517us\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 1.681ms\n</code></pre>"},{"location":"dev/lightning/basic/find_bottlenecks/#profile-a-distributed-model","title":"Profile a distributed model","text":"<p>To profile a distributed model, use the <code>PyTorchProfiler</code> with the filename argument which will save a report per rank.</p> <pre><code>from lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(filename=\"perf-logs\")\ntrainer = Trainer(profiler=profiler)\n</code></pre>"},{"location":"dev/lightning/basic/find_bottlenecks/#visualize-profiled-operations","title":"Visualize profiled operations","text":"<p>To visualize the profiled operations, enable <code>emit_nvtx</code> in the <code>PyTorchProfiler</code>.</p> <pre><code>from lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(emit_nvtx=True)\ntrainer = Trainer(profiler=profiler)\n</code></pre> <p>Then run as following:</p> <pre><code>nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;\n</code></pre> <p>To visualize the profiled operation, you can either use <code>nvvp</code>:</p> <pre><code>nvvp trace_name.prof\n</code></pre> <p>or python:</p> <pre><code>python -c 'import torch; print(torch.autograd.profiler.load_nvprof(\"trace_name.prof\"))'\n</code></pre>"},{"location":"dev/lightning/basic/saving_loading_checkpoints/","title":"Saving Loading checkpoints","text":""},{"location":"dev/lightning/basic/saving_loading_checkpoints/#lightning-checkpoints","title":"Lightning checkpoints","text":""},{"location":"dev/lightning/basic/saving_loading_checkpoints/#more-details-are-here-transfer-learning","title":"More details are here: Transfer Learning","text":"<p>The new checkpoints, generated by <code>lightning</code>, are very easy to save and load:</p> <pre><code>trainer = Trainer(\n    # Save checkpoints to the `default_root_dir` directory\n    default_root_dir=\"checkpoints/acoustic\",\n    limit_train_batches=2,\n    max_epochs=1,\n    accelerator=\"cuda\",\n)\n\n# Training process...\nresult = trainer.fit(model=module, train_dataloaders=train_dataloader)\n\n# ...\n\n# Restore from the checkpoint\nacoustic_module = AcousticModule.load_from_checkpoint(\n    \"./checkpoints/am_pitche_stats.ckpt\",\n)\n\nvocoder_module = VocoderModule.load_from_checkpoint(\n    \"./checkpoints/vocoder.ckpt\",\n)\n</code></pre>"},{"location":"dev/lightning/basic/saving_loading_checkpoints/#initialize-with-other-parameters","title":"Initialize with other parameters","text":"<pre><code># if you train and save the model like this it will use these values when loading\n# the weights. But you can overwrite this\nLitModel(in_dim=32, out_dim=10)\n\n# uses in_dim=32, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH)\n\n# uses in_dim=128, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n</code></pre>"},{"location":"dev/lightning/basic/saving_loading_checkpoints/#resume-training-state","title":"Resume training state","text":"<p>If you don't just want to load weights, but instead restore the full training, do the following:</p> <pre><code>model = LitModel()\ntrainer = Trainer()\n\n# automatically restores model, epoch, step, LR schedulers, etc...\ntrainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")\n</code></pre>"},{"location":"dev/lightning/basic/visualize_experiments/","title":"Visualize Experiments","text":""},{"location":"dev/lightning/basic/visualize_experiments/#track-and-visualize-experiments","title":"Track and Visualize Experiments","text":""},{"location":"dev/lightning/basic/visualize_experiments/#more-details-are-here-track-and-visualize-experiments","title":"More details are here: Track and Visualize Experiments","text":"<p>In model development, we track values of interest such as the validation_loss to visualize the learning process for our models. Model development is like driving a car without windows, charts and logs provide the windows to know where to drive the car.</p> <p>With Lightning, you can visualize virtually anything you can think of: numbers, text, images, audio. Your creativity and imagination are the only limiting factor.</p>"},{"location":"dev/lightning/basic/visualize_experiments/#track-metrics","title":"Track metrics","text":"<p>To track a metric, simply use the <code>self.log</code> method available inside the <code>LightningModule</code></p> <pre><code>class LitModel(pl.LightningModule):\n    def training_step(self, batch, batch_idx):\n        value = ...\n        self.log(\"some_value\", value)\n</code></pre> <p>To log multiple metrics at once, use <code>self.log_dict</code></p> <pre><code>values = {\"loss\": loss, \"acc\": acc, \"metric_n\": metric_n}  # add more items if needed\nself.log_dict(values)\n</code></pre>"},{"location":"dev/lightning/basic/visualize_experiments/#view-in-the-commandline","title":"View in the commandline","text":"<p>To view metrics in the commandline progress bar, set the <code>prog_bar</code> argument to True.</p> <pre><code>self.log(..., prog_bar=True)\n</code></pre>"},{"location":"dev/lightning/basic/visualize_experiments/#view-in-the-browser","title":"View in the browser","text":"<p>By Default, <code>Lightning</code> uses <code>Tensorboard</code> (if available) and a simple CSV logger otherwise.</p> <pre><code># every trainer already has tensorboard enabled by default (if the dependency is available)\ntrainer = Trainer()\n</code></pre> <p>To launch the <code>tensorboard</code> dashboard run the following command on the commandline.</p> <pre><code>tensorboard --logdir=lightning_logs/\n</code></pre> <p>If you\u2019re using a notebook environment, launch Tensorboard with this command</p> <pre><code>%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/\n</code></pre>"},{"location":"dev/lightning/basic/visualize_experiments/#accumulate-a-metric","title":"Accumulate a metric","text":"<p>When <code>self.log</code> is called inside the <code>training_step</code>, it generates a timeseries showing how the metric behaves over time.</p> <p>When you call <code>self.log</code> inside the <code>validation_step</code> and <code>test_step</code>, Lightning automatically accumulates the metric and averages it once it\u2019s gone through the whole split (epoch).</p> <pre><code>def validation_step(self, batch, batch_idx):\n    value = batch_idx + 1\n    self.log(\"average_value\", value)\n</code></pre> <p>If you don't want to average you can also choose from <code>{min,max,sum}</code> by passing the <code>reduce_fx</code> argument.</p> <pre><code># default function\nself.log(..., reduce_fx=\"mean\")\n</code></pre>"},{"location":"dev/lightning/basic/visualize_experiments/#configure-the-saving-directory","title":"Configure the saving directory","text":"<pre><code>Trainer(default_root_dir=\"/your/custom/path\")\n</code></pre>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/","title":"Configure hyperparameters from the CLI","text":""},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#configure-hyperparameters-from-the-cli-intermediate","title":"Configure hyperparameters from the CLI (Intermediate)","text":""},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#more-details-are-here-configure-hyperparameters-from-the-cli-intermediate","title":"More details are here: Configure hyperparameters from the CLI (Intermediate)","text":""},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#lightningcli-requirements","title":"LightningCLI requirements","text":"<p>The <code>LightningCLI</code> class is designed to significantly ease the implementation of CLIs. To use this class, an additional Python requirement is necessary than the minimal installation of Lightning provides. To enable, either install all extras:</p> <pre><code>pip install \"lightning[pytorch-extra]\"\n</code></pre> <p>or if only interested in <code>LightningCLI</code>, just install <code>jsonargparse</code>:</p> <pre><code>pip install \"jsonargparse[signatures]\"\n</code></pre>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#implementing-a-cli","title":"Implementing a CLI","text":"<p>Implementing a CLI is as simple as instantiating a <code>LightningCLI</code> object giving as arguments classes for a <code>LightningModule</code> and optionally a <code>LightningDataModule</code>:</p> <pre><code># main.py\nfrom lightning.pytorch.cli import LightningCLI\n\n# simple demo classes for your convenience\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\ndef cli_main():\n    cli = LightningCLI(DemoModel, BoringDataModule)\n    # note: don't call fit!!\n\n\nif __name__ == \"__main__\":\n    cli_main()\n    # note: it is good practice to implement the CLI in a function and call it in the main if block\n</code></pre> <p>Now your model can be managed via the CLI. To see the available commands type:</p> <pre><code>python main.py --help\n</code></pre> <p>which prints out:</p> <pre><code>usage: main.py [-h] [-c CONFIG] [--print_config [={comments,skip_null,skip_default}+]]\n        {fit,validate,test,predict} ...\n\nLightning Trainer command line tool\n\noptional arguments:\n-h, --help            Show this help message and exit.\n-c CONFIG, --config CONFIG\n                        Path to a configuration file in json or yaml format.\n--print_config [={comments,skip_null,skip_default}+]\n                        Print configuration and exit.\n\nsubcommands:\nFor more details of each subcommand add it as argument followed by --help.\n\n{fit,validate,test,predict}\n    fit                 Runs the full optimization routine.\n    validate            Perform one evaluation epoch over the validation set.\n    test                Perform one evaluation epoch over the test set.\n    predict             Run inference on your data.\n</code></pre> <p>The message tells us that we have a few available subcommands:</p> <pre><code>python main.py [subcommand]\n</code></pre> <p>which you can use depending on your use case:</p> <pre><code>python main.py fit\npython main.py validate\npython main.py test\npython main.py predict\n</code></pre>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#train-a-model-with-the-cli","title":"Train a model with the CLI","text":"<p>To train a model, use the <code>fit</code> subcommand:</p> <pre><code>python main.py fit\n</code></pre> <p>View all available options with the <code>--help</code> argument given after the subcommand:</p> <pre><code>python main.py fit --help\n\nusage: main.py [options] fit [-h] [-c CONFIG]\n                            [--seed_everything SEED_EVERYTHING] [--trainer CONFIG]\n                            ...\n                            [--ckpt_path CKPT_PATH]\n    --trainer.logger LOGGER\n\noptional arguments:\n&lt;class '__main__.DemoModel'&gt;:\n    --model.out_dim OUT_DIM\n                            (type: int, default: 10)\n    --model.learning_rate LEARNING_RATE\n                            (type: float, default: 0.02)\n&lt;class 'lightning.pytorch.demos.boring_classes.BoringDataModule'&gt;:\n--data CONFIG         Path to a configuration file.\n--data.data_dir DATA_DIR\n                        (type: str, default: ./)\n</code></pre> <p>With the Lightning CLI enabled, you can now change the parameters without touching your code:</p> <pre><code># change the learning_rate\npython main.py fit --model.learning_rate 0.1\n\n# change the output dimensions also\npython main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n# change trainer and data arguments too\npython main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n</code></pre> <p>The options that become available in the CLI are the <code>__init__</code> parameters of the <code>LightningModule</code> and <code>LightningDataModule</code> classes. Thus, to make hyperparameters configurable, just add them to your class\u2019s <code>__init__</code>. It is highly recommended that these parameters are described in the docstring so that the CLI shows them in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give understandable error messages when incorrect values are given.</p>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#why-mix-models-and-datasets","title":"Why mix models and datasets","text":"<p>Lightning projects usually begin with one model and one dataset. As the project grows in complexity and you introduce more models and more datasets, it becomes desirable to mix any model with any dataset directly from the command line without changing your code.</p> <pre><code># Mix and match anything\npython main.py fit --model=GAN --data=MNIST\npython main.py fit --model=Transformer --data=MNIST\n</code></pre> <p><code>LightningCLI</code> makes this very simple. Otherwise, this kind of configuration requires a significant amount of boilerplate that often looks like this:</p> <pre><code># choose model\nif args.model == \"gan\":\n    model = GAN(args.feat_dim)\nelif args.model == \"transformer\":\n    model = Transformer(args.feat_dim)\n...\n\n# choose datamodule\nif args.data == \"MNIST\":\n    datamodule = MNIST()\nelif args.data == \"imagenet\":\n    datamodule = Imagenet()\n...\n\n# mix them!\ntrainer.fit(model, datamodule)\n</code></pre> <p>NOTE: It is highly recommended that you avoid writing this kind of boilerplate and use <code>LightningCLI</code> instead.</p>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#multiple-lightningmodules","title":"Multiple LightningModules","text":"<p>To support multiple models, when instantiating <code>LightningCLI</code> omit the <code>model_class</code> parameter:</p> <pre><code># main.py\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass Model1(DemoModel):\n    def configure_optimizers(self):\n        print(\"\u26a1\", \"using Model1\", \"\u26a1\")\n        return super().configure_optimizers()\n\n\nclass Model2(DemoModel):\n    def configure_optimizers(self):\n        print(\"\u26a1\", \"using Model2\", \"\u26a1\")\n        return super().configure_optimizers()\n\n\ncli = LightningCLI(datamodule_class=BoringDataModule)\n</code></pre> <p>Now you can choose between any model from the CLI:</p> <pre><code># use Model1\npython main.py fit --model Model1\n\n# use Model2\npython main.py fit --model Model2\n</code></pre> <p>Note: Instead of omitting the model_class parameter, you can give a base class and <code>subclass_mode_model=True</code>. This will make the CLI only accept models which are a subclass of the given base class.</p>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#multiple-lightningdatamodules","title":"Multiple LightningDataModules","text":"<p>To support multiple data modules, when instantiating <code>LightningCLI</code> omit the <code>datamodule_class</code> parameter:</p> <pre><code># main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass FakeDataset1(BoringDataModule):\n    def train_dataloader(self):\n        print(\"\u26a1\", \"using FakeDataset1\", \"\u26a1\")\n        return torch.utils.data.DataLoader(self.random_train)\n\n\nclass FakeDataset2(BoringDataModule):\n    def train_dataloader(self):\n        print(\"\u26a1\", \"using FakeDataset2\", \"\u26a1\")\n        return torch.utils.data.DataLoader(self.random_train)\n\n\ncli = LightningCLI(DemoModel)\n</code></pre> <p>Now you can choose between any dataset at runtime:</p> <pre><code># use Model1\npython main.py fit --data FakeDataset1\n\n# use Model2\npython main.py fit --data FakeDataset2\n</code></pre> <p>Instead of omitting the <code>datamodule_class</code> parameter, you can give a base class and <code>subclass_mode_data=True</code>. This will make the CLI only accept data modules that are a subclass of the given base class.</p>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#multiple-optimizers","title":"Multiple optimizers","text":"<p>Standard optimizers from <code>torch.optim</code> work out of the box:</p> <pre><code>python main.py fit --optimizer AdamW\n</code></pre> <p>If the optimizer you want needs other arguments, add them via the CLI (no need to change your code)!</p> <pre><code>python main.py fit --optimizer SGD --optimizer.lr=0.01\n</code></pre> <p>Furthermore, any custom subclass of <code>torch.optim.Optimizer</code> can be used as an optimizer:</p> <pre><code># main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass LitAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"\u26a1\", \"using LitAdam\", \"\u26a1\")\n        super().step(closure)\n\n\nclass FancyAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"\u26a1\", \"using FancyAdam\", \"\u26a1\")\n        super().step(closure)\n\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n</code></pre> <p>Now you can choose between any optimizer at runtime:</p> <pre><code># use LitAdam\npython main.py fit --optimizer LitAdam\n\n# use FancyAdam\npython main.py fit --optimizer FancyAdam\n</code></pre> <p>Maybe it's an overhead. I'm not sure about this approach.</p>"},{"location":"dev/lightning/intermediate/configure_hyperparameters_from_the_cli/#more-details-are-here-configure-hyperparameters-from-the-cli-intermediate_1","title":"More details are here: Configure hyperparameters from the CLI (Intermediate)","text":""},{"location":"dev/lightning/intermediate/customize_checkpointing_behavior/","title":"Customize checkpointing behavior","text":""},{"location":"dev/lightning/intermediate/customize_checkpointing_behavior/#customize-checkpointing-behavior","title":"Customize checkpointing behavior","text":""},{"location":"dev/lightning/intermediate/customize_checkpointing_behavior/#more-details-are-here-customize-checkpointing-behavior-intermediate","title":"More details are here: Customize checkpointing behavior (intermediate)","text":""},{"location":"dev/lightning/intermediate/customize_checkpointing_behavior/#modify-checkpointing-behavior","title":"Modify checkpointing behavior","text":"<p>For fine-grained control over checkpointing behavior, use the <code>ModelCheckpoint</code> object</p> <pre><code>from lightning.pytorch.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(dirpath=\"my/path/\", save_top_k=2, monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[checkpoint_callback])\ntrainer.fit(model)\ncheckpoint_callback.best_model_path\n</code></pre> <p>Any value that has been logged via <code>self.log</code> in the <code>LightningModule</code> can be monitored.</p> <pre><code>class LitModel(pl.LightningModule):\n    def training_step(self, batch, batch_idx):\n        self.log(\"my_metric\", x)\n\n\n# 'my_metric' is now able to be monitored\ncheckpoint_callback = ModelCheckpoint(monitor=\"my_metric\")\n</code></pre>"},{"location":"dev/lightning/intermediate/customize_checkpointing_behavior/#more-details-are-here-customize-checkpointing-behavior-intermediate_1","title":"More details are here: Customize checkpointing behavior (intermediate)","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/","title":"Deploy Models into Production","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/#deploy-models-into-production","title":"Deploy models into production","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/#more-details-are-here-deploy-models-into-production","title":"More details are here: Deploy models into production","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/#compile-your-model-to-onnx","title":"Compile your model to ONNX","text":"<p>ONNX is a package developed by Microsoft to optimize inference. ONNX allows the model to be independent of PyTorch and run on any ONNX Runtime.</p> <p>To export your model to ONNX format call the <code>to_onnx()</code> function on your <code>LightningModule</code> with the <code>filepath</code> and <code>input_sample</code>.</p> <pre><code>class SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nfilepath = \"model.onnx\"\ninput_sample = torch.randn((1, 64))\nmodel.to_onnx(filepath, input_sample, export_params=True)\n</code></pre> <p>You can also skip passing the input sample if the <code>example_input_array</code> property is specified in your <code>LightningModule</code>.</p> <pre><code>class SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n        self.example_input_array = torch.randn(7, 64)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nfilepath = \"model.onnx\"\nmodel.to_onnx(filepath, export_params=True)\n</code></pre> <p>Once you have the exported model, you can run it on your ONNX runtime in the following way:</p> <pre><code>import onnxruntime\n\nort_session = onnxruntime.InferenceSession(filepath)\ninput_name = ort_session.get_inputs()[0].name\nort_inputs = {input_name: np.random.randn(1, 64)}\nort_outs = ort_session.run(None, ort_inputs)\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#validate-a-model-is-servable","title":"Validate a Model Is Servable","text":"<p>PyTorch Lightning provides a way for you to validate a model can be served even before starting training.</p> <p>In order to do so, your <code>LightningModule</code> needs to subclass the <code>ServableModule</code>, implements its hooks and pass a <code>ServableModuleValidator</code> callback to the <code>Trainer</code>.</p> <p>Below you can find an example of how the serving of a resnet18 can be validated.</p> <pre><code>import base64\nfrom dataclasses import dataclass\nfrom io import BytesIO\nfrom os import path\nfrom typing import Dict, Optional\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom lightning.pytorch import LightningDataModule, LightningModule, cli_lightning_logo\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.serve import ServableModule, ServableModuleValidator\nfrom lightning.pytorch.utilities.model_helpers import get_torchvision_model\nfrom PIL import Image as PILImage\n\nDATASETS_PATH = path.join(path.dirname(__file__), \"..\", \"..\", \"Datasets\")\n\n\nclass LitModule(LightningModule):\n    def __init__(self, name: str = \"resnet18\"):\n        super().__init__()\n        self.model = get_torchvision_model(name, weights=\"DEFAULT\")\n        self.model.fc = torch.nn.Linear(self.model.fc.in_features, 10)\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        outputs = self.model(inputs)\n        loss = self.criterion(outputs, labels)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        outputs = self.model(inputs)\n        loss = self.criterion(outputs, labels)\n        self.log(\"val_loss\", loss)\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n\n\nclass CIFAR10DataModule(LightningDataModule):\n    transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n\n    def train_dataloader(self, *args, **kwargs):\n        trainset = torchvision.datasets.CIFAR10(root=DATASETS_PATH, train=True, download=True, transform=self.transform)\n        return torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True, num_workers=0)\n\n    def val_dataloader(self, *args, **kwargs):\n        valset = torchvision.datasets.CIFAR10(root=DATASETS_PATH, train=False, download=True, transform=self.transform)\n        return torch.utils.data.DataLoader(valset, batch_size=2, shuffle=True, num_workers=0)\n\n\n@dataclass(unsafe_hash=True)\nclass Image:\n    height: Optional[int] = None\n    width: Optional[int] = None\n    extension: str = \"JPEG\"\n    mode: str = \"RGB\"\n    channel_first: bool = False\n\n    def deserialize(self, data: str) -&gt; torch.Tensor:\n        encoded_with_padding = (data + \"===\").encode(\"UTF-8\")\n        img = base64.b64decode(encoded_with_padding)\n        buffer = BytesIO(img)\n        img = PILImage.open(buffer, mode=\"r\")\n        if self.height and self.width:\n            img = img.resize((self.width, self.height))\n        arr = np.array(img)\n        return T.ToTensor()(arr).unsqueeze(0)\n\n\nclass Top1:\n    def serialize(self, tensor: torch.Tensor) -&gt; int:\n        return torch.nn.functional.softmax(tensor).argmax().item()\n\n\nclass ProductionReadyModel(LitModule, ServableModule):\n    def configure_payload(self):\n        # 1: Access the train dataloader and load a single sample.\n        image, _ = self.trainer.train_dataloader.dataset[0]\n\n        # 2: Convert the image into a PIL Image to bytes and encode it with base64\n        pil_image = T.ToPILImage()(image)\n        buffered = BytesIO()\n        pil_image.save(buffered, format=\"JPEG\")\n        img_str = base64.b64encode(buffered.getvalue()).decode(\"UTF-8\")\n\n        return {\"body\": {\"x\": img_str}}\n\n    def configure_serialization(self):\n        return {\"x\": Image(224, 224).deserialize}, {\"output\": Top1().serialize}\n\n    def serve_step(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        return {\"output\": self.model(x)}\n\n    def configure_response(self):\n        return {\"output\": 7}\n\n\ndef cli_main():\n    cli = LightningCLI(\n        ProductionReadyModel,\n        CIFAR10DataModule,\n        seed_everything_default=42,\n        save_config_kwargs={\"overwrite\": True},\n        run=False,\n        trainer_defaults={\n            \"accelerator\": \"cpu\",\n            \"callbacks\": [ServableModuleValidator()],\n            \"max_epochs\": 1,\n            \"limit_train_batches\": 5,\n            \"limit_val_batches\": 5,\n        },\n    )\n    cli.trainer.fit(cli.model, cli.datamodule)\n\n\nif __name__ == \"__main__\":\n    cli_lightning_logo()\n    cli_main()\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#compile-your-model-to-torchscript","title":"Compile your model to TorchScript","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/#more-details-are-here-compile-your-model-to-torchscript","title":"More details are here: Compile your model to TorchScript","text":"<p>TorchScript allows you to serialize your models in a way that it can be loaded in non-Python environments. The <code>LightningModule</code> has a handy method <code>to_torchscript()</code> that returns a scripted module which you can save or directly use.</p> <pre><code>class SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nscript = model.to_torchscript()\n\n# save for use in production environment\ntorch.jit.save(script, \"model.pt\")\n</code></pre> <p>It is recommended that you install the latest supported version of <code>PyTorch</code> to use this feature without limitations. Once you have the exported model, you can run it in <code>PyTorch</code> or <code>C++</code> runtime:</p> <pre><code>inp = torch.rand(1, 64)\nscripted_module = torch.jit.load(\"model.pt\")\noutput = scripted_module(inp)\n</code></pre> <p>If you want to script a different method, you can decorate the method with <code>torch.jit.export()</code>:</p> <pre><code>class LitMCdropoutModel(pl.LightningModule):\n    def __init__(self, model, mc_iteration):\n        super().__init__()\n        self.model = model\n        self.dropout = nn.Dropout()\n        self.mc_iteration = mc_iteration\n\n    @torch.jit.export\n    def predict_step(self, batch, batch_idx):\n        # enable Monte Carlo Dropout\n        self.dropout.train()\n\n        # take average of `self.mc_iteration` iterations\n        pred = [self.dropout(self.model(x)).unsqueeze(0) for _ in range(self.mc_iteration)]\n        pred = torch.vstack(pred).mean(dim=0)\n        return pred\n\n\nmodel = LitMCdropoutModel(...)\nscript = model.to_torchscript(file_path=\"model.pt\", method=\"script\")\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#pruning-and-quantization","title":"Pruning and Quantization","text":""},{"location":"dev/lightning/intermediate/deploy_models_into_production/#more-details-are-here-pruning-and-quantization","title":"More details are here: Pruning and Quantization","text":"<p>Pruning and Quantization are techniques to compress model size for deployment, allowing inference speed up and energy saving without significant accuracy losses.</p> <p>Pruning has been shown to achieve significant efficiency improvements while minimizing the drop in model performance (prediction quality). Model pruning is recommended for cloud endpoints, deploying models on edge devices, or mobile inference (among others).</p> <p>To enable pruning during training in Lightning, simply pass in the <code>ModelPruning</code> callback to the Lightning Trainer. PyTorch's native pruning implementation is used under the hood.</p> <p>This callback supports multiple pruning functions: pass any <code>torch.nn.utils.prune</code> function as a string to select which weights to prune (<code>random_unstructured</code>, <code>RandomStructured</code>, etc) or implement your own by subclassing <code>BasePruningMethod</code>.</p> <pre><code>from lightning.pytorch.callbacks import ModelPruning\n\n# set the amount to be the fraction of parameters to prune\ntrainer = Trainer(callbacks=[ModelPruning(\"l1_unstructured\", amount=0.5)])\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#post-training-quantization","title":"Post-training Quantization","text":"<p>Most deep learning applications are using 32-bits of floating-point precision for inference. But low precision data types, especially INT8, are attracting more attention due to significant performance margin. One of the essential concerns of adopting low precision is how to easily mitigate the possible accuracy loss and reach predefined accuracy requirements.</p> <p>Intel\u00ae Neural Compressor, is an open-source Python library that runs on Intel CPUs and GPUs, which could address the aforementioned concern by extending the PyTorch Lightning model with accuracy-driven automatic quantization tuning strategies to help users quickly find out the best-quantized model on Intel hardware. It also supports multiple popular network compression technologies such as sparse, pruning, and knowledge distillation.</p>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#more-details-are-here-post-training-quantization","title":"More details are here: Post-training Quantization","text":"<p>Installation:</p> <pre><code># Install stable basic version from pip\npip install neural-compressor\n# Or install stable full version from pip (including GUI)\npip install neural-compressor-full\n</code></pre> <p>Or from conda:</p> <pre><code># install stable basic version from from conda\nconda install opencv-python-headless -c fastai\nconda install neural-compressor -c conda-forge -c intel\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#accuracy-driven-quantization-config","title":"Accuracy-driven quantization config","text":"<pre><code>from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n\naccuracy_criterion = AccuracyCriterion(tolerable_loss=0.01)\ntuning_criterion = TuningCriterion(max_trials=600)\nconf = PostTrainingQuantConfig(\n    approach=\"static\", backend=\"default\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion\n)\n</code></pre>"},{"location":"dev/lightning/intermediate/deploy_models_into_production/#quantize-the-model","title":"Quantize the model","text":"<pre><code>from neural_compressor.quantization import fit\n\nq_model = fit(model=model.model, conf=conf, calib_dataloader=val_dataloader(), eval_func=eval_func)\n</code></pre>"},{"location":"dev/lightning/intermediate/effective_training_techniques/","title":"Important! Effective Training Techniques (Acc grad,cliping)","text":""},{"location":"dev/lightning/intermediate/effective_training_techniques/#important-effective-training-techniques","title":"Important! Effective Training Techniques","text":""},{"location":"dev/lightning/intermediate/effective_training_techniques/#more-details-are-here-effective-training-techniques","title":"More details are here: Effective Training Techniques","text":"<p>Lightning implements various techniques to help during training that can help make the training smoother.</p>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#accumulate-gradients","title":"Accumulate Gradients","text":"<p>TODO: check this approach</p> <p>Accumulated gradients run <code>K</code> small batches of size <code>N</code> before doing a backward pass. The effect is a large effective batch size of size <code>KxN</code>, where <code>N</code> is the batch size. Internally it doesn\u2019t stack up the batches and do a forward pass rather it accumulates the gradients for <code>K</code> batches and then do an <code>optimizer.step</code> to make sure the effective batch size is increased but there is no memory overhead.</p> <p>WARNING: When using distributed training for eg. DDP, with let\u2019s say with <code>P</code> devices, each device accumulates independently i.e. it stores the gradients after each <code>loss.backward()</code> and doesn't sync the gradients across the devices until we call <code>optimizer.step()</code>. So for each accumulation step, the effective batch size on each device will remain <code>N*K</code> but right before the <code>optimizer.step()</code>, the gradient sync will make the effective batch size as <code>P*N*K</code>. For DP, since the batch is split across devices, the final effective batch size will be <code>N*K</code>.</p> <pre><code># DEFAULT (ie: no accumulated grads)\ntrainer = Trainer(accumulate_grad_batches=1)\n\n# Accumulate gradients for 7 batches\ntrainer = Trainer(accumulate_grad_batches=7)\n</code></pre> <p>Optionally, you can make the accumulate_grad_batches value change over time by using the <code>GradientAccumulationScheduler</code>. Pass in a scheduling dictionary, where the key represents the epoch at which the value for gradient accumulation should be updated.</p> <pre><code>from lightning.pytorch.callbacks import GradientAccumulationScheduler\n\n# till 5th epoch, it will accumulate every 8 batches. From 5th epoch\n# till 9th epoch it will accumulate every 4 batches and after that no accumulation\n# will happen. Note that you need to use zero-indexed epoch keys here\naccumulator = GradientAccumulationScheduler(scheduling={0: 8, 4: 4, 8: 1})\ntrainer = Trainer(callbacks=accumulator)\n</code></pre>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#gradient-clipping","title":"Gradient Clipping","text":"<p>Gradient clipping can be enabled to avoid exploding gradients. By default, this will clip the gradient norm by calling <code>torch.nn.utils.clip_grad_norm_()</code> computed over all model parameters together. If the Trainer's <code>gradient_clip_algorithm</code> is set to <code>'value'</code> (<code>'norm'</code> by default), this will use instead <code>torch.nn.utils.clip_grad_value_()</code> for each parameter instead.</p> <p>If using mixed precision, the <code>gradient_clip_val</code> does not need to be changed as the gradients are unscaled before applying the clipping function.</p> <pre><code># DEFAULT (ie: don't clip)\ntrainer = Trainer(gradient_clip_val=0)\n\n# clip gradients' global norm to &lt;=0.5 using gradient_clip_algorithm='norm' by default\ntrainer = Trainer(gradient_clip_val=0.5)\n\n# clip gradients' maximum magnitude to &lt;=0.5\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n</code></pre>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#stochastic-weight-averaging","title":"Stochastic Weight Averaging","text":""},{"location":"dev/lightning/intermediate/effective_training_techniques/#todo-add-this-approach","title":"TODO: add this approach","text":"<p>Stochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost. This can be used with both non-trained and trained models. The SWA procedure smooths the loss landscape thus making it harder to end up in a local minimum during optimization.</p> <p>For a more detailed explanation of SWA and how it works, read this post by the PyTorch team.</p> <p>The <code>StochasticWeightAveraging</code> callback</p> <pre><code># Enable Stochastic Weight Averaging using the callback\ntrainer = Trainer(callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)])\n</code></pre>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#batch-size-finder","title":"Batch Size Finder","text":"<p>Auto-scaling of batch size can be enabled to find the largest batch size that fits into memory. Large batch size often yields a better estimation of the gradients, but may also result in longer training time. Inspired by https://github.com/BlackHC/toma.</p> <pre><code>from lightning.pytorch.tuner import Tuner\n\n# Create a tuner for the trainer\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\n\n# Auto-scale batch size by growing it exponentially (default)\ntuner.scale_batch_size(model, mode=\"power\")\n\n# Auto-scale batch size with binary search\ntuner.scale_batch_size(model, mode=\"binsearch\")\n\n# Fit as normal with new batch size\ntrainer.fit(model)\n</code></pre> <p>Currently, this feature supports two modes <code>'power'</code> scaling and <code>'binsearch'</code> scaling. In <code>'power'</code> scaling, starting from a batch size of 1 keeps doubling the batch size until an out-of-memory (OOM) error is encountered. Setting the argument to <code>'binsearch'</code> will initially also try doubling the batch size until it encounters an OOM, after which it will do a binary search that will finetune the batch size. Additionally, it should be noted that the batch size scaler cannot search for batch sizes larger than the size of the training dataset.</p> <p>NOTE: This feature expects that a <code>batch_size</code> field is either located as a model attribute i.e. <code>model.batch_size</code> or as a field in your hparams i.e. <code>model.hparams.batch_size</code>. Similarly it can work with datamodules too. The field should exist and will be updated by the results of this algorithm. Additionally, your <code>train_dataloader()</code> method should depend on this field for this feature to work i.e.</p> <pre><code># using LightningModule\nclass LitModel(LightningModule):\n    def __init__(self, batch_size):\n        super().__init__()\n        self.save_hyperparameters()\n        # or\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(train_dataset, batch_size=self.batch_size | self.hparams.batch_size)\n\n\nmodel = LitModel(batch_size=32)\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model)\n\n\n# using LightningDataModule\nclass LitDataModule(LightningDataModule):\n    def __init__(self, batch_size):\n        super().__init__()\n        self.save_hyperparameters()\n        # or\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(train_dataset, batch_size=self.batch_size | self.hparams.batch_size)\n\n\nmodel = MyModel()\ndatamodule = LitDataModule(batch_size=32)\n\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\ntuner.scale_batch_size(model, datamodule=datamodule)\n</code></pre> <p>NOTE: Note that the <code>train_dataloader</code> can be either part of the <code>LightningModule</code> or <code>LightningDataModule</code> as shown above. If both the <code>LightningModule</code> and the <code>LightningDataModule</code> contain a <code>train_dataloader</code>, the <code>LightningDataModule</code> takes precedence.</p> <p>WARNING: Batch size finder is not yet supported for DDP or any of its variations, it is coming soon.</p>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#learning-rate-finder","title":"Learning Rate Finder","text":""},{"location":"dev/lightning/intermediate/effective_training_techniques/#todo-add-this-approach_1","title":"TODO: add this approach","text":"<p>WARNING: For the moment, this feature only works with models having a single optimizer. It means I can't implement it for the <code>univnet</code> at this moment!</p> <p>For training deep neural networks, selecting a good learning rate is essential for both better performance and faster convergence. Even optimizers such as <code>Adam</code> that are self-adjusting the learning rate can benefit from more optimal choices.</p> <p>To reduce the amount of guesswork concerning choosing a good initial learning rate, a learning rate finder can be used. As described in this paper a learning rate finder does a small run where the learning rate is increased after each processed batch and the corresponding loss is logged. The result of this is a <code>lr</code> vs. <code>loss</code> plot that can be used as guidance for choosing an optimal initial learning rate.</p> <p>Cyclical Learning Rates for Training Neural Networks</p> <p>The key ideas:</p> <p></p>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#31-cyclical-learning-rates","title":"3.1. Cyclical Learning Rates","text":"<p>The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds. Experiments with numerous functional forms, such as a triangular window (linear), a Welch window (parabolic) and a Hann window (sinusoidal) all produced equivalent results. This led to adopting a triangular window (linearly increasing then linearly decreasing), which is illustrated in Figure2, because it is the simplest function that incorporates this idea. The rest of this paper refers to this as the triangular learning rate policy.</p>"},{"location":"dev/lightning/intermediate/effective_training_techniques/#using-lightnings-built-in-lr-finder","title":"Using Lightning\u2019s built-in LR finder","text":"<p>To enable the learning rate finder, your lightning module needs to have a <code>learning_rate</code> or <code>lr</code> attribute (or as a field in your <code>hparams</code> i.e. <code>hparams.learning_rate</code> or <code>hparams.lr</code>). Then, create the <code>Tuner</code> via <code>tuner = Tuner(trainer)</code> and call <code>tuner.lr_find(model)</code> to run the LR finder. The suggested <code>learning_rate</code> will be written to the console and will be automatically set to your lightning module, which can be accessed via <code>self.learning_rate</code> or <code>self.lr</code>.</p> <pre><code>from lightning.pytorch.tuner import Tuner\n\n\nclass LitModel(LightningModule):\n    def __init__(self, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model = Model(...)\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=(self.lr or self.learning_rate))\n\n\nmodel = LitModel()\ntrainer = Trainer(...)\n\n# Create a Tuner\ntuner = Tuner(trainer)\n\n# finds learning rate automatically\n# sets hparams.lr or hparams.learning_rate to that learning rate\ntuner.lr_find(model)\n</code></pre> <p>If your model is using an arbitrary value instead of <code>self.lr</code> or <code>self.learning_rate</code>, set that value in <code>lr_find</code>:</p> <pre><code>model = LitModel()\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\n\n# to set to your own hparams.my_value\ntuner.lr_find(model, attr_name=\"my_value\")\n</code></pre> <p>You can also inspect the results of the learning rate finder or just play around with the parameters of the algorithm. A typical example of this would look like:</p> <pre><code>model = MyModelClass(hparams)\ntrainer = Trainer()\ntuner = Tuner(trainer)\n\n# Run learning rate finder\nlr_finder = tuner.lr_find(model)\n\n# Results can be found in\nprint(lr_finder.results)\n\n# Plot with\nfig = lr_finder.plot(suggest=True)\nfig.show()\n\n# Pick point based on plot, or get suggestion\nnew_lr = lr_finder.suggestion()\n\n# update hparams of the model\nmodel.hparams.lr = new_lr\n\n# Fit model\ntrainer.fit(model)\n</code></pre> <p>The figure produced by <code>lr_finder.plot()</code> should look something like the figure below. It is recommended to not pick the learning rate that achieves the lowest loss, but instead something in the middle of the sharpest downward slope (red point). This is the point returned by <code>lr_finder.suggestion()</code>.</p>"},{"location":"dev/lightning/intermediate/gpu_training/","title":"GPU training","text":""},{"location":"dev/lightning/intermediate/gpu_training/#gpu-training","title":"GPU training","text":""},{"location":"dev/lightning/intermediate/gpu_training/#more-details-are-here-gpu-training-basic","title":"More details are here: GPU training (Basic)","text":"<p>The Trainer will run on all available GPUs by default. Make sure you\u2019re running on a machine with at least one GPU. There's no need to specify any NVIDIA flags as Lightning will do it for you.</p> <pre><code># run on as many GPUs as available by default\ntrainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n# equivalent to\ntrainer = Trainer()\n\n# run on one GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n# run on multiple GPUs\ntrainer = Trainer(accelerator=\"gpu\", devices=8)\n# choose the number of devices automatically\ntrainer = Trainer(accelerator=\"gpu\", devices=\"auto\")\n</code></pre>"},{"location":"dev/lightning/intermediate/gpu_training/#choosing-gpu-devices","title":"Choosing GPU devices","text":"<pre><code># DEFAULT (int) specifies how many GPUs to use per node\nTrainer(accelerator=\"gpu\", devices=k)\n\n# Above is equivalent to\nTrainer(accelerator=\"gpu\", devices=list(range(k)))\n\n# Specify which GPUs to use (don't use when running on cluster)\nTrainer(accelerator=\"gpu\", devices=[0, 1])\n\n# Equivalent using a string\nTrainer(accelerator=\"gpu\", devices=\"0, 1\")\n\n# To use all available GPUs put -1 or '-1'\n# equivalent to `list(range(torch.cuda.device_count())) and `\"auto\"`\nTrainer(accelerator=\"gpu\", devices=-1)\n</code></pre>"},{"location":"dev/lightning/intermediate/gpu_training/#find-usable-cuda-devices","title":"Find usable CUDA devices","text":"<pre><code>from lightning.pytorch.accelerators import find_usable_cuda_devices\n\n# Find two GPUs on the system that are not already occupied\ntrainer = Trainer(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n\nfrom lightning.fabric.accelerators import find_usable_cuda_devices\n\n# Works with Fabric too\nfabric = Fabric(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n</code></pre>"},{"location":"dev/lightning/intermediate/hardware_agnostic_training/","title":"Hardware-Agnostic training","text":""},{"location":"dev/lightning/intermediate/hardware_agnostic_training/#hardware-agnostic-training-preparation","title":"Hardware agnostic training (preparation)","text":""},{"location":"dev/lightning/intermediate/hardware_agnostic_training/#more-details-are-here-hardware-agnostic-training-preparation","title":"More details are here: Hardware agnostic training (preparation)","text":""},{"location":"dev/lightning/intermediate/hardware_agnostic_training/#delete-cuda-or-to-calls","title":"Delete <code>.cuda()</code> or <code>.to()</code> calls","text":"<pre><code># before lightning\ndef forward(self, x):\n    x = x.cuda(0)\n    layer_1.cuda(0)\n    x_hat = layer_1(x)\n\n\n# after lightning\ndef forward(self, x):\n    x_hat = layer_1(x)\n</code></pre> <p>Example from the code:</p> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    pitches_range: Tuple[float, float],\n    speakers: torch.Tensor,\n    langs: torch.Tensor,\n    p_control: float = 1.0,\n    d_control: float = 1.0,\n) -&gt; torch.Tensor:\n    # Generate masks for padding positions in the source sequences\n    src_mask = tools.get_mask_from_lengths(\n        torch.tensor([x.shape[1]], dtype=torch.int64),\n    ).to(x.device) # Read the device from the input `x.device`\n</code></pre>"},{"location":"dev/lightning/intermediate/hardware_agnostic_training/#synchronize-validation-and-test-logging","title":"Synchronize validation and test logging","text":"<p>When running in distributed mode, we have to ensure that the validation and test step logging calls are synchronized across processes. This is done by adding <code>sync_dist=True</code> to all <code>self.log</code> calls in the validation and test step. This ensures that each GPU worker has the same behaviour when tracking model checkpoints, which is important for later downstream tasks such as testing the best checkpoint across all workers. The <code>sync_dist</code> option can also be used in logging calls during the step methods, but be aware that this can lead to significant communication overhead and slow down your training.</p> <pre><code>def validation_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = self.loss(logits, y)\n    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n    self.log(\"validation_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n\n\ndef test_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = self.loss(logits, y)\n    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n    self.log(\"test_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n</code></pre>"},{"location":"dev/lightning/intermediate/lightning_data_module/","title":"Lightning Data Module","text":""},{"location":"dev/lightning/intermediate/lightning_data_module/#lightning-data-module","title":"Lightning Data Module","text":""},{"location":"dev/lightning/intermediate/lightning_data_module/#more-details-are-here-lightningdatamodule","title":"More details are here: LightningDataModule","text":"<p>A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data</p> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n    def setup(self, stage: str):\n        self.mnist_test = MNIST(self.data_dir, train=False)\n        self.mnist_predict = MNIST(self.data_dir, train=False)\n        mnist_full = MNIST(self.data_dir, train=True)\n        self.mnist_train, self.mnist_val = random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n\n    def teardown(self, stage: str):\n        # Used to clean-up when the run is finished\n        ...\n</code></pre> <p>Not sure about this module, I use the <code>from torch.utils.data import Dataset</code>, because it's much easy to transfer from dunky11 code.</p> <p>Maybe for the next datasets it makes sense to try the new way to build the dataset with <code>LightningDataModule</code></p>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/","title":"SOTA scaling techniques (N-Bit Precision)","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#sota-scaling-techniques","title":"SOTA scaling techniques","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#more-details-are-here-track-and-visualize-experiments","title":"More details are here: Track and Visualize Experiments","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#n-bit-precision","title":"N-Bit Precision","text":"<p>If you\u2019re looking to run models faster or consume less memory, consider tweaking the precision settings of your models.</p> <p>Lower precision, such as 16-bit floating-point, requires less memory and enables training and deploying larger models. Higher precision, such as the 64-bit floating-point, can be used for highly sensitive use-cases.</p>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#16-bit-precision","title":"16-bit Precision","text":"<p>Use 16-bit mixed precision to speed up training and inference. If your GPUs are [Tensor Core] GPUs, you can expect a ~3x speed improvement.</p> <pre><code>Trainer(precision=\"16-mixed\")\n</code></pre> <p>In most cases, mixed precision uses FP16. Supported PyTorch operations automatically run in FP16, saving memory and improving throughput on the supported accelerators. Since computation happens in FP16, which has a very limited \u201cdynamic range\u201d, there is a chance of numerical instability during training. This is handled internally by a dynamic grad scaler which skips invalid steps and adjusts the scaler to ensure subsequent steps fall within a finite range. For more information see the autocast docs.</p> <p>With true 16-bit precision you can additionally lower your memory consumption by up to half so that you can train and deploy larger models. However, this setting can sometimes lead to unstable training.</p> <pre><code>Trainer(precision=\"16-true\")\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#32-bit-precision","title":"32-bit Precision","text":"<p>32-bit precision is the default used across all models and research. This precision is known to be stable in contrast to lower precision settings.</p> <pre><code>Trainer(precision=\"32-true\")\n\n# or (legacy)\nTrainer(precision=\"32\")\n\n# or (legacy)\nTrainer(precision=32)\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#64-bit-precision","title":"64-bit Precision","text":"<p>For certain scientific computations, 64-bit precision enables more accurate models. However, doubling the precision from 32 to 64 bit also doubles the memory requirements.</p> <pre><code>Trainer(precision=\"64-true\")\n\n# or (legacy)\nTrainer(precision=\"64\")\n\n# or (legacy)\nTrainer(precision=64)\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#more-details-are-here-track-and-visualize-experiments_1","title":"More details are here: Track and Visualize Experiments","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#n-bit-precision-intermediate","title":"N-Bit Precision (Intermediate)","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#more-details-are-here-n-bit-precision-intermediate","title":"More details are here: N-Bit Precision (Intermediate)","text":"<p>PyTorch, like most deep learning frameworks, trains on 32-bit floating-point (FP32) arithmetic by default. However, many deep learning models do not require this to reach complete accuracy. By conducting operations in half-precision format while keeping minimum information in single-precision to maintain as much information as possible in crucial areas of the network, mixed precision training delivers significant computational speedup. Switching to mixed precision has resulted in considerable training speedups since the introduction of Tensor Cores in the Volta and Turing architectures. It combines FP32 and lower-bit floating-points (such as FP16) to reduce memory footprint and increase performance during model training and evaluation. It accomplishes this by recognizing the steps that require complete accuracy and employing a 32-bit floating-point for those steps only, while using a 16-bit floating-point for the rest. When compared to complete precision training, mixed precision training delivers all of these benefits while ensuring that no task-specific accuracy is lost.</p>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#bfloat16-mixed-precision","title":"BFloat16 Mixed Precision","text":"<p>BFloat16 may not provide significant speedups or memory improvements or offer better numerical stability. For GPUs, the most significant benefits require Ampere based GPUs or newer, such as A100s or 3090s.</p> <pre><code>Trainer(accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\")\n</code></pre> <p>It is also possible to use BFloat16 mixed precision on the CPU, relying on MKLDNN under the hood.</p> <pre><code>Trainer(precision=\"bf16-mixed\")\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#true-half-precision","title":"True Half Precision","text":"<pre><code># Select FP16 precision\ntrainer = Trainer(precision=\"16-true\")\ntrainer.fit(model)  # model gets cast to torch.float16\n\n# Select BF16 precision\ntrainer = Trainer(precision=\"bf16-true\")\ntrainer.fit(model)  # model gets cast to torch.bfloat16\n</code></pre> <p>Tip: For faster initialization, you can create model parameters with the desired dtype directly on the device:</p> <pre><code>trainer = Trainer(precision=\"bf16-true\")\n\n# init the model directly on the device and with parameters in half-precision\nwith trainer.init_module():\n    model = MyModel()\n\ntrainer.fit(model)\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#efficient-initialization-advanced","title":"Efficient initialization (Advanced)","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#more-details-are-here-efficient-initialization","title":"More details are here: Efficient initialization","text":""},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#half-precision","title":"Half-precision","text":"<p>Instantiating a nn.Module in PyTorch creates all parameters on CPU in float32 precision by default. To speed up initialization, you can force PyTorch to create the model directly on the target device and with the desired precision without changing your model code.</p> <pre><code>trainer = Trainer(accelerator=\"cuda\", precision=\"16-true\")\n\nwith trainer.init_module():\n    # models created here will be on GPU and in float16\n    model = MyLightningModule()\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#loading-checkpoints-for-inference-or-finetuning","title":"Loading checkpoints for inference or finetuning","text":"<pre><code>with trainer.init_module(empty_init=True):\n    # creation of the model is fast\n    # and depending on the strategy allocates no memory, or uninitialized memory\n    model = MyLightningModule.load_from_checkpoint(\"my/checkpoint/path.ckpt\")\n\ntrainer.fit(model)\n</code></pre>"},{"location":"dev/lightning/intermediate/sota_scaling_techniques/#float8-mixed-precision-via-nvidias-transformerengine","title":"Float8 Mixed Precision via Nvidia\u2019s TransformerEngine","text":"<pre><code># Select 8bit mixed precision via TransformerEngine, with model weights in bfloat16\ntrainer = Trainer(precision=\"transformer-engine\")\n\n# Select 8bit mixed precision via TransformerEngine, with model weights in float16\ntrainer = Trainer(precision=\"transformer-engine-float16\")\n\n# Customize the fp8 recipe or set a different base precision:\nfrom lightning.trainer.plugins import TransformerEnginePrecision\n\nrecipe = {\"fp8_format\": \"HYBRID\", \"amax_history_len\": 16, \"amax_compute_algo\": \"max\"}\nprecision = TransformerEnginePrecision(dtype=torch.bfloat16, recipe=recipe)\ntrainer = Trainer(plugins=precision)\n</code></pre> <p>This requires Hopper based GPUs or newer, such the H100.</p>"},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/","title":"Track and Visualize Experiments","text":""},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#track-and-visualize-experiments","title":"Track and Visualize Experiments","text":""},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#more-details-are-here-track-and-visualize-experiments","title":"More details are here: Track and Visualize Experiments","text":""},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#track-audio-and-other-artifacts","title":"Track audio and other artifacts","text":"<p>To track other artifacts, such as histograms or model topology graphs first select one of the many loggers supported by Lightning</p> <pre><code>from lightning.pytorch import loggers as pl_loggers\n\ntensorboard = pl_loggers.TensorBoardLogger(save_dir=\"\")\ntrainer = Trainer(logger=tensorboard)\n</code></pre> <p>then access the logger\u2019s API directly</p> <pre><code>def training_step(self):\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n</code></pre>"},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#more-details-are-here-track-and-visualize-experiments_1","title":"More details are here: Track and Visualize Experiments","text":"<p>Libs:</p>"},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#mlflow","title":"mlflow","text":""},{"location":"dev/lightning/intermediate/track_and_visualize_experiments/#comet","title":"comet","text":""},{"location":"experiments/conv_leaky_relu/","title":"Conv Leaky ReLU","text":""},{"location":"experiments/conv_leaky_relu/#notebooks.experiments.conv_leaky_relu.ConvLeakyReLU","title":"<code>ConvLeakyReLU</code>","text":"<p>             Bases: <code>Module</code></p> <p>Class implements a Convolution followed by a Leaky ReLU activation layer.</p> <p>Attributes     layers (nn.Sequential): Sequential container that holds the Convolution and LeakyReLU layers.</p> <p>Methods forward(x: torch.Tensor) -&gt; torch.Tensor     Passes the input through the Conv1d and LeakyReLU layers.</p> Source code in <code>notebooks/experiments/conv_leaky_relu.py</code> <pre><code>class ConvLeakyReLU(Module):\n    r\"\"\"Class implements a Convolution followed by a Leaky ReLU activation layer.\n\n    Attributes\n        layers (nn.Sequential): Sequential container that holds the Convolution and LeakyReLU layers.\n\n    Methods\n    forward(x: torch.Tensor) -&gt; torch.Tensor\n        Passes the input through the Conv1d and LeakyReLU layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        padding: int,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        r\"\"\"Args:\n        in_channels (int): The number of channels in the input data. This could refer to different color channels (like RGB in an image) or different input features in a dataset.\n\n        out_channels (int): The number of channels in the output data. This typically corresponds to the number of filters applied on the input.\n\n        kernel_size (int): The size of the convolving kernel used in the convolution operation. This is usually an odd integer.\n\n        padding (int): The number of zero-padding pixels added on each side of the input data. This is used to control the spatial dimensions of the output data.\n\n        leaky_relu_slope (float, default=LEAKY_RELU_SLOPE): The slope of the function for negative values in a Leaky ReLU activation function. This controls the amount of \"leakiness\" or the degree to which the function allows negative values to pass through.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv1d(\n                in_channels, out_channels, kernel_size=kernel_size, padding=padding,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Defines the forward pass of the ConvLeakyReLU.\n\n        Args:\n        x (torch.Tensor): The input tensor.\n\n        Returns:\n        torch.Tensor: The output tensor after being passed through the Conv1d and LeakyReLU layers.\n        \"\"\"\n        return self.layers(x)\n</code></pre>"},{"location":"experiments/conv_leaky_relu/#notebooks.experiments.conv_leaky_relu.ConvLeakyReLU.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, padding, leaky_relu_slope=LEAKY_RELU_SLOPE)</code>","text":"<p>Args: in_channels (int): The number of channels in the input data. This could refer to different color channels (like RGB in an image) or different input features in a dataset.</p> <p>out_channels (int): The number of channels in the output data. This typically corresponds to the number of filters applied on the input.</p> <p>kernel_size (int): The size of the convolving kernel used in the convolution operation. This is usually an odd integer.</p> <p>padding (int): The number of zero-padding pixels added on each side of the input data. This is used to control the spatial dimensions of the output data.</p> <p>leaky_relu_slope (float, default=LEAKY_RELU_SLOPE): The slope of the function for negative values in a Leaky ReLU activation function. This controls the amount of \"leakiness\" or the degree to which the function allows negative values to pass through.</p> Source code in <code>notebooks/experiments/conv_leaky_relu.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    padding: int,\n    leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n):\n    r\"\"\"Args:\n    in_channels (int): The number of channels in the input data. This could refer to different color channels (like RGB in an image) or different input features in a dataset.\n\n    out_channels (int): The number of channels in the output data. This typically corresponds to the number of filters applied on the input.\n\n    kernel_size (int): The size of the convolving kernel used in the convolution operation. This is usually an odd integer.\n\n    padding (int): The number of zero-padding pixels added on each side of the input data. This is used to control the spatial dimensions of the output data.\n\n    leaky_relu_slope (float, default=LEAKY_RELU_SLOPE): The slope of the function for negative values in a Leaky ReLU activation function. This controls the amount of \"leakiness\" or the degree to which the function allows negative values to pass through.\n    \"\"\"\n    super().__init__()\n    self.layers = nn.Sequential(\n        nn.Conv1d(\n            in_channels, out_channels, kernel_size=kernel_size, padding=padding,\n        ),\n        nn.LeakyReLU(leaky_relu_slope),\n    )\n</code></pre>"},{"location":"experiments/conv_leaky_relu/#notebooks.experiments.conv_leaky_relu.ConvLeakyReLU.forward","title":"<code>forward(x)</code>","text":"<p>Defines the forward pass of the ConvLeakyReLU.</p> <p>Args: x (torch.Tensor): The input tensor.</p> <p>Returns: torch.Tensor: The output tensor after being passed through the Conv1d and LeakyReLU layers.</p> Source code in <code>notebooks/experiments/conv_leaky_relu.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Defines the forward pass of the ConvLeakyReLU.\n\n    Args:\n    x (torch.Tensor): The input tensor.\n\n    Returns:\n    torch.Tensor: The output tensor after being passed through the Conv1d and LeakyReLU layers.\n    \"\"\"\n    return self.layers(x)\n</code></pre>"},{"location":"experiments/mas_torch/","title":"MAS torch","text":""},{"location":"experiments/readme/","title":"References","text":""},{"location":"experiments/readme/#references","title":"References","text":"<p>Experiments for the project. Here you can find some ideas, that prepared or failed during the development process</p>"},{"location":"experiments/readme/#conv-leaky-relu","title":"Conv Leaky Relu","text":""},{"location":"experiments/readme/#mas-torch","title":"MAS torch","text":""},{"location":"experiments/readme/#tokenization-bert-based-with-custom-vocab","title":"Tokenization (BERT based with custom vocab)","text":""},{"location":"experiments/tokenization/","title":"Tokenization","text":""},{"location":"experiments/tokenization/#notebooks.experiments.tokenization.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>A wrapper class for the BERT tokenizer from the Hugging Face Transformers library. Use this with <code>vocab_file</code> and it makes sure that the correct vocabulary is used.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>str</code> <p>The name or path of the pre-trained BERT checkpoint to use.</p> <code>'bert-base-uncased'</code> <code>vocab_file</code> <code>str</code> <p>The path to the custom vocabulary file to use (optional).</p> <code>'config/vocab.txt'</code> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>BertTokenizer</code> <p>The BERT tokenizer object.</p> Source code in <code>notebooks/experiments/tokenization.py</code> <pre><code>class Tokenizer:\n    r\"\"\"A wrapper class for the BERT tokenizer from the Hugging Face Transformers library.\n    Use this with `vocab_file` and it makes sure that the correct vocabulary is used.\n\n    Args:\n        checkpoint (str): The name or path of the pre-trained BERT checkpoint to use.\n        vocab_file (str): The path to the custom vocabulary file to use (optional).\n\n    Attributes:\n        tokenizer (BertTokenizer): The BERT tokenizer object.\n\n    \"\"\"\n\n    def __init__(self, checkpoint: str = \"bert-base-uncased\", vocab_file: str = \"config/vocab.txt\") -&gt; None:\n        r\"\"\"Initializes the Tokenizer object with the specified checkpoint and vocabulary file.\n\n        Args:\n            checkpoint (str): The name or path of the pre-trained BERT checkpoint to use.\n            vocab_file (str): The path to the custom vocabulary file to use (optional).\n\n        Returns:\n            None.\n\n        \"\"\"\n        self.tokenizer = BertTokenizer.from_pretrained(checkpoint, vocab_file=vocab_file)\n\n    def __call__(self, text: Union[str, List[str]], add_special_tokens: bool = True) -&gt; list[int]:\n        r\"\"\"Tokenizes the input text using the BERT tokenizer.\n\n        Args:\n            text (str): The input text to tokenize.\n            add_special_tokens (bool): Whether to add special tokens to the tokenized text (optional).\n\n        Returns:\n            tokens (List[int]): A list of token IDs representing the tokenized text.\n\n        \"\"\"\n        tokens = self.tokenizer.encode(text, add_special_tokens=add_special_tokens)\n        return tokens\n\n    def decode(self, tokens: list[int], skip_special_tokens: bool = True) -&gt; list[str]:\n        r\"\"\"Decodes the input token IDs into a list of strings.\n\n        Args:\n            tokens (List[int]): A list of token IDs to decode.\n            skip_special_tokens (bool): Whether to add special tokens to the tokenized text (optional).\n\n        Returns:\n            text (List[str]): A list of strings representing the decoded tokens.\n\n        \"\"\"\n        text_list = self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n        return text_list\n</code></pre>"},{"location":"experiments/tokenization/#notebooks.experiments.tokenization.Tokenizer.__call__","title":"<code>__call__(text, add_special_tokens=True)</code>","text":"<p>Tokenizes the input text using the BERT tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to tokenize.</p> required <code>add_special_tokens</code> <code>bool</code> <p>Whether to add special tokens to the tokenized text (optional).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tokens</code> <code>List[int]</code> <p>A list of token IDs representing the tokenized text.</p> Source code in <code>notebooks/experiments/tokenization.py</code> <pre><code>def __call__(self, text: Union[str, List[str]], add_special_tokens: bool = True) -&gt; list[int]:\n    r\"\"\"Tokenizes the input text using the BERT tokenizer.\n\n    Args:\n        text (str): The input text to tokenize.\n        add_special_tokens (bool): Whether to add special tokens to the tokenized text (optional).\n\n    Returns:\n        tokens (List[int]): A list of token IDs representing the tokenized text.\n\n    \"\"\"\n    tokens = self.tokenizer.encode(text, add_special_tokens=add_special_tokens)\n    return tokens\n</code></pre>"},{"location":"experiments/tokenization/#notebooks.experiments.tokenization.Tokenizer.__init__","title":"<code>__init__(checkpoint='bert-base-uncased', vocab_file='config/vocab.txt')</code>","text":"<p>Initializes the Tokenizer object with the specified checkpoint and vocabulary file.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>str</code> <p>The name or path of the pre-trained BERT checkpoint to use.</p> <code>'bert-base-uncased'</code> <code>vocab_file</code> <code>str</code> <p>The path to the custom vocabulary file to use (optional).</p> <code>'config/vocab.txt'</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>notebooks/experiments/tokenization.py</code> <pre><code>def __init__(self, checkpoint: str = \"bert-base-uncased\", vocab_file: str = \"config/vocab.txt\") -&gt; None:\n    r\"\"\"Initializes the Tokenizer object with the specified checkpoint and vocabulary file.\n\n    Args:\n        checkpoint (str): The name or path of the pre-trained BERT checkpoint to use.\n        vocab_file (str): The path to the custom vocabulary file to use (optional).\n\n    Returns:\n        None.\n\n    \"\"\"\n    self.tokenizer = BertTokenizer.from_pretrained(checkpoint, vocab_file=vocab_file)\n</code></pre>"},{"location":"experiments/tokenization/#notebooks.experiments.tokenization.Tokenizer.decode","title":"<code>decode(tokens, skip_special_tokens=True)</code>","text":"<p>Decodes the input token IDs into a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>A list of token IDs to decode.</p> required <code>skip_special_tokens</code> <code>bool</code> <p>Whether to add special tokens to the tokenized text (optional).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings representing the decoded tokens.</p> Source code in <code>notebooks/experiments/tokenization.py</code> <pre><code>def decode(self, tokens: list[int], skip_special_tokens: bool = True) -&gt; list[str]:\n    r\"\"\"Decodes the input token IDs into a list of strings.\n\n    Args:\n        tokens (List[int]): A list of token IDs to decode.\n        skip_special_tokens (bool): Whether to add special tokens to the tokenized text (optional).\n\n    Returns:\n        text (List[str]): A list of strings representing the decoded tokens.\n\n    \"\"\"\n    text_list = self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n    return text_list\n</code></pre>"},{"location":"experiments/optimizer/readme/","title":"References","text":""},{"location":"experiments/optimizer/readme/#references","title":"References","text":"<p>Here you can find optimizer code docs.</p>"},{"location":"experiments/optimizer/readme/#scheduled-optim-pretraining","title":"Scheduled Optim Pretraining","text":"<p>ScheduledOptimPretraining optimizer.</p>"},{"location":"experiments/optimizer/readme/#scheduled-optim-finetuning","title":"Scheduled Optim Finetuning","text":"<p>ScheduledOptimFinetuning optimizer</p>"},{"location":"experiments/optimizer/scheduled_optim_finetuning/","title":"Scheduled Optim Finetuning","text":""},{"location":"experiments/optimizer/scheduled_optim_finetuning/#notebooks.experiments.optimizer.scheduled_optim_finetuning.ScheduledOptimFinetuning","title":"<code>ScheduledOptimFinetuning</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>DEPRECATED: moved to AcousticModule. A custom optimizer that uses <code>AdamW</code> for optimization and an <code>ExponentialLR</code> for learning rate scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>train_config</code> <code>AcousticTrainingConfig</code> <p>Training configuration with optimizer and scheduler parameters.</p> required <code>parameters</code> <code>Iterable</code> <p>Iterable of parameters to optimize.</p> required <code>defaults</code> <code>Dict[str, Any]</code> <p>Default optimization options. Defaults to an empty dictionary.</p> <code>{}</code> <code>step</code> <code>Optional[int]</code> <p>The current training step. Defaults to None.</p> <code>None</code> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_finetuning.py</code> <pre><code>class ScheduledOptimFinetuning(Optimizer):\n    r\"\"\"DEPRECATED: moved to AcousticModule.\n    A custom optimizer that uses `AdamW` for optimization and an `ExponentialLR` for learning rate scheduling.\n\n    Args:\n        train_config (AcousticTrainingConfig): Training configuration with optimizer and scheduler parameters.\n        parameters (Iterable): Iterable of parameters to optimize.\n        defaults (Dict[str, Any]): Default optimization options. Defaults to an empty dictionary.\n        step (Optional[int]): The current training step. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_config: AcousticTrainingConfig,\n        parameters: Iterable,\n        defaults: Dict[str, Any] = {},\n        step: Optional[int] = None,\n    ):\n        super().__init__(params=parameters, defaults=defaults)\n\n        # Compute the gamma and initial learning rate based on the current step\n        lr_decay = train_config.optimizer_config.lr_decay\n        default_lr = train_config.optimizer_config.learning_rate\n\n        init_lr = default_lr if step is None else default_lr * (lr_decay ** step)\n\n        self._optimizer = torch.optim.AdamW(\n            parameters,\n            betas=train_config.optimizer_config.betas,\n            eps=train_config.optimizer_config.eps,\n            lr=init_lr,\n        )\n\n        self._scheduler = ExponentialLR(self._optimizer, gamma=lr_decay)\n\n    def step(self, closure):\n        r\"\"\"Performs a single optimization step.\"\"\"\n        self._optimizer.step(closure)\n        self._scheduler.step()\n\n    def zero_grad(self) -&gt; None:\n        r\"\"\"Clears the gradients of all optimized parameters.\n        This should be called before the backward pass in PyTorch.\n        \"\"\"\n        self._optimizer.zero_grad()\n\n    def get_lr(self) -&gt; float:\n        r\"\"\"Returns the current learning rate.\"\"\"\n        return self._optimizer.param_groups[0][\"lr\"]\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        r\"\"\"Loads the optimizer state.\n\n        Args:\n        state_dict (Dict[str, Any]): A dictionary containing a whole state of the optimizer.\n        \"\"\"\n        self._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_finetuning/#notebooks.experiments.optimizer.scheduled_optim_finetuning.ScheduledOptimFinetuning.get_lr","title":"<code>get_lr()</code>","text":"<p>Returns the current learning rate.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_finetuning.py</code> <pre><code>def get_lr(self) -&gt; float:\n    r\"\"\"Returns the current learning rate.\"\"\"\n    return self._optimizer.param_groups[0][\"lr\"]\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_finetuning/#notebooks.experiments.optimizer.scheduled_optim_finetuning.ScheduledOptimFinetuning.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Loads the optimizer state.</p> <p>Args: state_dict (Dict[str, Any]): A dictionary containing a whole state of the optimizer.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_finetuning.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    r\"\"\"Loads the optimizer state.\n\n    Args:\n    state_dict (Dict[str, Any]): A dictionary containing a whole state of the optimizer.\n    \"\"\"\n    self._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_finetuning/#notebooks.experiments.optimizer.scheduled_optim_finetuning.ScheduledOptimFinetuning.step","title":"<code>step(closure)</code>","text":"<p>Performs a single optimization step.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_finetuning.py</code> <pre><code>def step(self, closure):\n    r\"\"\"Performs a single optimization step.\"\"\"\n    self._optimizer.step(closure)\n    self._scheduler.step()\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_finetuning/#notebooks.experiments.optimizer.scheduled_optim_finetuning.ScheduledOptimFinetuning.zero_grad","title":"<code>zero_grad()</code>","text":"<p>Clears the gradients of all optimized parameters. This should be called before the backward pass in PyTorch.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_finetuning.py</code> <pre><code>def zero_grad(self) -&gt; None:\n    r\"\"\"Clears the gradients of all optimized parameters.\n    This should be called before the backward pass in PyTorch.\n    \"\"\"\n    self._optimizer.zero_grad()\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/","title":"Scheduled Optim Pretraining","text":""},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining","title":"<code>ScheduledOptimPretraining</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>DEPRECATED: moved to AcousticModule. A custom optimizer that uses <code>AdamW</code> for optimization and an <code>LambdaLR</code> for learning rate scheduling.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>class ScheduledOptimPretraining(Optimizer):\n    r\"\"\"DEPRECATED: moved to AcousticModule.\n    A custom optimizer that uses `AdamW` for optimization and an `LambdaLR` for learning rate scheduling.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_config: AcousticTrainingConfig,\n        model_config: AcousticModelConfigType,\n        parameters: Iterable,\n        defaults: Dict[str, Any] = {},\n        step: int = 0,\n    ):\n        r\"\"\"Initializes the ScheduledOptimPretraining optimizer.\n\n        Args:\n            train_config (AcousticPretrainingConfig): The training configuration.\n            model_config (AcousticModelConfigType): The model configuration.\n            parameters (Iterable): The model parameters to optimize.\n            defaults (Dict[str, Any]): Default optimization options. Defaults to an empty dictionary.\n            step (int): The current training step. Defaults to None.\n        \"\"\"\n        super().__init__(params=parameters, defaults=defaults)\n\n        init_lr, lr_lambda = get_lr_lambda(\n            model_config=model_config,\n            train_config=train_config,\n            current_step=step,\n        )\n\n        self._optimizer = torch.optim.Adam(\n            parameters,\n            betas=train_config.optimizer_config.betas,\n            eps=train_config.optimizer_config.eps,\n            lr=init_lr,\n        )\n\n        self._scheduler = LambdaLR(self._optimizer, lr_lambda)\n\n    def step(self, closure):\n        r\"\"\"Performs a single optimization step.\"\"\"\n        self._optimizer.step(closure)\n        self._scheduler.step()\n\n    def zero_grad(self) -&gt; None:\n        r\"\"\"Zeroes the gradients of the optimizer.\"\"\"\n        self._optimizer.zero_grad()\n\n    def get_lr(self) -&gt; float:\n        r\"\"\"Returns the current learning rate.\"\"\"\n        return self._optimizer.param_groups[0][\"lr\"]\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        r\"\"\"Loads the optimizer state dictionary.\n\n        Args:\n            state_dict (Dict[str, Any]): The optimizer state dictionary.\n        \"\"\"\n        self._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining.__init__","title":"<code>__init__(train_config, model_config, parameters, defaults={}, step=0)</code>","text":"<p>Initializes the ScheduledOptimPretraining optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>train_config</code> <code>AcousticPretrainingConfig</code> <p>The training configuration.</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>The model configuration.</p> required <code>parameters</code> <code>Iterable</code> <p>The model parameters to optimize.</p> required <code>defaults</code> <code>Dict[str, Any]</code> <p>Default optimization options. Defaults to an empty dictionary.</p> <code>{}</code> <code>step</code> <code>int</code> <p>The current training step. Defaults to None.</p> <code>0</code> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def __init__(\n    self,\n    train_config: AcousticTrainingConfig,\n    model_config: AcousticModelConfigType,\n    parameters: Iterable,\n    defaults: Dict[str, Any] = {},\n    step: int = 0,\n):\n    r\"\"\"Initializes the ScheduledOptimPretraining optimizer.\n\n    Args:\n        train_config (AcousticPretrainingConfig): The training configuration.\n        model_config (AcousticModelConfigType): The model configuration.\n        parameters (Iterable): The model parameters to optimize.\n        defaults (Dict[str, Any]): Default optimization options. Defaults to an empty dictionary.\n        step (int): The current training step. Defaults to None.\n    \"\"\"\n    super().__init__(params=parameters, defaults=defaults)\n\n    init_lr, lr_lambda = get_lr_lambda(\n        model_config=model_config,\n        train_config=train_config,\n        current_step=step,\n    )\n\n    self._optimizer = torch.optim.Adam(\n        parameters,\n        betas=train_config.optimizer_config.betas,\n        eps=train_config.optimizer_config.eps,\n        lr=init_lr,\n    )\n\n    self._scheduler = LambdaLR(self._optimizer, lr_lambda)\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining.get_lr","title":"<code>get_lr()</code>","text":"<p>Returns the current learning rate.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def get_lr(self) -&gt; float:\n    r\"\"\"Returns the current learning rate.\"\"\"\n    return self._optimizer.param_groups[0][\"lr\"]\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Loads the optimizer state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The optimizer state dictionary.</p> required Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    r\"\"\"Loads the optimizer state dictionary.\n\n    Args:\n        state_dict (Dict[str, Any]): The optimizer state dictionary.\n    \"\"\"\n    self._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining.step","title":"<code>step(closure)</code>","text":"<p>Performs a single optimization step.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def step(self, closure):\n    r\"\"\"Performs a single optimization step.\"\"\"\n    self._optimizer.step(closure)\n    self._scheduler.step()\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.ScheduledOptimPretraining.zero_grad","title":"<code>zero_grad()</code>","text":"<p>Zeroes the gradients of the optimizer.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def zero_grad(self) -&gt; None:\n    r\"\"\"Zeroes the gradients of the optimizer.\"\"\"\n    self._optimizer.zero_grad()\n</code></pre>"},{"location":"experiments/optimizer/scheduled_optim_pretraining/#notebooks.experiments.optimizer.scheduled_optim_pretraining.get_lr_lambda","title":"<code>get_lr_lambda(model_config, train_config, current_step=0)</code>","text":"<p>DEPRECATED: moved to AcousticModule. Returns the custom lambda function for the learning rate schedule.</p> <p>Returns     function: The custom lambda function for the learning rate schedule.</p> Source code in <code>notebooks/experiments/optimizer/scheduled_optim_pretraining.py</code> <pre><code>def get_lr_lambda(\n    model_config: AcousticModelConfigType,\n    train_config: AcousticTrainingConfig,\n    current_step: int = 0,\n) -&gt; Tuple[float, Callable[[int], float]]:\n    r\"\"\"DEPRECATED: moved to AcousticModule.\n    Returns the custom lambda function for the learning rate schedule.\n\n    Returns\n        function: The custom lambda function for the learning rate schedule.\n    \"\"\"\n    init_lr = model_config.encoder.n_hidden ** -0.5\n\n    def lr_lambda(step: int = current_step) -&gt; float:\n        r\"\"\"Computes the learning rate scale factor.\n\n        Args:\n            step (int): The current training step.\n\n        Returns:\n            float: The learning rate scale factor.\n        \"\"\"\n        step = 1 if step == 0 else step\n\n        warmup = train_config.optimizer_config.warm_up_step\n        anneal_steps = train_config.optimizer_config.anneal_steps\n        anneal_rate = train_config.optimizer_config.anneal_rate\n\n        lr_scale = min(\n            step ** -0.5,\n            step * warmup ** -1.5,\n        )\n\n        for s in anneal_steps:\n            if step &gt; s:\n                lr_scale *= anneal_rate\n\n        return init_lr * lr_scale\n\n    return init_lr, lr_lambda\n</code></pre>"},{"location":"models/config/langs/","title":"Langs","text":""},{"location":"models/config/langs/#models.config.langs.LangItem","title":"<code>LangItem</code>  <code>dataclass</code>","text":"<p>A class for storing language information.</p> Source code in <code>models/config/langs.py</code> <pre><code>@dataclass\nclass LangItem:\n    r\"\"\"A class for storing language information.\"\"\"\n\n    phonemizer: str\n    phonemizer_espeak: str\n    nemo: str\n    processing_lang_type: PreprocessLangType\n</code></pre>"},{"location":"models/config/langs/#models.config.langs.get_lang_map","title":"<code>get_lang_map(lang)</code>","text":"<p>Returns a LangItem object for the given language.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language to get the LangItem for.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the language is not supported.</p> <p>Returns:</p> Name Type Description <code>LangItem</code> <code>LangItem</code> <p>The LangItem object for the given language.</p> Source code in <code>models/config/langs.py</code> <pre><code>def get_lang_map(lang: str) -&gt; LangItem:\n    r\"\"\"Returns a LangItem object for the given language.\n\n    Args:\n        lang (str): The language to get the LangItem for.\n\n    Raises:\n        ValueError: If the language is not supported.\n\n    Returns:\n        LangItem: The LangItem object for the given language.\n    \"\"\"\n    if lang not in langs_map:\n        raise ValueError(f\"Language {lang} is not supported!\")\n    return langs_map[lang]\n</code></pre>"},{"location":"models/config/readme/","title":"References","text":""},{"location":"models/config/readme/#references","title":"References","text":"<p>Config docs</p>"},{"location":"models/config/readme/#langs","title":"Langs","text":""},{"location":"models/enhancer/post_net/","title":"Post net","text":""},{"location":"models/enhancer/post_net/#models.enhancer.post_net.ConvNorm","title":"<code>ConvNorm</code>","text":"<p>             Bases: <code>Module</code></p> <p>1D Convolution</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>kernel_size</code> <code>int</code> <p>Kernel size</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding</p> <code>None</code> <code>dilation</code> <code>int</code> <p>Dilation</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to use bias</p> <code>True</code> Source code in <code>models/enhancer/post_net.py</code> <pre><code>class ConvNorm(nn.Module):\n    \"\"\"1D Convolution\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        kernel_size (int): Kernel size\n        stride (int): Stride\n        padding (int): Padding\n        dilation (int): Dilation\n        bias (bool): Whether to use bias\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 1,\n        stride: int = 1,\n        padding: Optional[int] = None,\n        dilation: int = 1,\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        if padding is None:\n            assert kernel_size % 2 == 1\n            padding = int(dilation * (kernel_size - 1) / 2)\n\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.conv(x)\n</code></pre>"},{"location":"models/enhancer/post_net/#models.enhancer.post_net.PostNet","title":"<code>PostNet</code>","text":"<p>             Bases: <code>Module</code></p> <p>PostNet: Five 1-d convolution with 512 channels and kernel size 5</p> <p>Parameters:</p> Name Type Description Default <code>n_mel_channels</code> <code>int</code> <p>Number of mel channels</p> <code>100</code> <code>postnet_embedding_dim</code> <code>int</code> <p>PostNet embedding dimension</p> <code>512</code> <code>postnet_kernel_size</code> <code>int</code> <p>PostNet kernel size</p> <code>5</code> <code>postnet_n_convolutions</code> <code>int</code> <p>Number of PostNet convolutions</p> <code>3</code> <code>upsampling_factor</code> <code>int</code> <p>Upsampling factor for mel-spectrogram</p> required <code>p_dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>models/enhancer/post_net.py</code> <pre><code>class PostNet(nn.Module):\n    \"\"\"PostNet: Five 1-d convolution with 512 channels and kernel size 5\n\n    Args:\n        n_mel_channels (int): Number of mel channels\n        postnet_embedding_dim (int): PostNet embedding dimension\n        postnet_kernel_size (int): PostNet kernel size\n        postnet_n_convolutions (int): Number of PostNet convolutions\n        upsampling_factor (int): Upsampling factor for mel-spectrogram\n        p_dropout (float): Dropout probability\n    \"\"\"\n\n    def __init__(\n        self,\n        n_hidden: int,\n        n_mel_channels: int = 100,\n        postnet_embedding_dim: int = 512,\n        postnet_kernel_size: int = 5,\n        postnet_n_convolutions: int = 3,\n        p_dropout: float = 0.1,\n    ):\n        super().__init__()\n\n        self.convolutions = nn.ModuleList()\n\n        self.convolutions.append(\n            nn.Sequential(\n                ConvNorm(\n                    n_hidden,\n                    postnet_embedding_dim,\n                    kernel_size=postnet_kernel_size,\n                    stride=1,\n                    padding=int((postnet_kernel_size - 1) / 2),\n                    dilation=1,\n                ),\n                nn.BatchNorm1d(postnet_embedding_dim),\n                nn.Dropout(p_dropout),\n            ),\n        )\n\n        for _ in range(postnet_n_convolutions):\n            self.convolutions.append(\n                nn.Sequential(\n                    ConvNorm(\n                        postnet_embedding_dim,\n                        postnet_embedding_dim,\n                        kernel_size=postnet_kernel_size,\n                        stride=1,\n                        padding=int((postnet_kernel_size - 1) / 2),\n                        dilation=1,\n                    ),\n                    nn.LayerNorm(\n                        postnet_embedding_dim,\n                    ),\n                    nn.Dropout(p_dropout),\n                ),\n            )\n\n        self.to_mel = nn.Linear(\n            postnet_embedding_dim,\n            n_mel_channels,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        for conv in self.convolutions:\n            x = conv(x)\n\n        return self.to_mel(x)\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/denoiser/","title":"Denoiser","text":""},{"location":"models/enhancer/gaussian_diffusion/denoiser/#models.enhancer.gaussian_diffusion.denoiser.Denoiser","title":"<code>Denoiser</code>","text":"<p>             Bases: <code>Module</code></p> <p>Conditional Diffusion Denoiser.</p> <p>This module implements a denoising model conditioned on a diffusion step, a conditioner, and a speaker embedding. It consists of several convolutional and linear projections followed by residual blocks.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DiffusionConfig</code> <p>Model configuration dictionary.</p> required <p>Attributes:</p> Name Type Description <code>input_projection</code> <code>Sequential</code> <p>Sequential module for input projection.</p> <code>diffusion_embedding</code> <code>DiffusionEmbedding</code> <p>Diffusion step embedding module.</p> <code>mlp</code> <code>Sequential</code> <p>Multilayer perceptron module.</p> <code>residual_layers</code> <code>ModuleList</code> <p>List of residual blocks.</p> <code>skip_projection</code> <code>ConvNorm</code> <p>Convolutional projection for skip connections.</p> <code>output_projection</code> <code>ConvNorm</code> <p>Convolutional projection for output.</p> Source code in <code>models/enhancer/gaussian_diffusion/denoiser.py</code> <pre><code>class Denoiser(nn.Module):\n    r\"\"\"Conditional Diffusion Denoiser.\n\n    This module implements a denoising model conditioned on a diffusion step, a conditioner, and a speaker embedding.\n    It consists of several convolutional and linear projections followed by residual blocks.\n\n    Args:\n        model_config (DiffusionConfig): Model configuration dictionary.\n\n    Attributes:\n        input_projection (nn.Sequential): Sequential module for input projection.\n        diffusion_embedding (DiffusionEmbedding): Diffusion step embedding module.\n        mlp (nn.Sequential): Multilayer perceptron module.\n        residual_layers (nn.ModuleList): List of residual blocks.\n        skip_projection (ConvNorm): Convolutional projection for skip connections.\n        output_projection (ConvNorm): Convolutional projection for output.\n\n    \"\"\"\n\n    def __init__(self, model_config: DiffusionConfig):\n        super().__init__()\n\n        # Model configuration\n        multi_speaker = model_config.multi_speaker\n        n_mel_channels = model_config.n_mel_channels\n        d_encoder = model_config.encoder_hidden\n        d_spk_prj = model_config.speaker_embed_dim\n        residual_channels = model_config.residual_channels\n        residual_layers = model_config.residual_layers\n        dropout = model_config.denoiser_dropout\n\n        self.input_projection = nn.Sequential(\n            ConvNorm(n_mel_channels, residual_channels, kernel_size=1),\n            nn.ReLU(),\n        )\n        self.diffusion_embedding = DiffusionEmbedding(residual_channels)\n        self.mlp = nn.Sequential(\n            LinearNorm(residual_channels, residual_channels * 4),\n            Mish(),\n            LinearNorm(residual_channels * 4, residual_channels),\n        )\n        self.residual_layers = nn.ModuleList(\n            [\n                ResidualBlock(\n                    d_encoder,\n                    residual_channels,\n                    dropout=dropout,\n                    d_spk_prj=d_spk_prj,\n                    multi_speaker=multi_speaker,\n                )\n                for _ in range(residual_layers)\n            ],\n        )\n        self.skip_projection = ConvNorm(\n            residual_channels, residual_channels, kernel_size=1,\n        )\n        self.output_projection = ConvNorm(\n            residual_channels, n_mel_channels, kernel_size=1,\n        )\n        nn.init.zeros_(self.output_projection.conv.weight)\n\n    def forward(\n        self,\n        mel: Tensor,\n        diffusion_step: Tensor,\n        conditioner: Tensor,\n        speaker_emb: Tensor,\n    ) -&gt; Tensor:\n        r\"\"\"Forward pass through the Denoiser module.\n\n        Args:\n            mel (torch.Tensor): Mel-spectrogram tensor of shape [B, 1, M, T].\n            diffusion_step (torch.Tensor): Diffusion step tensor of shape [B,].\n            conditioner (torch.Tensor): Conditioner tensor of shape [B, M, T].\n            speaker_emb (torch.Tensor): Speaker embedding tensor of shape [B, M].\n\n        Returns:\n            torch.Tensor: Output mel-spectrogram tensor of shape [B, 1, M, T].\n        \"\"\"\n        x = mel[:, 0]\n        x = self.input_projection(x)  # x [B, residual_channel, T]\n        x = F.relu(x)\n\n        diffusion_step = self.diffusion_embedding(diffusion_step)\n        diffusion_step = self.mlp(diffusion_step)\n\n        skip = []\n        for layer in self.residual_layers:\n            x, skip_connection = layer(x, conditioner, diffusion_step, speaker_emb)\n            skip.append(skip_connection)\n\n        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n        x = self.skip_projection(x)\n        x = F.relu(x)\n        x = self.output_projection(x)  # [B, 80, T]\n\n        return x[:, None, :, :]\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/denoiser/#models.enhancer.gaussian_diffusion.denoiser.Denoiser.forward","title":"<code>forward(mel, diffusion_step, conditioner, speaker_emb)</code>","text":"<p>Forward pass through the Denoiser module.</p> <p>Parameters:</p> Name Type Description Default <code>mel</code> <code>Tensor</code> <p>Mel-spectrogram tensor of shape [B, 1, M, T].</p> required <code>diffusion_step</code> <code>Tensor</code> <p>Diffusion step tensor of shape [B,].</p> required <code>conditioner</code> <code>Tensor</code> <p>Conditioner tensor of shape [B, M, T].</p> required <code>speaker_emb</code> <code>Tensor</code> <p>Speaker embedding tensor of shape [B, M].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output mel-spectrogram tensor of shape [B, 1, M, T].</p> Source code in <code>models/enhancer/gaussian_diffusion/denoiser.py</code> <pre><code>def forward(\n    self,\n    mel: Tensor,\n    diffusion_step: Tensor,\n    conditioner: Tensor,\n    speaker_emb: Tensor,\n) -&gt; Tensor:\n    r\"\"\"Forward pass through the Denoiser module.\n\n    Args:\n        mel (torch.Tensor): Mel-spectrogram tensor of shape [B, 1, M, T].\n        diffusion_step (torch.Tensor): Diffusion step tensor of shape [B,].\n        conditioner (torch.Tensor): Conditioner tensor of shape [B, M, T].\n        speaker_emb (torch.Tensor): Speaker embedding tensor of shape [B, M].\n\n    Returns:\n        torch.Tensor: Output mel-spectrogram tensor of shape [B, 1, M, T].\n    \"\"\"\n    x = mel[:, 0]\n    x = self.input_projection(x)  # x [B, residual_channel, T]\n    x = F.relu(x)\n\n    diffusion_step = self.diffusion_embedding(diffusion_step)\n    diffusion_step = self.mlp(diffusion_step)\n\n    skip = []\n    for layer in self.residual_layers:\n        x, skip_connection = layer(x, conditioner, diffusion_step, speaker_emb)\n        skip.append(skip_connection)\n\n    x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n    x = self.skip_projection(x)\n    x = F.relu(x)\n    x = self.output_projection(x)  # [B, 80, T]\n\n    return x[:, None, :, :]\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/","title":"Gaussian Diffusion","text":""},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion","title":"<code>GaussianDiffusion</code>","text":"<p>             Bases: <code>Module</code></p> <p>Class implementing Gaussian Diffusion, a method used for modeling noise in data.</p> <p>Diffusion models typically consist of two Markov chains: the diffusion process and the reverse process (or denoising process). The diffusion process gradually introduces small Gaussian noises into the data until its structure is completely degraded at step T, while the reverse process learns a denoising function to remove the added noise and restore the original data structure.</p> <p>In this implementation: - The diffusion process models the gradual transformation from the original data <code>x_0</code> to the latent variable <code>x_T</code> using a predefined variance schedule. - The reverse process, implemented as a denoising function, aims to remove the added noise and reconstruct the original data.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Type of diffusion model.</p> <code>denoise_fn</code> <code>Denoiser</code> <p>Denoising function used in the reverse process.</p> <code>mel_bins</code> <code>int</code> <p>Number of Mel bins in the data.</p> <code>num_timesteps</code> <code>int</code> <p>Number of diffusion steps.</p> <code>loss_type</code> <code>str</code> <p>Type of noise loss used in training.</p> <code>betas</code> <code>Tensor</code> <p>Variance schedule for the diffusion process.</p> <code>alphas_cumprod</code> <code>Tensor</code> <p>Cumulative product of (1 - betas).</p> <code>alphas_cumprod_prev</code> <code>Tensor</code> <p>Cumulative product of (1 - betas) excluding the last element.</p> <code>sqrt_alphas_cumprod</code> <code>Tensor</code> <p>Square root of alphas_cumprod.</p> <code>sqrt_one_minus_alphas_cumprod</code> <code>Tensor</code> <p>Square root of (1 - alphas_cumprod).</p> <code>log_one_minus_alphas_cumprod</code> <code>Tensor</code> <p>Logarithm of (1 - alphas_cumprod).</p> <code>sqrt_recip_alphas_cumprod</code> <code>Tensor</code> <p>Square root of the reciprocal of alphas_cumprod.</p> <code>sqrt_recipm1_alphas_cumprod</code> <code>Tensor</code> <p>Square root of the reciprocal of (alphas_cumprod - 1).</p> <code>posterior_variance</code> <code>Tensor</code> <p>Variance of the posterior distribution.</p> <code>posterior_log_variance_clipped</code> <code>Tensor</code> <p>Clipped logarithm of the posterior variance.</p> <code>posterior_mean_coef1</code> <code>Tensor</code> <p>Coefficient for calculating posterior mean.</p> <code>posterior_mean_coef2</code> <code>Tensor</code> <p>Coefficient for calculating posterior mean.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>class GaussianDiffusion(nn.Module):\n    r\"\"\"Class implementing Gaussian Diffusion, a method used for modeling noise in data.\n\n    Diffusion models typically consist of two Markov chains: the diffusion process and the reverse process (or denoising process).\n    The diffusion process gradually introduces small Gaussian noises into the data until its structure is completely degraded at step T,\n    while the reverse process learns a denoising function to remove the added noise and restore the original data structure.\n\n    In this implementation:\n    - The diffusion process models the gradual transformation from the original data `x_0` to the latent variable `x_T` using a predefined variance schedule.\n    - The reverse process, implemented as a denoising function, aims to remove the added noise and reconstruct the original data.\n\n    Attributes:\n        model (str): Type of diffusion model.\n        denoise_fn (Denoiser): Denoising function used in the reverse process.\n        mel_bins (int): Number of Mel bins in the data.\n        num_timesteps (int): Number of diffusion steps.\n        loss_type (str): Type of noise loss used in training.\n        betas (Tensor): Variance schedule for the diffusion process.\n        alphas_cumprod (Tensor): Cumulative product of (1 - betas).\n        alphas_cumprod_prev (Tensor): Cumulative product of (1 - betas) excluding the last element.\n        sqrt_alphas_cumprod (Tensor): Square root of alphas_cumprod.\n        sqrt_one_minus_alphas_cumprod (Tensor): Square root of (1 - alphas_cumprod).\n        log_one_minus_alphas_cumprod (Tensor): Logarithm of (1 - alphas_cumprod).\n        sqrt_recip_alphas_cumprod (Tensor): Square root of the reciprocal of alphas_cumprod.\n        sqrt_recipm1_alphas_cumprod (Tensor): Square root of the reciprocal of (alphas_cumprod - 1).\n        posterior_variance (Tensor): Variance of the posterior distribution.\n        posterior_log_variance_clipped (Tensor): Clipped logarithm of the posterior variance.\n        posterior_mean_coef1 (Tensor): Coefficient for calculating posterior mean.\n        posterior_mean_coef2 (Tensor): Coefficient for calculating posterior mean.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: DiffusionConfig,\n    ):\n        r\"\"\"Initialize the Gaussian Diffusion module.\n\n        Args:\n            model_config (DiffusionConfig): Model configuration.\n        \"\"\"\n        super().__init__()\n\n        # Model configuration\n        self.model = model_config.model\n        self.denoise_fn = Denoiser(model_config)\n\n        self.mel_bins = model_config.n_mel_channels\n\n        betas = get_noise_schedule_list(\n            schedule_mode=model_config.noise_schedule_naive,\n            timesteps=model_config.timesteps if self.model == \"naive\" else model_config.shallow_timesteps,\n            min_beta=model_config.min_beta,\n            max_beta=model_config.max_beta,\n            s=model_config.s,\n        )\n\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.loss_type = model_config.noise_loss\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer(\"betas\", to_torch(betas))\n        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer(\"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer(\"log_one_minus_alphas_cumprod\", to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer(\"sqrt_recipm1_alphas_cumprod\", to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer(\"posterior_variance\", to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer(\"posterior_log_variance_clipped\", to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer(\"posterior_mean_coef1\", to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer(\"posterior_mean_coef2\", to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n    def q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Calculate mean, variance, and log variance of the diffusion process.\n\n        Args:\n            x_start (Tensor): Input tensor.\n            t (Tensor): Time step.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: Mean, variance, and log variance.\n        \"\"\"\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1. - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n        r\"\"\"Predict the start from noise at a given time step.\n\n        Args:\n            x_t (Tensor): Input tensor.\n            t (Tensor): Time step.\n            noise (Tensor): Noise tensor.\n\n        Returns:\n            Tensor: Predicted start from noise.\n        \"\"\"\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        r\"\"\"Calculate posterior mean, variance, and clipped log variance.\n\n        Args:\n            x_start (Tensor): Start tensor.\n            x_t (Tensor): Tensor at time step.\n            t (Tensor): Time step.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: Posterior mean, variance, and clipped log variance.\n        \"\"\"\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_posterior_sample(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        repeat_noise: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Sample from the posterior distribution.\n\n        Args:\n            x_start (Tensor): Start tensor.\n            x_t (Tensor): Tensor at time step.\n            t (Tensor): Time step.\n            repeat_noise (bool, optional): Whether to repeat noise. Defaults to False.\n\n        Returns:\n            Tensor: Sampled tensor from posterior distribution.\n        \"\"\"\n        b, *_, device = *x_start.shape, x_start.device\n        model_mean, _, model_log_variance = self.q_posterior(x_start=x_start, x_t=x_t, t=t)\n        noise = noise_like(x_start.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x_start.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample(\n        self,\n        x_t: Tensor,\n        t: Tensor,\n        cond: Tensor,\n        spk_emb: Tensor,\n        clip_denoised: bool = True,\n    ):\n        r\"\"\"Sample from the distribution.\n\n        Args:\n            x_t (Tensor): Tensor at time step.\n            t (Tensor): Time tensor.\n            cond (Tensor): Conditional tensor.\n            spk_emb (Tensor): Speaker embedding tensor.\n            clip_denoised (bool, optional): Whether to clip denoised tensor. Defaults to True.\n\n        Returns:\n            Tensor: Sampled tensor.\n        \"\"\"\n        b, *_, device = *x_t.shape, x_t.device\n        x_0_pred = self.denoise_fn.forward(x_t, t, cond, spk_emb)\n\n        if clip_denoised:\n            x_0_pred.clamp_(-1., 1.)\n\n        return self.q_posterior_sample(x_start=x_0_pred, x_t=x_t, t=t)\n\n    @torch.no_grad()\n    def interpolate(\n        self,\n        x1: Tensor,\n        x2: Tensor,\n        t: int,\n        cond: Tensor,\n        spk_emb: Tensor,\n        lam: float = 0.5,\n    ):\n        r\"\"\"Interpolate between two tensors.\n\n        Args:\n            x1 (Tensor): First tensor.\n            x2 (Tensor): Second tensor.\n            t (int): Time step.\n            cond (Tensor): Conditional tensor.\n            spk_emb (Tensor): Speaker embedding tensor.\n            lam (float, optional): Lambda value. Defaults to 0.5.\n\n        Returns:\n            Tensor: Interpolated tensor.\n        \"\"\"\n        b, *_, device = *x1.shape, x1.device\n        t = default(t, self.num_timesteps - 1)\n\n        assert x1.shape == x2.shape\n\n        t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n        xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n        x = (1 - lam) * xt1 + lam * xt2\n        for i in reversed(range(t)):\n            x = self.p_sample(x, torch.full((b,), i, device=device, dtype=torch.long), cond, spk_emb)\n\n        x = x[:, 0].transpose(1, 2)\n        return x\n        # return self.denorm_spec(x)\n\n    def q_sample(\n        self,\n        x_start: Tensor,\n        t: Tensor,\n        noise: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        r\"\"\"Sample from the diffusion process.\n\n        Args:\n            x_start (Tensor): Start tensor.\n            t (Tensor): Time tensor.\n            noise (Tensor, optional): Noise tensor. Defaults to None.\n\n        Returns:\n            Tensor: Sampled tensor.\n        \"\"\"\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    @torch.no_grad()\n    def sampling(\n        self,\n        cond: Tensor,\n        spk_emb: Tensor,\n        noise: Optional[Tensor] = None,\n    ) -&gt; List[Tensor]:\n        r\"\"\"Perform sampling.\n\n        Args:\n            noise (Tensor, optional): Noise tensor. Defaults to None.\n            cond (Tensor): Conditional tensor.\n            spk_emb (Tensor): Speaker embedding tensor.\n\n        Returns:\n            List[Tensor]: List of sampled tensors.\n        \"\"\"\n        b, *_, device = *cond.shape, cond.device # type: ignore\n        t = self.num_timesteps\n        shape = (cond.shape[0], 1, self.mel_bins, cond.shape[2])\n        xs = [torch.randn(shape, device=device) if noise is None else noise]\n        for i in reversed(range(t)):\n            x = self.p_sample(\n                xs[-1],\n                torch.full((b,), i, device=device, dtype=torch.long),\n                cond,\n                spk_emb,\n            )\n            xs.append(x)\n        # output = [self.denorm_spec(x[:, 0].transpose(1, 2)) for x in xs]\n        output = [x[:, 0].transpose(1, 2) for x in xs]\n        return output\n\n    def diffuse_fn(\n        self,\n        x_start: Tensor,\n        t: Tensor,\n        noise: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        r\"\"\"Diffuse function.\n\n        Args:\n            x_start (Tensor): Start tensor.\n            t (Tensor): Time tensor.\n            noise (Tensor, optional): Noise tensor. Defaults to None.\n\n        Returns:\n            Tensor: Diffused tensor.\n        \"\"\"\n        # x_start = self.norm_spec(x_start)\n        # x_start = x_start.transpose(1, 2)[:, None, :, :]  # [B, 1, M, T]\n        x_start = x_start[:, None, :, :] # [B, 1, T, M]\n        zero_idx = t &lt; 0 # for items where t is -1\n        t[zero_idx] = 0\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        out = self.q_sample(x_start=x_start, t=t, noise=noise)\n        out[zero_idx] = x_start[zero_idx] # set x_{-1} as the gt mel\n        return out\n\n    def forward(\n        self,\n        mel: Optional[Tensor],\n        cond: Tensor,\n        spk_emb: Tensor,\n        mel_mask: Tensor,\n        coarse_mel: Tensor,\n        clip_denoised: bool = True,\n    ) -&gt; Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]:\n        r\"\"\"Forward pass.\n\n        Args:\n            mel (Tensor, optional): Mel tensor. Defaults to None.\n            cond (Tensor): Conditional tensor.\n            spk_emb (Tensor): Speaker embedding tensor.\n            mel_mask (Tensor): Mel mask tensor.\n            coarse_mel (Tensor, optional): Coarse mel tensor. Defaults to None.\n            clip_denoised (bool, optional): Whether to clip denoised tensor. Defaults to True.\n\n        Returns:\n            Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]: Tuple containing predicted start, tensor at time step, tensor at previous time step, predicted tensor at previous time step, and time tensor.\n        \"\"\"\n        b, *_, device = *cond.shape, cond.device\n\n        mel_mask = ~mel_mask.unsqueeze(-1)\n        cond = cond.transpose(1, 2)\n\n        if mel is None:\n            if self.model != \"shallow\":\n                noise = None\n            else:\n                t = torch.full((b,), self.num_timesteps - 1, device=device, dtype=torch.long)\n                # noise = self.diffuse_fn(coarse_mel, t) * mel_mask.unsqueeze(1)\n                # noise = self.diffuse_fn(coarse_mel, t) * mel_mask.unsqueeze(-1).transpose(1, -1)\n                noise = (self.diffuse_fn(coarse_mel, t) * mel_mask).transpose(-1, -2)\n\n            x_0_pred = self.sampling(\n                cond,\n                spk_emb,\n                noise=noise,\n            )[-1] * mel_mask\n\n            return x_0_pred, None, None, None, None\n        else:\n            mel_mask = mel_mask.unsqueeze(-1).transpose(1, -1)\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            # Diffusion\n            x_t = self.diffuse_fn(mel, t) * mel_mask\n            x_t_prev = self.diffuse_fn(mel, t - 1) * mel_mask\n\n            # Predict x_{start}\n            x_0_pred = self.denoise_fn.forward(x_t, t, cond, spk_emb) * mel_mask\n            if clip_denoised:\n                x_0_pred.clamp_(-1., 1.)\n\n            # Sample x_{t-1} using the posterior distribution\n            if self.model != \"shallow\":\n                x_start = x_0_pred\n            else:\n                # x_start = self.norm_spec(coarse_mel)\n                x_start = coarse_mel\n                # x_start = x_start.transpose(1, 2)[:, None, :, :]  # [B, 1, M, T]\n                x_start = x_start[:, None, :, :]  # [B, 1, T, M]\n\n            x_t_prev_pred = self.q_posterior_sample(x_start=x_start, x_t=x_t, t=t) * mel_mask\n\n            x_0_pred = x_0_pred[:, 0].transpose(1, 2)\n            x_t = x_t[:, 0].transpose(1, 2)\n\n            x_t_prev = x_t_prev[:, 0].transpose(1, 2)\n            x_t_prev_pred = x_t_prev_pred[:, 0].transpose(1, 2)\n\n        return x_0_pred, x_t, x_t_prev, x_t_prev_pred, t\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.__init__","title":"<code>__init__(model_config)</code>","text":"<p>Initialize the Gaussian Diffusion module.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DiffusionConfig</code> <p>Model configuration.</p> required Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def __init__(\n    self,\n    model_config: DiffusionConfig,\n):\n    r\"\"\"Initialize the Gaussian Diffusion module.\n\n    Args:\n        model_config (DiffusionConfig): Model configuration.\n    \"\"\"\n    super().__init__()\n\n    # Model configuration\n    self.model = model_config.model\n    self.denoise_fn = Denoiser(model_config)\n\n    self.mel_bins = model_config.n_mel_channels\n\n    betas = get_noise_schedule_list(\n        schedule_mode=model_config.noise_schedule_naive,\n        timesteps=model_config.timesteps if self.model == \"naive\" else model_config.shallow_timesteps,\n        min_beta=model_config.min_beta,\n        max_beta=model_config.max_beta,\n        s=model_config.s,\n    )\n\n    alphas = 1. - betas\n    alphas_cumprod = np.cumprod(alphas, axis=0)\n    alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n    timesteps, = betas.shape\n    self.num_timesteps = int(timesteps)\n    self.loss_type = model_config.noise_loss\n\n    to_torch = partial(torch.tensor, dtype=torch.float32)\n\n    self.register_buffer(\"betas\", to_torch(betas))\n    self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n    self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n\n    # calculations for diffusion q(x_t | x_{t-1}) and others\n    self.register_buffer(\"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod)))\n    self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1. - alphas_cumprod)))\n    self.register_buffer(\"log_one_minus_alphas_cumprod\", to_torch(np.log(1. - alphas_cumprod)))\n    self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1. / alphas_cumprod)))\n    self.register_buffer(\"sqrt_recipm1_alphas_cumprod\", to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n    # calculations for posterior q(x_{t-1} | x_t, x_0)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n    # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n    self.register_buffer(\"posterior_variance\", to_torch(posterior_variance))\n    # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n    self.register_buffer(\"posterior_log_variance_clipped\", to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n    self.register_buffer(\"posterior_mean_coef1\", to_torch(\n        betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n    self.register_buffer(\"posterior_mean_coef2\", to_torch(\n        (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.diffuse_fn","title":"<code>diffuse_fn(x_start, t, noise=None)</code>","text":"<p>Diffuse function.</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Tensor</code> <p>Start tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time tensor.</p> required <code>noise</code> <code>Tensor</code> <p>Noise tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Diffused tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def diffuse_fn(\n    self,\n    x_start: Tensor,\n    t: Tensor,\n    noise: Optional[Tensor] = None,\n) -&gt; Tensor:\n    r\"\"\"Diffuse function.\n\n    Args:\n        x_start (Tensor): Start tensor.\n        t (Tensor): Time tensor.\n        noise (Tensor, optional): Noise tensor. Defaults to None.\n\n    Returns:\n        Tensor: Diffused tensor.\n    \"\"\"\n    # x_start = self.norm_spec(x_start)\n    # x_start = x_start.transpose(1, 2)[:, None, :, :]  # [B, 1, M, T]\n    x_start = x_start[:, None, :, :] # [B, 1, T, M]\n    zero_idx = t &lt; 0 # for items where t is -1\n    t[zero_idx] = 0\n    noise = default(noise, lambda: torch.randn_like(x_start))\n    out = self.q_sample(x_start=x_start, t=t, noise=noise)\n    out[zero_idx] = x_start[zero_idx] # set x_{-1} as the gt mel\n    return out\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.forward","title":"<code>forward(mel, cond, spk_emb, mel_mask, coarse_mel, clip_denoised=True)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>mel</code> <code>Tensor</code> <p>Mel tensor. Defaults to None.</p> required <code>cond</code> <code>Tensor</code> <p>Conditional tensor.</p> required <code>spk_emb</code> <code>Tensor</code> <p>Speaker embedding tensor.</p> required <code>mel_mask</code> <code>Tensor</code> <p>Mel mask tensor.</p> required <code>coarse_mel</code> <code>Tensor</code> <p>Coarse mel tensor. Defaults to None.</p> required <code>clip_denoised</code> <code>bool</code> <p>Whether to clip denoised tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]</code> <p>Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]: Tuple containing predicted start, tensor at time step, tensor at previous time step, predicted tensor at previous time step, and time tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def forward(\n    self,\n    mel: Optional[Tensor],\n    cond: Tensor,\n    spk_emb: Tensor,\n    mel_mask: Tensor,\n    coarse_mel: Tensor,\n    clip_denoised: bool = True,\n) -&gt; Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]:\n    r\"\"\"Forward pass.\n\n    Args:\n        mel (Tensor, optional): Mel tensor. Defaults to None.\n        cond (Tensor): Conditional tensor.\n        spk_emb (Tensor): Speaker embedding tensor.\n        mel_mask (Tensor): Mel mask tensor.\n        coarse_mel (Tensor, optional): Coarse mel tensor. Defaults to None.\n        clip_denoised (bool, optional): Whether to clip denoised tensor. Defaults to True.\n\n    Returns:\n        Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]: Tuple containing predicted start, tensor at time step, tensor at previous time step, predicted tensor at previous time step, and time tensor.\n    \"\"\"\n    b, *_, device = *cond.shape, cond.device\n\n    mel_mask = ~mel_mask.unsqueeze(-1)\n    cond = cond.transpose(1, 2)\n\n    if mel is None:\n        if self.model != \"shallow\":\n            noise = None\n        else:\n            t = torch.full((b,), self.num_timesteps - 1, device=device, dtype=torch.long)\n            # noise = self.diffuse_fn(coarse_mel, t) * mel_mask.unsqueeze(1)\n            # noise = self.diffuse_fn(coarse_mel, t) * mel_mask.unsqueeze(-1).transpose(1, -1)\n            noise = (self.diffuse_fn(coarse_mel, t) * mel_mask).transpose(-1, -2)\n\n        x_0_pred = self.sampling(\n            cond,\n            spk_emb,\n            noise=noise,\n        )[-1] * mel_mask\n\n        return x_0_pred, None, None, None, None\n    else:\n        mel_mask = mel_mask.unsqueeze(-1).transpose(1, -1)\n        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n        # Diffusion\n        x_t = self.diffuse_fn(mel, t) * mel_mask\n        x_t_prev = self.diffuse_fn(mel, t - 1) * mel_mask\n\n        # Predict x_{start}\n        x_0_pred = self.denoise_fn.forward(x_t, t, cond, spk_emb) * mel_mask\n        if clip_denoised:\n            x_0_pred.clamp_(-1., 1.)\n\n        # Sample x_{t-1} using the posterior distribution\n        if self.model != \"shallow\":\n            x_start = x_0_pred\n        else:\n            # x_start = self.norm_spec(coarse_mel)\n            x_start = coarse_mel\n            # x_start = x_start.transpose(1, 2)[:, None, :, :]  # [B, 1, M, T]\n            x_start = x_start[:, None, :, :]  # [B, 1, T, M]\n\n        x_t_prev_pred = self.q_posterior_sample(x_start=x_start, x_t=x_t, t=t) * mel_mask\n\n        x_0_pred = x_0_pred[:, 0].transpose(1, 2)\n        x_t = x_t[:, 0].transpose(1, 2)\n\n        x_t_prev = x_t_prev[:, 0].transpose(1, 2)\n        x_t_prev_pred = x_t_prev_pred[:, 0].transpose(1, 2)\n\n    return x_0_pred, x_t, x_t_prev, x_t_prev_pred, t\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.interpolate","title":"<code>interpolate(x1, x2, t, cond, spk_emb, lam=0.5)</code>","text":"<p>Interpolate between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tensor</code> <p>First tensor.</p> required <code>x2</code> <code>Tensor</code> <p>Second tensor.</p> required <code>t</code> <code>int</code> <p>Time step.</p> required <code>cond</code> <code>Tensor</code> <p>Conditional tensor.</p> required <code>spk_emb</code> <code>Tensor</code> <p>Speaker embedding tensor.</p> required <code>lam</code> <code>float</code> <p>Lambda value. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Interpolated tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>@torch.no_grad()\ndef interpolate(\n    self,\n    x1: Tensor,\n    x2: Tensor,\n    t: int,\n    cond: Tensor,\n    spk_emb: Tensor,\n    lam: float = 0.5,\n):\n    r\"\"\"Interpolate between two tensors.\n\n    Args:\n        x1 (Tensor): First tensor.\n        x2 (Tensor): Second tensor.\n        t (int): Time step.\n        cond (Tensor): Conditional tensor.\n        spk_emb (Tensor): Speaker embedding tensor.\n        lam (float, optional): Lambda value. Defaults to 0.5.\n\n    Returns:\n        Tensor: Interpolated tensor.\n    \"\"\"\n    b, *_, device = *x1.shape, x1.device\n    t = default(t, self.num_timesteps - 1)\n\n    assert x1.shape == x2.shape\n\n    t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    x = (1 - lam) * xt1 + lam * xt2\n    for i in reversed(range(t)):\n        x = self.p_sample(x, torch.full((b,), i, device=device, dtype=torch.long), cond, spk_emb)\n\n    x = x[:, 0].transpose(1, 2)\n    return x\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.p_sample","title":"<code>p_sample(x_t, t, cond, spk_emb, clip_denoised=True)</code>","text":"<p>Sample from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Tensor at time step.</p> required <code>t</code> <code>Tensor</code> <p>Time tensor.</p> required <code>cond</code> <code>Tensor</code> <p>Conditional tensor.</p> required <code>spk_emb</code> <code>Tensor</code> <p>Speaker embedding tensor.</p> required <code>clip_denoised</code> <code>bool</code> <p>Whether to clip denoised tensor. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Sampled tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>@torch.no_grad()\ndef p_sample(\n    self,\n    x_t: Tensor,\n    t: Tensor,\n    cond: Tensor,\n    spk_emb: Tensor,\n    clip_denoised: bool = True,\n):\n    r\"\"\"Sample from the distribution.\n\n    Args:\n        x_t (Tensor): Tensor at time step.\n        t (Tensor): Time tensor.\n        cond (Tensor): Conditional tensor.\n        spk_emb (Tensor): Speaker embedding tensor.\n        clip_denoised (bool, optional): Whether to clip denoised tensor. Defaults to True.\n\n    Returns:\n        Tensor: Sampled tensor.\n    \"\"\"\n    b, *_, device = *x_t.shape, x_t.device\n    x_0_pred = self.denoise_fn.forward(x_t, t, cond, spk_emb)\n\n    if clip_denoised:\n        x_0_pred.clamp_(-1., 1.)\n\n    return self.q_posterior_sample(x_start=x_0_pred, x_t=x_t, t=t)\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.predict_start_from_noise","title":"<code>predict_start_from_noise(x_t, t, noise)</code>","text":"<p>Predict the start from noise at a given time step.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Input tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time step.</p> required <code>noise</code> <code>Tensor</code> <p>Noise tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Predicted start from noise.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def predict_start_from_noise(self, x_t: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n    r\"\"\"Predict the start from noise at a given time step.\n\n    Args:\n        x_t (Tensor): Input tensor.\n        t (Tensor): Time step.\n        noise (Tensor): Noise tensor.\n\n    Returns:\n        Tensor: Predicted start from noise.\n    \"\"\"\n    return (\n        extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n        extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n    )\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.q_mean_variance","title":"<code>q_mean_variance(x_start, t)</code>","text":"<p>Calculate mean, variance, and log variance of the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Tensor</code> <p>Input tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time step.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor, Tensor]: Mean, variance, and log variance.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Calculate mean, variance, and log variance of the diffusion process.\n\n    Args:\n        x_start (Tensor): Input tensor.\n        t (Tensor): Time step.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor]: Mean, variance, and log variance.\n    \"\"\"\n    mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n    variance = extract(1. - self.alphas_cumprod, t, x_start.shape)\n    log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n    return mean, variance, log_variance\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.q_posterior","title":"<code>q_posterior(x_start, x_t, t)</code>","text":"<p>Calculate posterior mean, variance, and clipped log variance.</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Tensor</code> <p>Start tensor.</p> required <code>x_t</code> <code>Tensor</code> <p>Tensor at time step.</p> required <code>t</code> <code>Tensor</code> <p>Time step.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor, Tensor]: Posterior mean, variance, and clipped log variance.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def q_posterior(\n    self,\n    x_start: Tensor,\n    x_t: Tensor,\n    t: Tensor,\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"Calculate posterior mean, variance, and clipped log variance.\n\n    Args:\n        x_start (Tensor): Start tensor.\n        x_t (Tensor): Tensor at time step.\n        t (Tensor): Time step.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor]: Posterior mean, variance, and clipped log variance.\n    \"\"\"\n    posterior_mean = (\n        extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n        extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n    )\n    posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n    posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.q_posterior_sample","title":"<code>q_posterior_sample(x_start, x_t, t, repeat_noise=False)</code>","text":"<p>Sample from the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Tensor</code> <p>Start tensor.</p> required <code>x_t</code> <code>Tensor</code> <p>Tensor at time step.</p> required <code>t</code> <code>Tensor</code> <p>Time step.</p> required <code>repeat_noise</code> <code>bool</code> <p>Whether to repeat noise. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Sampled tensor from posterior distribution.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def q_posterior_sample(\n    self,\n    x_start: Tensor,\n    x_t: Tensor,\n    t: Tensor,\n    repeat_noise: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Sample from the posterior distribution.\n\n    Args:\n        x_start (Tensor): Start tensor.\n        x_t (Tensor): Tensor at time step.\n        t (Tensor): Time step.\n        repeat_noise (bool, optional): Whether to repeat noise. Defaults to False.\n\n    Returns:\n        Tensor: Sampled tensor from posterior distribution.\n    \"\"\"\n    b, *_, device = *x_start.shape, x_start.device\n    model_mean, _, model_log_variance = self.q_posterior(x_start=x_start, x_t=x_t, t=t)\n    noise = noise_like(x_start.shape, device, repeat_noise)\n    # no noise when t == 0\n    nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x_start.shape) - 1)))\n    return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.q_sample","title":"<code>q_sample(x_start, t, noise=None)</code>","text":"<p>Sample from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Tensor</code> <p>Start tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time tensor.</p> required <code>noise</code> <code>Tensor</code> <p>Noise tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Sampled tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>def q_sample(\n    self,\n    x_start: Tensor,\n    t: Tensor,\n    noise: Optional[Tensor] = None,\n) -&gt; Tensor:\n    r\"\"\"Sample from the diffusion process.\n\n    Args:\n        x_start (Tensor): Start tensor.\n        t (Tensor): Time tensor.\n        noise (Tensor, optional): Noise tensor. Defaults to None.\n\n    Returns:\n        Tensor: Sampled tensor.\n    \"\"\"\n    noise = default(noise, lambda: torch.randn_like(x_start))\n\n    return (\n        extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n        extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n    )\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/gaussian_diffusion/#models.enhancer.gaussian_diffusion.gaussian_diffusion.GaussianDiffusion.sampling","title":"<code>sampling(cond, spk_emb, noise=None)</code>","text":"<p>Perform sampling.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tensor</code> <p>Noise tensor. Defaults to None.</p> <code>None</code> <code>cond</code> <code>Tensor</code> <p>Conditional tensor.</p> required <code>spk_emb</code> <code>Tensor</code> <p>Speaker embedding tensor.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>List[Tensor]: List of sampled tensors.</p> Source code in <code>models/enhancer/gaussian_diffusion/gaussian_diffusion.py</code> <pre><code>@torch.no_grad()\ndef sampling(\n    self,\n    cond: Tensor,\n    spk_emb: Tensor,\n    noise: Optional[Tensor] = None,\n) -&gt; List[Tensor]:\n    r\"\"\"Perform sampling.\n\n    Args:\n        noise (Tensor, optional): Noise tensor. Defaults to None.\n        cond (Tensor): Conditional tensor.\n        spk_emb (Tensor): Speaker embedding tensor.\n\n    Returns:\n        List[Tensor]: List of sampled tensors.\n    \"\"\"\n    b, *_, device = *cond.shape, cond.device # type: ignore\n    t = self.num_timesteps\n    shape = (cond.shape[0], 1, self.mel_bins, cond.shape[2])\n    xs = [torch.randn(shape, device=device) if noise is None else noise]\n    for i in reversed(range(t)):\n        x = self.p_sample(\n            xs[-1],\n            torch.full((b,), i, device=device, dtype=torch.long),\n            cond,\n            spk_emb,\n        )\n        xs.append(x)\n    # output = [self.denorm_spec(x[:, 0].transpose(1, 2)) for x in xs]\n    output = [x[:, 0].transpose(1, 2) for x in xs]\n    return output\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/","title":"Layers","text":""},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.ConvNorm","title":"<code>ConvNorm</code>","text":"<p>             Bases: <code>Module</code></p> <p>1D Convolution with optional batch normalization.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolving kernel. Defaults to 1.</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride of the convolution. Defaults to 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Zero-padding added to both sides of the input. Defaults to None.</p> <code>None</code> <code>dilation</code> <code>int</code> <p>Spacing between kernel elements. Defaults to 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If True, adds a learnable bias to the output. Defaults to True.</p> <code>True</code> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>class ConvNorm(Module):\n    r\"\"\"1D Convolution with optional batch normalization.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (int, optional): Size of the convolving kernel. Defaults to 1.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Zero-padding added to both sides of the input. Defaults to None.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If True, adds a learnable bias to the output. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 1,\n        stride: int = 1,\n        padding: Optional[int] = None,\n        dilation: int = 1,\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        if padding is None:\n            assert kernel_size % 2 == 1\n            padding = int(dilation * (kernel_size - 1) / 2)\n\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n        nn.init.kaiming_normal_(self.conv.weight)\n\n    def forward(self, signal: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass through the convolutional layer.\n\n        Args:\n            signal (torch.Tensor): Input signal tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after convolution.\n        \"\"\"\n        conv_signal = self.conv(signal)\n\n        return conv_signal\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.ConvNorm.forward","title":"<code>forward(signal)</code>","text":"<p>Forward pass through the convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>Tensor</code> <p>Input signal tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after convolution.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>def forward(self, signal: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass through the convolutional layer.\n\n    Args:\n        signal (torch.Tensor): Input signal tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after convolution.\n    \"\"\"\n    conv_signal = self.conv(signal)\n\n    return conv_signal\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.DiffusionEmbedding","title":"<code>DiffusionEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Diffusion Step Embedding.</p> <p>This module generates diffusion step embeddings for the given input.</p> <p>Parameters:</p> Name Type Description Default <code>d_denoiser</code> <code>int</code> <p>Dimension of the denoiser.</p> required <p>Attributes:</p> Name Type Description <code>dim</code> <code>int</code> <p>Dimension of the diffusion step embedding.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>class DiffusionEmbedding(Module):\n    r\"\"\"Diffusion Step Embedding.\n\n    This module generates diffusion step embeddings for the given input.\n\n    Args:\n        d_denoiser (int): Dimension of the denoiser.\n\n    Attributes:\n        dim (int): Dimension of the diffusion step embedding.\n    \"\"\"\n\n    def __init__(self, d_denoiser: int):\n        super().__init__()\n        self.dim = d_denoiser\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass through the DiffusionEmbedding module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Diffusion step embeddings.\n        \"\"\"\n        device = x.device\n        half_dim = self.dim // 2\n\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n\n        return emb\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.DiffusionEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the DiffusionEmbedding module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Diffusion step embeddings.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass through the DiffusionEmbedding module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Diffusion step embeddings.\n    \"\"\"\n    device = x.device\n    half_dim = self.dim // 2\n\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n\n    emb = x[:, None] * emb[None, :]\n    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n\n    return emb\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.LinearNorm","title":"<code>LinearNorm</code>","text":"<p>             Bases: <code>Module</code></p> <p>LinearNorm Projection.</p> <p>This module performs a linear projection with optional bias.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features.</p> required <code>out_features</code> <code>int</code> <p>Number of output features.</p> required <code>bias</code> <code>bool</code> <p>If True, adds a learnable bias to the output. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear</code> <code>Linear</code> <p>Linear transformation module.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>class LinearNorm(Module):\n    r\"\"\"LinearNorm Projection.\n\n    This module performs a linear projection with optional bias.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        bias (bool, optional): If True, adds a learnable bias to the output. Default is False.\n\n    Attributes:\n        linear (torch.nn.Linear): Linear transformation module.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = False,\n    ):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n\n        nn.init.xavier_uniform_(self.linear.weight)\n        if bias:\n            nn.init.constant_(self.linear.bias, 0.0)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass through the LinearNorm module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after linear projection.\n        \"\"\"\n        x = self.linear(x)\n        return x\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.LinearNorm.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the LinearNorm module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after linear projection.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass through the LinearNorm module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after linear projection.\n    \"\"\"\n    x = self.linear(x)\n    return x\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.Mish","title":"<code>Mish</code>","text":"<p>             Bases: <code>Module</code></p> <p>Applies the Mish activation function.</p> <p>Mish is a smooth, non-monotonic function that attempts to mitigate the problems of dying ReLU units in deep neural networks.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>class Mish(Module):\n    r\"\"\"Applies the Mish activation function.\n\n    Mish is a smooth, non-monotonic function that attempts to mitigate the\n    problems of dying ReLU units in deep neural networks.\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass of the Mish activation function.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying Mish activation.\n        \"\"\"\n        return x * torch.tanh(F.softplus(x))\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.Mish.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the Mish activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying Mish activation.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass of the Mish activation function.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying Mish activation.\n    \"\"\"\n    return x * torch.tanh(F.softplus(x))\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.ResidualBlock","title":"<code>ResidualBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Residual Block.</p> <p>This module defines a residual block used in a neural network architecture. It consists of several convolutional and linear projections followed by nonlinear activations.</p> <p>Parameters:</p> Name Type Description Default <code>d_encoder</code> <code>int</code> <p>Dimension of the encoder output.</p> required <code>residual_channels</code> <code>int</code> <p>Number of channels in the residual block.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> required <code>d_spk_prj</code> <code>int</code> <p>Dimension of the speaker projection.</p> required <code>multi_speaker</code> <code>bool</code> <p>Flag indicating if the model is trained with multiple speakers. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>multi_speaker</code> <code>bool</code> <p>Flag indicating if the model is trained with multiple speakers.</p> <code>conv_layer</code> <code>ConvNorm</code> <p>Convolutional layer in the residual block.</p> <code>diffusion_projection</code> <code>LinearNorm</code> <p>Linear projection for the diffusion step.</p> <code>speaker_projection</code> <code>LinearNorm</code> <p>Linear projection for the speaker embedding.</p> <code>conditioner_projection</code> <code>ConvNorm</code> <p>Convolutional projection for the conditioner.</p> <code>output_projection</code> <code>ConvNorm</code> <p>Convolutional projection for the output.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>class ResidualBlock(Module):\n    r\"\"\"Residual Block.\n\n    This module defines a residual block used in a neural network architecture. It consists of\n    several convolutional and linear projections followed by nonlinear activations.\n\n    Args:\n        d_encoder (int): Dimension of the encoder output.\n        residual_channels (int): Number of channels in the residual block.\n        dropout (float): Dropout probability.\n        d_spk_prj (int): Dimension of the speaker projection.\n        multi_speaker (bool, optional): Flag indicating if the model is trained with multiple speakers. Defaults to True.\n\n    Attributes:\n        multi_speaker (bool): Flag indicating if the model is trained with multiple speakers.\n        conv_layer (ConvNorm): Convolutional layer in the residual block.\n        diffusion_projection (LinearNorm): Linear projection for the diffusion step.\n        speaker_projection (LinearNorm): Linear projection for the speaker embedding.\n        conditioner_projection (ConvNorm): Convolutional projection for the conditioner.\n        output_projection (ConvNorm): Convolutional projection for the output.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_encoder: int,\n        residual_channels: int,\n        dropout: float,\n        d_spk_prj: int,\n        multi_speaker: bool = True,\n    ):\n        super().__init__()\n        self.multi_speaker = multi_speaker\n        self.conv_layer = ConvNorm(\n            residual_channels,\n            2 * residual_channels,\n            kernel_size=3,\n            stride=1,\n            padding=int((3 - 1) / 2),\n            dilation=1,\n        )\n        self.diffusion_projection = LinearNorm(residual_channels, residual_channels)\n        if multi_speaker:\n            self.speaker_projection = LinearNorm(d_spk_prj, residual_channels)\n        self.conditioner_projection = ConvNorm(\n            d_encoder, residual_channels, kernel_size=1,\n        )\n        self.output_projection = ConvNorm(\n            residual_channels, 2 * residual_channels, kernel_size=1,\n        )\n\n    def forward(\n        self,\n        x: Tensor,\n        conditioner: Tensor,\n        diffusion_step: Tensor,\n        speaker_emb: Tensor,\n        mask: Optional[Tensor] = None,\n    ):\n        r\"\"\"Forward pass through the ResidualBlock module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            conditioner (torch.Tensor): Conditioner tensor.\n            diffusion_step (torch.Tensor): Diffusion step tensor.\n            speaker_emb (torch.Tensor): Speaker embedding tensor.\n            mask (torch.Tensor, optional): Mask tensor. Defaults to None.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Tuple containing the output tensor and skip tensor.\n        \"\"\"\n        diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n        conditioner = self.conditioner_projection(conditioner)\n        # conditioner = self.conditioner_projection(conditioner.transpose(1, 2))\n        if self.multi_speaker:\n            # speaker_emb = self.speaker_projection(speaker_emb).unsqueeze(1).expand(\n            #     -1, conditioner.shape[-1], -1,\n            # ).transpose(1, 2)\n            speaker_emb = self.speaker_projection(speaker_emb).expand(\n                -1, conditioner.shape[-1], -1,\n            ).transpose(1, 2)\n\n        residual = y = x + diffusion_step\n        y = self.conv_layer(\n            (y + conditioner + speaker_emb) if self.multi_speaker else (y + conditioner),\n        )\n        gate, filter = torch.chunk(y, 2, dim=1)\n        y = torch.sigmoid(gate) * torch.tanh(filter)\n\n        y = self.output_projection(y)\n        x, skip = torch.chunk(y, 2, dim=1)\n\n        return (x + residual) / math.sqrt(2.0), skip\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/layers/#models.enhancer.gaussian_diffusion.layers.ResidualBlock.forward","title":"<code>forward(x, conditioner, diffusion_step, speaker_emb, mask=None)</code>","text":"<p>Forward pass through the ResidualBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>conditioner</code> <code>Tensor</code> <p>Conditioner tensor.</p> required <code>diffusion_step</code> <code>Tensor</code> <p>Diffusion step tensor.</p> required <code>speaker_emb</code> <code>Tensor</code> <p>Speaker embedding tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Mask tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple[torch.Tensor, torch.Tensor]: Tuple containing the output tensor and skip tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/layers.py</code> <pre><code>def forward(\n    self,\n    x: Tensor,\n    conditioner: Tensor,\n    diffusion_step: Tensor,\n    speaker_emb: Tensor,\n    mask: Optional[Tensor] = None,\n):\n    r\"\"\"Forward pass through the ResidualBlock module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n        conditioner (torch.Tensor): Conditioner tensor.\n        diffusion_step (torch.Tensor): Diffusion step tensor.\n        speaker_emb (torch.Tensor): Speaker embedding tensor.\n        mask (torch.Tensor, optional): Mask tensor. Defaults to None.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Tuple containing the output tensor and skip tensor.\n    \"\"\"\n    diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n    conditioner = self.conditioner_projection(conditioner)\n    # conditioner = self.conditioner_projection(conditioner.transpose(1, 2))\n    if self.multi_speaker:\n        # speaker_emb = self.speaker_projection(speaker_emb).unsqueeze(1).expand(\n        #     -1, conditioner.shape[-1], -1,\n        # ).transpose(1, 2)\n        speaker_emb = self.speaker_projection(speaker_emb).expand(\n            -1, conditioner.shape[-1], -1,\n        ).transpose(1, 2)\n\n    residual = y = x + diffusion_step\n    y = self.conv_layer(\n        (y + conditioner + speaker_emb) if self.multi_speaker else (y + conditioner),\n    )\n    gate, filter = torch.chunk(y, 2, dim=1)\n    y = torch.sigmoid(gate) * torch.tanh(filter)\n\n    y = self.output_projection(y)\n    x, skip = torch.chunk(y, 2, dim=1)\n\n    return (x + residual) / math.sqrt(2.0), skip\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/","title":"Utils","text":""},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.default","title":"<code>default(val, d)</code>","text":"<p>Return the input value if it exists, otherwise return a default value.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Any</code> <p>Input value.</p> required <code>d</code> <code>Any</code> <p>Default value or function to generate the default value.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Input value if it exists, otherwise the default value.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def default(val: Any, d: Any) -&gt; Any:\n    r\"\"\"Return the input value if it exists, otherwise return a default value.\n\n    Args:\n        val (Any): Input value.\n        d (Any): Default value or function to generate the default value.\n\n    Returns:\n        Any: Input value if it exists, otherwise the default value.\n    \"\"\"\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.exists","title":"<code>exists(x)</code>","text":"<p>Check if the input variable is not None.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Input variable.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input variable is not None, False otherwise.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def exists(x: Any) -&gt; bool:\n    r\"\"\"Check if the input variable is not None.\n\n    Args:\n        x (Any): Input variable.\n\n    Returns:\n        bool: True if the input variable is not None, False otherwise.\n    \"\"\"\n    return x is not None\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.extract","title":"<code>extract(a, t, x_shape)</code>","text":"<p>Extract elements from tensor 'a' using indices 't'.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Tensor</code> <p>Input tensor.</p> required <code>t</code> <code>Tensor</code> <p>Indices tensor.</p> required <code>x_shape</code> <code>Size</code> <p>Shape of the input tensor 'a'.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Extracted elements tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def extract(\n    a: Tensor,\n    t: Tensor,\n    x_shape: Size,\n):\n    r\"\"\"Extract elements from tensor 'a' using indices 't'.\n\n    Args:\n        a (torch.Tensor): Input tensor.\n        t (torch.Tensor): Indices tensor.\n        x_shape (Size): Shape of the input tensor 'a'.\n\n    Returns:\n        torch.Tensor: Extracted elements tensor.\n    \"\"\"\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.get_noise_schedule_list","title":"<code>get_noise_schedule_list(schedule_mode, timesteps, min_beta=0.0, max_beta=0.01, s=0.008)</code>","text":"<p>Generate a noise schedule list based on the specified mode.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_mode</code> <code>str</code> <p>Mode for generating the noise schedule.                   Can be one of [\"linear\", \"cosine\", \"vpsde\"].</p> required <code>timesteps</code> <code>int</code> <p>Total number of time steps.</p> required <code>min_beta</code> <code>float</code> <p>Minimum value of beta for VPSDE mode. Defaults to 0.0.</p> <code>0.0</code> <code>max_beta</code> <code>float</code> <p>Maximum value of beta for VPSDE mode. Defaults to 0.01.</p> <code>0.01</code> <code>s</code> <code>float</code> <p>Parameter for cosine schedule mode. Defaults to 0.008.</p> <code>0.008</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: List or array of beta coefficients for each time step.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def get_noise_schedule_list(\n    schedule_mode: str,\n    timesteps: int,\n    min_beta: float = 0.0,\n    max_beta: float = 0.01,\n    s: float = 0.008,\n) -&gt; np.ndarray:\n    r\"\"\"Generate a noise schedule list based on the specified mode.\n\n    Args:\n        schedule_mode (str): Mode for generating the noise schedule. \n                             Can be one of [\"linear\", \"cosine\", \"vpsde\"].\n        timesteps (int): Total number of time steps.\n        min_beta (float, optional): Minimum value of beta for VPSDE mode. Defaults to 0.0.\n        max_beta (float, optional): Maximum value of beta for VPSDE mode. Defaults to 0.01.\n        s (float, optional): Parameter for cosine schedule mode. Defaults to 0.008.\n\n    Returns:\n        np.ndarray: List or array of beta coefficients for each time step.\n    \"\"\"\n    if schedule_mode == \"linear\":\n        schedule_list = np.linspace(1e-4, max_beta, timesteps)\n    elif schedule_mode == \"cosine\":\n        steps = timesteps + 1\n        x = np.linspace(0, steps, steps)\n        alphas_cumprod = np.cos(((x / steps) + s) / (1 + s) * np.pi * 0.5) ** 2\n        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n        schedule_list = np.clip(betas, a_min=0, a_max=0.999)\n    elif schedule_mode == \"vpsde\":\n        schedule_list = np.array([\n            vpsde_beta_t(t, timesteps, min_beta, max_beta) for t in range(1, timesteps + 1)])\n    else:\n        raise NotImplementedError\n    return schedule_list\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.noise_like","title":"<code>noise_like(shape, device, repeat=False)</code>","text":"<p>Generate random noise tensor with the given shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Size</code> <p>Shape of the noise tensor.</p> required <code>device</code> <code>device</code> <p>Device for the tensor.</p> required <code>repeat</code> <code>bool</code> <p>If True, repeat the noise tensor to match the given shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>torch.Tensor: Random noise tensor.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def noise_like(\n    shape: Size,\n    device: torch.device,\n    repeat: bool = False,\n):\n    r\"\"\"Generate random noise tensor with the given shape.\n\n    Args:\n        shape (Size): Shape of the noise tensor.\n        device (torch.device): Device for the tensor.\n        repeat (bool, optional): If True, repeat the noise tensor to match the given shape. Defaults to False.\n\n    Returns:\n        torch.Tensor: Random noise tensor.\n    \"\"\"\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n</code></pre>"},{"location":"models/enhancer/gaussian_diffusion/utils/#models.enhancer.gaussian_diffusion.utils.vpsde_beta_t","title":"<code>vpsde_beta_t(t, T, min_beta, max_beta)</code>","text":"<p>Calculate beta coefficient for VPSDE noise schedule at time step t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>int</code> <p>Current time step.</p> required <code>T</code> <code>int</code> <p>Total number of time steps.</p> required <code>min_beta</code> <code>float</code> <p>Minimum value of beta.</p> required <code>max_beta</code> <code>float</code> <p>Maximum value of beta.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Beta coefficient at time step t.</p> Source code in <code>models/enhancer/gaussian_diffusion/utils.py</code> <pre><code>def vpsde_beta_t(\n    t: int,\n    T: int,\n    min_beta: float,\n    max_beta: float,\n):\n    r\"\"\"Calculate beta coefficient for VPSDE noise schedule at time step t.\n\n    Args:\n        t (int): Current time step.\n        T (int): Total number of time steps.\n        min_beta (float): Minimum value of beta.\n        max_beta (float): Maximum value of beta.\n\n    Returns:\n        float: Beta coefficient at time step t.\n    \"\"\"\n    t_coef = (2 * t - 1) / (T ** 2)\n    return 1. - np.exp(-min_beta / T - 0.5 * (max_beta - min_beta) * t_coef)\n</code></pre>"},{"location":"models/helpers/acoustic/","title":"Acoustic","text":""},{"location":"models/helpers/acoustic/#models.helpers.acoustic.pitch_phoneme_averaging","title":"<code>pitch_phoneme_averaging(durations, pitches, max_phoneme_len)</code>","text":"<p>Function to compute the average pitch values over the duration of each phoneme.</p> <p>Parameters:</p> Name Type Description Default <code>durations</code> <code>Tensor</code> <p>Duration of each phoneme for each sample in a batch.                       Shape: (batch_size, n_phones)</p> required <code>pitches</code> <code>Tensor</code> <p>Per-frame pitch values for each sample in a batch.                     Shape: (batch_size, n_mel_timesteps)</p> required <code>max_phoneme_len</code> <code>int</code> <p>Maximum length of the phoneme sequence in a batch.</p> required <p>Returns:</p> Name Type Description <code>pitches_averaged</code> <code>Tensor</code> <p>Tensor containing the averaged pitch values                              for each phoneme. Shape: (batch_size, max_phoneme_len)</p> Source code in <code>models/helpers/acoustic.py</code> <pre><code>def pitch_phoneme_averaging(\n        durations: torch.Tensor,\n        pitches: torch.Tensor,\n        max_phoneme_len: int) -&gt; torch.Tensor:\n    r\"\"\"Function to compute the average pitch values over the duration of each phoneme.\n\n    Args:\n        durations (torch.Tensor): Duration of each phoneme for each sample in a batch.\n                                  Shape: (batch_size, n_phones)\n        pitches (torch.Tensor): Per-frame pitch values for each sample in a batch.\n                                Shape: (batch_size, n_mel_timesteps)\n        max_phoneme_len (int): Maximum length of the phoneme sequence in a batch.\n\n    Returns:\n        pitches_averaged (torch.Tensor): Tensor containing the averaged pitch values\n                                         for each phoneme. Shape: (batch_size, max_phoneme_len)\n    \"\"\"\n    # Initialize placeholder for averaged pitch values, filling with zeros\n    pitches_averaged = torch.zeros(\n        (pitches.shape[0], max_phoneme_len), device=pitches.device,\n    )\n    # Loop over each sample in the batch\n    for batch_idx in range(durations.shape[0]):\n        # Set the starting index of pitch sequence\n        start_idx = 0\n        # Loop over each phoneme duration\n        for i, duration in enumerate(durations[batch_idx]):\n            # Convert duration to integer\n            duration = duration.int().item()\n            # If the duration is not zero\n            if duration != 0:\n                # Calculate the mean pitch value for the duration of the current phoneme\n                mean = torch.mean(pitches[batch_idx, start_idx : start_idx + duration])\n                # Store the averaged pitch value\n                pitches_averaged[batch_idx][i] = mean\n                # Update the starting index for the next phoneme\n                start_idx += duration\n\n    # Return tensor with the averaged pitch values\n    return pitches_averaged\n</code></pre>"},{"location":"models/helpers/acoustic/#models.helpers.acoustic.positional_encoding","title":"<code>positional_encoding(d_model, length)</code>","text":"<p>Function to calculate positional encoding for transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimension of the model (often corresponds to embedding size).</p> required <code>length</code> <code>int</code> <p>Length of sequences.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor having positional encodings.</p> Source code in <code>models/helpers/acoustic.py</code> <pre><code>def positional_encoding(\n    d_model: int, length: int,\n) -&gt; torch.Tensor:\n    r\"\"\"Function to calculate positional encoding for transformer model.\n\n    Args:\n        d_model (int): Dimension of the model (often corresponds to embedding size).\n        length (int): Length of sequences.\n\n    Returns:\n        torch.Tensor: Tensor having positional encodings.\n    \"\"\"\n    # Initialize placeholder for positional encoding\n    pe = torch.zeros(length, d_model)\n\n    # Generate position indices and reshape to have shape (length, 1)\n    position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n\n    # Calculate term for division\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float()\n        * -(math.log(10000.0) / d_model),\n    )\n\n    # Assign sin of position * div_term to even indices in the encoding matrix\n    pe[:, 0::2] = torch.sin(position * div_term)\n\n    # Assign cos of position * div_term to odd indices in the encoding matrix\n    pe[:, 1::2] = torch.cos(position * div_term)\n\n    # Add an extra dimension to match expected output shape\n    return pe.unsqueeze(0)\n</code></pre>"},{"location":"models/helpers/initializer/","title":"Initializer","text":""},{"location":"models/helpers/initializer/#models.helpers.initializer.get_test_configs","title":"<code>get_test_configs(srink_factor=4)</code>","text":"<p>Returns a tuple of configuration objects for testing purposes.</p> <p>Parameters:</p> Name Type Description Default <code>srink_factor</code> <code>int</code> <p>The shrink factor to apply to the model configuration. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>Tuple[PreprocessingConfigUnivNet, AcousticENModelConfig, AcousticPretrainingConfig]</code> <p>Tuple[PreprocessingConfig, AcousticENModelConfig, AcousticPretrainingConfig]: A tuple of configuration objects for testing purposes.</p> <p>This function returns a tuple of configuration objects for testing purposes. The configuration objects are as follows: - <code>PreprocessingConfig</code>: A configuration object for preprocessing. - <code>AcousticENModelConfig</code>: A configuration object for the acoustic model. - <code>AcousticPretrainingConfig</code>: A configuration object for acoustic pretraining.</p> <p>The <code>srink_factor</code> parameter is used to shrink the dimensions of the model configuration to prevent out of memory issues during testing.</p> Source code in <code>models/helpers/initializer.py</code> <pre><code>def get_test_configs(\n    srink_factor: int = 4,\n) -&gt; Tuple[PreprocessingConfig, AcousticENModelConfig, AcousticPretrainingConfig]:\n    r\"\"\"Returns a tuple of configuration objects for testing purposes.\n\n    Args:\n        srink_factor (int, optional): The shrink factor to apply to the model configuration. Defaults to 4.\n\n    Returns:\n        Tuple[PreprocessingConfig, AcousticENModelConfig, AcousticPretrainingConfig]: A tuple of configuration objects for testing purposes.\n\n    This function returns a tuple of configuration objects for testing purposes. The configuration objects are as follows:\n    - `PreprocessingConfig`: A configuration object for preprocessing.\n    - `AcousticENModelConfig`: A configuration object for the acoustic model.\n    - `AcousticPretrainingConfig`: A configuration object for acoustic pretraining.\n\n    The `srink_factor` parameter is used to shrink the dimensions of the model configuration to prevent out of memory issues during testing.\n    \"\"\"\n    preprocess_config = PreprocessingConfig(\"english_only\")\n    model_config = AcousticENModelConfig()\n\n    model_config.speaker_embed_dim = model_config.speaker_embed_dim // srink_factor\n    model_config.encoder.n_hidden = model_config.encoder.n_hidden // srink_factor\n    model_config.decoder.n_hidden = model_config.decoder.n_hidden // srink_factor\n    model_config.variance_adaptor.n_hidden = (\n        model_config.variance_adaptor.n_hidden // srink_factor\n    )\n\n    acoustic_pretraining_config = AcousticPretrainingConfig()\n\n    return (preprocess_config, model_config, acoustic_pretraining_config)\n</code></pre>"},{"location":"models/helpers/initializer/#models.helpers.initializer.init_acoustic_model","title":"<code>init_acoustic_model(preprocess_config, model_config, n_speakers=10)</code>","text":"<p>Function to initialize an <code>AcousticModel</code> with given preprocessing and model configurations.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfigUnivNet</code> <p>Configuration object for pre-processing.</p> required <code>model_config</code> <code>AcousticENModelConfig</code> <p>Configuration object for English Acoustic model.</p> required <code>n_speakers</code> <code>int</code> <p>Number of speakers. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>AcousticModel</code> <code>Tuple[AcousticModel, AcousticModelConfig]</code> <p>Initialized Acoustic Model.</p> <p>The function creates an <code>AcousticModelConfig</code> instance which is then used to initialize the <code>AcousticModel</code>. The <code>AcousticModelConfig</code> is configured as follows: - preprocess_config: Pre-processing configuration. - model_config: English Acoustic model configuration. - fine_tuning: Boolean flag set to True indicating the model is for fine-tuning. - n_speakers: Number of speakers.</p> Source code in <code>models/helpers/initializer.py</code> <pre><code>def init_acoustic_model(\n    preprocess_config: PreprocessingConfig,\n    model_config: AcousticENModelConfig,\n    n_speakers: int = 10,\n) -&gt; Tuple[AcousticModel, AcousticModelConfig]:\n    r\"\"\"Function to initialize an `AcousticModel` with given preprocessing and model configurations.\n\n    Args:\n        preprocess_config (PreprocessingConfig): Configuration object for pre-processing.\n        model_config (AcousticENModelConfig): Configuration object for English Acoustic model.\n        n_speakers (int, optional): Number of speakers. Defaults to 10.\n\n    Returns:\n        AcousticModel: Initialized Acoustic Model.\n\n    The function creates an `AcousticModelConfig` instance which is then used to initialize the `AcousticModel`.\n    The `AcousticModelConfig` is configured as follows:\n    - preprocess_config: Pre-processing configuration.\n    - model_config: English Acoustic model configuration.\n    - fine_tuning: Boolean flag set to True indicating the model is for fine-tuning.\n    - n_speakers: Number of speakers.\n\n    \"\"\"\n    # Create an AcousticModelConfig instance\n    acoustic_model_config = AcousticModelConfig(\n        preprocess_config=preprocess_config,\n        model_config=model_config,\n        n_speakers=n_speakers,\n    )\n\n    model = AcousticModel(**vars(acoustic_model_config))\n\n    return model, acoustic_model_config\n</code></pre>"},{"location":"models/helpers/initializer/#models.helpers.initializer.init_conformer","title":"<code>init_conformer(model_config)</code>","text":"<p>Function to initialize a <code>Conformer</code> with a given <code>AcousticModelConfigType</code> configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>AcousticModelConfigType</code> <p>The object that holds the configuration details.</p> required <p>Returns:</p> Name Type Description <code>Conformer</code> <code>Tuple[Conformer, ConformerConfig]</code> <p>Initialized Conformer object.</p> <p>The function sets the details of the <code>Conformer</code> object based on the <code>model_config</code> parameter. The <code>Conformer</code> configuration is set as follows: - dim: The number of hidden units, taken from the encoder part of the <code>model_config.encoder.n_hidden</code>. - n_layers: The number of layers, taken from the encoder part of the <code>model_config.encoder.n_layers</code>. - n_heads: The number of attention heads, taken from the encoder part of the <code>model_config.encoder.n_heads</code>. - embedding_dim: The sum of dimensions of speaker embeddings and language embeddings.   The speaker_embed_dim and lang_embed_dim are a part of the <code>model_config.speaker_embed_dim</code>. - p_dropout: Dropout rate taken from the encoder part of the <code>model_config.encoder.p_dropout</code>.   It adds a regularization parameter to prevent overfitting. - kernel_size_conv_mod: The kernel size for the convolution module taken from the encoder part of the <code>model_config.encoder.kernel_size_conv_mod</code>. - with_ff: A Boolean value denoting if feedforward operation is involved, taken from the encoder part of the <code>model_config.encoder.with_ff</code>.</p> Source code in <code>models/helpers/initializer.py</code> <pre><code>def init_conformer(\n    model_config: AcousticModelConfigType,\n) -&gt; Tuple[Conformer, ConformerConfig]:\n    r\"\"\"Function to initialize a `Conformer` with a given `AcousticModelConfigType` configuration.\n\n    Args:\n        model_config (AcousticModelConfigType): The object that holds the configuration details.\n\n    Returns:\n        Conformer: Initialized Conformer object.\n\n    The function sets the details of the `Conformer` object based on the `model_config` parameter.\n    The `Conformer` configuration is set as follows:\n    - dim: The number of hidden units, taken from the encoder part of the `model_config.encoder.n_hidden`.\n    - n_layers: The number of layers, taken from the encoder part of the `model_config.encoder.n_layers`.\n    - n_heads: The number of attention heads, taken from the encoder part of the `model_config.encoder.n_heads`.\n    - embedding_dim: The sum of dimensions of speaker embeddings and language embeddings.\n      The speaker_embed_dim and lang_embed_dim are a part of the `model_config.speaker_embed_dim`.\n    - p_dropout: Dropout rate taken from the encoder part of the `model_config.encoder.p_dropout`.\n      It adds a regularization parameter to prevent overfitting.\n    - kernel_size_conv_mod: The kernel size for the convolution module taken from the encoder part of the `model_config.encoder.kernel_size_conv_mod`.\n    - with_ff: A Boolean value denoting if feedforward operation is involved, taken from the encoder part of the `model_config.encoder.with_ff`.\n\n    \"\"\"\n    conformer_config = ConformerConfig(\n        dim=model_config.encoder.n_hidden,\n        n_layers=model_config.encoder.n_layers,\n        n_heads=model_config.encoder.n_heads,\n        embedding_dim=model_config.speaker_embed_dim\n        + model_config.lang_embed_dim,  # speaker_embed_dim + lang_embed_dim = 385\n        p_dropout=model_config.encoder.p_dropout,\n        kernel_size_conv_mod=model_config.encoder.kernel_size_conv_mod,\n        with_ff=model_config.encoder.with_ff,\n    )\n\n    model = Conformer(**vars(conformer_config))\n\n    return model, conformer_config\n</code></pre>"},{"location":"models/helpers/initializer/#models.helpers.initializer.init_forward_trains_params","title":"<code>init_forward_trains_params(model_config, acoustic_pretraining_config, preprocess_config, n_speakers=10)</code>","text":"<p>Function to initialize the parameters for forward propagation during training.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>AcousticENModelConfig</code> <p>Configuration object for English Acoustic model.</p> required <code>acoustic_pretraining_config</code> <code>AcousticPretrainingConfig</code> <p>Configuration object for acoustic pretraining.</p> required <code>preprocess_config</code> <code>PreprocessingConfigUnivNet</code> <p>Configuration object for pre-processing.</p> required <code>n_speakers</code> <code>int</code> <p>Number of speakers. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>ForwardTrainParams</code> <code>ForwardTrainParams</code> <p>Initialized parameters for forward propagation during training.</p> <p>The function initializes the ForwardTrainParams object with the following parameters: - x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size] - speakers: Tensor containing the speaker indices. Shape: [speaker_embed_dim, batch_size] - src_lens: Tensor containing the lengths of source sequences. Shape: [batch_size] - mels: Tensor containing the mel spectrogram. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden] - mel_lens: Tensor containing the lengths of mel sequences. Shape: [batch_size] - pitches: Tensor containing the pitch values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden] - energies: Tensor containing the energy values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden] - langs: Tensor containing the language indices. Shape: [speaker_embed_dim, batch_size] - attn_priors: Tensor containing the attention priors. Shape: [batch_size, speaker_embed_dim, speaker_embed_dim] - use_ground_truth: Boolean flag indicating if ground truth values should be used or not.</p> <p>All the Tensors are initialized with random values.</p> Source code in <code>models/helpers/initializer.py</code> <pre><code>def init_forward_trains_params(\n    model_config: AcousticENModelConfig,\n    acoustic_pretraining_config: AcousticPretrainingConfig,\n    preprocess_config: PreprocessingConfig,\n    n_speakers: int = 10,\n) -&gt; ForwardTrainParams:\n    r\"\"\"Function to initialize the parameters for forward propagation during training.\n\n    Args:\n        model_config (AcousticENModelConfig): Configuration object for English Acoustic model.\n        acoustic_pretraining_config (AcousticPretrainingConfig): Configuration object for acoustic pretraining.\n        preprocess_config (PreprocessingConfig): Configuration object for pre-processing.\n        n_speakers (int, optional): Number of speakers. Defaults to 10.\n\n    Returns:\n        ForwardTrainParams: Initialized parameters for forward propagation during training.\n\n    The function initializes the ForwardTrainParams object with the following parameters:\n    - x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size]\n    - speakers: Tensor containing the speaker indices. Shape: [speaker_embed_dim, batch_size]\n    - src_lens: Tensor containing the lengths of source sequences. Shape: [batch_size]\n    - mels: Tensor containing the mel spectrogram. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden]\n    - mel_lens: Tensor containing the lengths of mel sequences. Shape: [batch_size]\n    - pitches: Tensor containing the pitch values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden]\n    - energies: Tensor containing the energy values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden]\n    - langs: Tensor containing the language indices. Shape: [speaker_embed_dim, batch_size]\n    - attn_priors: Tensor containing the attention priors. Shape: [batch_size, speaker_embed_dim, speaker_embed_dim]\n    - use_ground_truth: Boolean flag indicating if ground truth values should be used or not.\n\n    All the Tensors are initialized with random values.\n    \"\"\"\n    return ForwardTrainParams(\n        # x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size]\n        x=torch.randint(\n            1,\n            255,\n            (\n                model_config.speaker_embed_dim,\n                acoustic_pretraining_config.batch_size,\n            ),\n        ),\n        pitches_range=(0.0, 1.0),\n        # speakers: Tensor containing the speaker indices. Shape: [speaker_embed_dim, batch_size]\n        speakers=torch.randint(\n            1,\n            n_speakers - 1,\n            (\n                model_config.speaker_embed_dim,\n                acoustic_pretraining_config.batch_size,\n            ),\n        ),\n        # src_lens: Tensor containing the lengths of source sequences. Shape: [speaker_embed_dim]\n        src_lens=torch.cat(\n            [\n                # torch.tensor([self.model_config.speaker_embed_dim]),\n                torch.randint(\n                    1,\n                    acoustic_pretraining_config.batch_size + 1,\n                    (model_config.speaker_embed_dim,),\n                ),\n            ],\n            dim=0,\n        ),\n        # mels: Tensor containing the mel spectrogram. Shape: [batch_size, stft.n_mel_channels, encoder.n_hidden]\n        mels=torch.randn(\n            model_config.speaker_embed_dim,\n            preprocess_config.stft.n_mel_channels,\n            model_config.encoder.n_hidden,\n        ),\n        # enc_len: Tensor containing the lengths of mel sequences. Shape: [speaker_embed_dim]\n        enc_len=torch.cat(\n            [\n                torch.randint(\n                    1,\n                    model_config.speaker_embed_dim,\n                    (model_config.speaker_embed_dim - 1,),\n                ),\n                torch.tensor([model_config.speaker_embed_dim]),\n            ],\n            dim=0,\n        ),\n        # mel_lens: Tensor containing the lengths of mel sequences. Shape: [batch_size]\n        mel_lens=torch.cat(\n            [\n                torch.randint(\n                    1,\n                    model_config.speaker_embed_dim,\n                    (model_config.speaker_embed_dim - 1,),\n                ),\n                torch.tensor([model_config.speaker_embed_dim]),\n            ],\n            dim=0,\n        ),\n        # pitches: Tensor containing the pitch values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden]\n        pitches=torch.randn(\n            # acoustic_pretraining_config.batch_size,\n            model_config.speaker_embed_dim,\n            # model_config.speaker_embed_dim,\n            model_config.encoder.n_hidden,\n        ),\n        # energies: Tensor containing the energy values. Shape: [batch_size, speaker_embed_dim, encoder.n_hidden]\n        energies=torch.randn(\n            model_config.speaker_embed_dim,\n            1,\n            model_config.encoder.n_hidden,\n        ),\n        # langs: Tensor containing the language indices. Shape: [speaker_embed_dim, batch_size]\n        langs=torch.randint(\n            1,\n            len(SUPPORTED_LANGUAGES) - 1,\n            (\n                model_config.speaker_embed_dim,\n                acoustic_pretraining_config.batch_size,\n            ),\n        ),\n        # attn_priors: Tensor containing the attention priors. Shape: [batch_size, speaker_embed_dim, speaker_embed_dim]\n        attn_priors=torch.randn(\n            model_config.speaker_embed_dim,\n            model_config.speaker_embed_dim,\n            acoustic_pretraining_config.batch_size,\n        ),\n        use_ground_truth=True,\n    )\n</code></pre>"},{"location":"models/helpers/initializer/#models.helpers.initializer.init_mask_input_embeddings_encoding_attn_mask","title":"<code>init_mask_input_embeddings_encoding_attn_mask(acoustic_model, forward_train_params, model_config)</code>","text":"<p>Function to initialize masks for padding positions, input sequences, embeddings, positional encoding and attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>acoustic_model</code> <code>AcousticModel</code> <p>Initialized Acoustic Model.</p> required <code>forward_train_params</code> <code>ForwardTrainParams</code> <p>Parameters for the forward training process.</p> required <code>model_config</code> <code>AcousticENModelConfig</code> <p>Configuration object for English Acoustic model.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the following elements: - src_mask: Tensor containing the masks for padding positions in the source sequences. Shape: [1, batch_size] - x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim] - embeddings: Tensor containing the embeddings. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim + lang_embed_dim] - encoding: Tensor containing the positional encoding. Shape: [lang_embed_dim, max(forward_train_params.mel_lens), model_config.encoder.n_hidden] - attn_mask\u0416 Tensor containing the attention masks. Shape: [1, 1, 1, batch_size]</p> <p>The function starts by generating masks for padding positions in the source and mel sequences. Then, it uses the acoustic model to get the input sequences and embeddings. Finally, it computes the positional encoding.</p> Source code in <code>models/helpers/initializer.py</code> <pre><code>def init_mask_input_embeddings_encoding_attn_mask(\n    acoustic_model: AcousticModel,\n    forward_train_params: ForwardTrainParams,\n    model_config: AcousticENModelConfig,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Function to initialize masks for padding positions, input sequences, embeddings, positional encoding and attention masks.\n\n    Args:\n        acoustic_model (AcousticModel): Initialized Acoustic Model.\n        forward_train_params (ForwardTrainParams): Parameters for the forward training process.\n        model_config (AcousticENModelConfig): Configuration object for English Acoustic model.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the following elements:\n            - src_mask: Tensor containing the masks for padding positions in the source sequences. Shape: [1, batch_size]\n            - x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim]\n            - embeddings: Tensor containing the embeddings. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim + lang_embed_dim]\n            - encoding: Tensor containing the positional encoding. Shape: [lang_embed_dim, max(forward_train_params.mel_lens), model_config.encoder.n_hidden]\n            - attn_mask\u0416 Tensor containing the attention masks. Shape: [1, 1, 1, batch_size]\n\n    The function starts by generating masks for padding positions in the source and mel sequences.\n    Then, it uses the acoustic model to get the input sequences and embeddings.\n    Finally, it computes the positional encoding.\n\n    \"\"\"\n    # Generate masks for padding positions in the source sequences and mel sequences\n    # src_mask: Tensor containing the masks for padding positions in the source sequences. Shape: [1, batch_size]\n    src_mask = tools.get_mask_from_lengths(forward_train_params.src_lens)\n\n    # x: Tensor containing the input sequences. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim]\n    # embeddings: Tensor containing the embeddings. Shape: [speaker_embed_dim, batch_size, speaker_embed_dim + lang_embed_dim]\n    x, embeddings = acoustic_model.get_embeddings(\n        token_idx=forward_train_params.x,\n        speaker_idx=forward_train_params.speakers,\n        src_mask=src_mask,\n        lang_idx=forward_train_params.langs,\n    )\n\n    # encoding: Tensor containing the positional encoding\n    # Shape: [lang_embed_dim, max(forward_train_params.mel_lens), encoder.n_hidden]\n    encoding = positional_encoding(\n        model_config.encoder.n_hidden,\n        max(x.shape[1], int(forward_train_params.mel_lens.max().item())),\n    )\n\n    attn_mask = src_mask.view((src_mask.shape[0], 1, 1, src_mask.shape[1]))\n\n    return src_mask, x, embeddings, encoding, attn_mask\n</code></pre>"},{"location":"models/helpers/readme/","title":"References","text":""},{"location":"models/helpers/readme/#references","title":"References","text":""},{"location":"models/helpers/readme/#tools","title":"Tools","text":"<p>Models tools.</p>"},{"location":"models/helpers/readme/#initializer","title":"Initializer","text":"<p>This is a helper that is used to initialize a <code>Conformer</code> with a given <code>AcousticModelConfigType</code> configuration, initialize an <code>AcousticModel</code> with given preprocessing and model configurations, an the parameters for forward propagation during training. Used for the test cases.</p>"},{"location":"models/helpers/readme/#acoustic","title":"Acoustic","text":"<p>Acoustic model helpers.</p>"},{"location":"models/helpers/tools/","title":"Tools","text":""},{"location":"models/helpers/tools/#models.helpers.tools.calc_same_padding","title":"<code>calc_same_padding(kernel_size)</code>","text":"<p>Calculates the necessary padding for 'same' padding in convolutional operations.</p> <p>For 'same' padding, the output size is the same as the input size for <code>stride=1</code>. This function returns two integers, representing the padding to be added on either side of the input to achieve 'same' padding.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int</code> <p>Size of the convolving kernel.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Tuple[int, int]: A tuple of two integers representing the number of padding elements to be applied on</p> <code>int</code> <p>left and right (or top and bottom for 2D) of the input tensor respectively.</p> Source code in <code>models/helpers/tools.py</code> <pre><code>def calc_same_padding(kernel_size: int) -&gt; Tuple[int, int]:\n    r\"\"\"Calculates the necessary padding for 'same' padding in convolutional operations.\n\n    For 'same' padding, the output size is the same as the input size for `stride=1`. This function returns\n    two integers, representing the padding to be added on either side of the input to achieve 'same' padding.\n\n    Args:\n        kernel_size (int): Size of the convolving kernel.\n\n    Returns:\n        Tuple[int, int]: A tuple of two integers representing the number of padding elements to be applied on\n        left and right (or top and bottom for 2D) of the input tensor respectively.\n    \"\"\"\n    # Check if kernel_size is an integer greater than zero\n    if not isinstance(kernel_size, int) or kernel_size &lt;= 0:\n        raise ValueError(\"kernel_size must be an integer greater than zero\")\n\n    # Determine base padding amount (equal to half the kernel size, truncated down)\n    pad = kernel_size // 2\n\n    # Return padding for each side of the kernel. If kernel size is odd, padding is (pad, pad).\n    # If kernel size is even, padding is (pad, pad - 1) because we can't pad equally on both sides.\n    return (pad, pad - (kernel_size + 1) % 2)\n</code></pre>"},{"location":"models/helpers/tools/#models.helpers.tools.get_device","title":"<code>get_device()</code>","text":"<p>Function returns the device where the model and tensors should be placed.</p> <p>Returns     torch.device: The device where the model and tensors should be placed.</p> Source code in <code>models/helpers/tools.py</code> <pre><code>def get_device() -&gt; torch.device:\n    r\"\"\"Function returns the device where the model and tensors should be placed.\n\n    Returns\n        torch.device: The device where the model and tensors should be placed.\n    \"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"models/helpers/tools/#models.helpers.tools.get_mask_from_lengths","title":"<code>get_mask_from_lengths(lengths)</code>","text":"<p>Generate a mask tensor from a tensor of sequence lengths.</p> <p>Parameters:</p> Name Type Description Default <code>lengths</code> <code>Tensor</code> <p>A tensor of sequence lengths of shape: (batch_size, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A mask tensor of shape: (batch_size, max_len) where max_len is the maximum sequence length in the provided tensor. The mask tensor has a value of True at each position that is more than the length of the sequence (padding positions).</p> Example <p>lengths: <code>torch.tensor([2, 3, 1, 4])</code> Mask tensor will be: <code>torch.tensor([       [False, False, True, True],       [False, False, False, True],       [False, True, True, True],       [False, False, False, False]   ])</code></p> Source code in <code>models/helpers/tools.py</code> <pre><code>def get_mask_from_lengths(lengths: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Generate a mask tensor from a tensor of sequence lengths.\n\n    Args:\n        lengths (torch.Tensor): A tensor of sequence lengths of shape: (batch_size, )\n\n    Returns:\n        torch.Tensor: A mask tensor of shape: (batch_size, max_len) where max_len is the\n            maximum sequence length in the provided tensor. The mask tensor has a value of\n            True at each position that is more than the length of the sequence (padding positions).\n\n    Example:\n      lengths: `torch.tensor([2, 3, 1, 4])`\n      Mask tensor will be: `torch.tensor([\n            [False, False, True, True],\n            [False, False, False, True],\n            [False, True, True, True],\n            [False, False, False, False]\n        ])`\n    \"\"\"\n    # Get batch size\n    batch_size = lengths.shape[0]\n\n    # Get maximum sequence length in the batch\n    max_len = int(torch.max(lengths).item())\n\n    # Generate a tensor of shape (batch_size, max_len)\n    # where each row contains values from 0 to max_len\n    ids = (\n        torch.arange(0, max_len, device=lengths.device)\n        .unsqueeze(0)\n        .expand(batch_size, -1)\n    )\n    # Compare each value in the ids tensor with\n    # corresponding sequence length to generate a mask.\n    # The mask will have True at positions where id &gt;= sequence length,\n    # indicating padding positions in the original sequences\n    return ids &gt;= lengths.unsqueeze(1).type(torch.int64).expand(-1, max_len)\n</code></pre>"},{"location":"models/helpers/tools/#models.helpers.tools.initialize_embeddings","title":"<code>initialize_embeddings(shape)</code>","text":"<p>Initialize embeddings using Kaiming initialization (He initialization).</p> <p>This method is specifically designed for 2D matrices and helps to avoid the vanishing/exploding gradient problem in deep neural networks. This is achieved by keeping the variance of the outputs of a layer to be the same as the variance of its inputs.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the embedding matrix to create, denoted as a tuple of integers.                      The shape should comprise 2 dimensions, i.e., (embedding_dim, num_embeddings).</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided shape is not 2D.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: the created embedding matrix.</p> Source code in <code>models/helpers/tools.py</code> <pre><code>def initialize_embeddings(shape: Tuple[int, ...]) -&gt; torch.Tensor:\n    r\"\"\"Initialize embeddings using Kaiming initialization (He initialization).\n\n    This method is specifically designed for 2D matrices and helps to avoid\n    the vanishing/exploding gradient problem in deep neural networks.\n    This is achieved by keeping the variance of the outputs of a layer to be\n    the same as the variance of its inputs.\n\n    Args:\n        shape (Tuple[int, ...]): The shape of the embedding matrix to create, denoted as a tuple of integers.\n                                 The shape should comprise 2 dimensions, i.e., (embedding_dim, num_embeddings).\n\n    Raises:\n        AssertionError: if the provided shape is not 2D.\n\n    Returns:\n        torch.Tensor: the created embedding matrix.\n    \"\"\"\n    # Check if the input shape is 2D\n    assert len(shape) == 2, \"Can only initialize 2-D embedding matrices ...\"\n\n    # Initialize the embedding matrix using Kaiming initialization\n    return torch.randn(shape) * np.sqrt(2 / shape[1])\n</code></pre>"},{"location":"models/helpers/tools/#models.helpers.tools.pad","title":"<code>pad(input_ele, max_len)</code>","text":"<p>Takes a list of 1D or 2D tensors and pads them to match the maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>input_ele</code> <code>List[Tensor]</code> <p>The list of tensors to be padded.</p> required <code>max_len</code> <code>int</code> <p>The length to which the tensors should be padded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing all the padded input tensors.</p> Source code in <code>models/helpers/tools.py</code> <pre><code>def pad(input_ele: List[torch.Tensor], max_len: int) -&gt; torch.Tensor:\n    r\"\"\"Takes a list of 1D or 2D tensors and pads them to match the maximum length.\n\n    Args:\n        input_ele (List[torch.Tensor]): The list of tensors to be padded.\n        max_len (int): The length to which the tensors should be padded.\n\n    Returns:\n        torch.Tensor: A tensor containing all the padded input tensors.\n    \"\"\"\n    # Create an empty list to store the padded tensors\n    out_list = torch.jit.annotate(List[torch.Tensor], [])\n    for batch in input_ele:\n        if len(batch.shape) == 1:\n            # Perform padding for 1D tensor\n            one_batch_padded = F.pad(\n                batch, (0, max_len - batch.size(0)), \"constant\", 0.0,\n            )\n        else:\n            # Perform padding for 2D tensor\n            one_batch_padded = F.pad(\n                batch, (0, 0, 0, max_len - batch.size(0)), \"constant\", 0.0,\n            )\n        # Append the padded tensor to the list\n        out_list.append(one_batch_padded)\n\n    # Stack all the tensors in the list into a single tensor\n    return torch.stack(out_list)\n</code></pre>"},{"location":"models/helpers/tools/#models.helpers.tools.stride_lens_downsampling","title":"<code>stride_lens_downsampling(lens, stride=2)</code>","text":"<p>Function computes the lengths of 1D tensor when applying a stride for downsampling.</p> <p>Parameters:</p> Name Type Description Default <code>lens</code> <code>Tensor</code> <p>Tensor containing the lengths to be downsampled.</p> required <code>stride</code> <code>int</code> <p>The stride to be used for downsampling. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of the same shape as the input containing the downsampled lengths.</p> Source code in <code>models/helpers/tools.py</code> <pre><code>def stride_lens_downsampling(lens: torch.Tensor, stride: int = 2) -&gt; torch.Tensor:\n    r\"\"\"Function computes the lengths of 1D tensor when applying a stride for downsampling.\n\n    Args:\n        lens (torch.Tensor): Tensor containing the lengths to be downsampled.\n        stride (int, optional): The stride to be used for downsampling. Defaults to 2.\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as the input containing the downsampled lengths.\n    \"\"\"\n    # The torch.ceil function is used to handle cases where the length is not evenly divisible\n    # by the stride. The torch.ceil function rounds up to the nearest integer, ensuring that\n    # each item is present at least once in the downsampled lengths.\n    # Finally, the .int() is used to convert the resulting float32 tensor to an integer tensor.\n    return torch.ceil(lens / stride).int()\n</code></pre>"},{"location":"models/tts/delightful_tts/delightful_tts/","title":"DelightfulTTS","text":""},{"location":"models/tts/delightful_tts/delightful_tts/#models.tts.delightful_tts.delightful_tts.DelightfulTTS","title":"<code>DelightfulTTS</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Trainer for the acoustic model.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>The preprocessing configuration.</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>The model configuration.</p> <code>AcousticMultilingualModelConfig()</code> <code>fine_tuning</code> <code>bool</code> <p>Whether to use fine-tuning mode or not. Defaults to False.</p> <code>False</code> <code>bin_warmup</code> <code>bool</code> <p>Whether to use binarization warmup for the loss or not. Defaults to True.</p> <code>True</code> <code>lang</code> <code>str</code> <p>Language of the dataset.</p> <code>'en'</code> <code>n_speakers</code> <code>int</code> <p>Number of speakers in the dataset.generation during training.</p> <code>5392</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>19</code> Source code in <code>models/tts/delightful_tts/delightful_tts.py</code> <pre><code>class DelightfulTTS(LightningModule):\n    r\"\"\"Trainer for the acoustic model.\n\n    Args:\n        preprocess_config PreprocessingConfig: The preprocessing configuration.\n        model_config AcousticModelConfigType: The model configuration.\n        fine_tuning (bool, optional): Whether to use fine-tuning mode or not. Defaults to False.\n        bin_warmup (bool, optional): Whether to use binarization warmup for the loss or not. Defaults to True.\n        lang (str): Language of the dataset.\n        n_speakers (int): Number of speakers in the dataset.generation during training.\n        batch_size (int): The batch size.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        model_config: AcousticModelConfigType = AcousticMultilingualModelConfig(),\n        fine_tuning: bool = False,\n        bin_warmup: bool = True,\n        lang: str = \"en\",\n        n_speakers: int = 5392,\n        batch_size: int = 19,\n    ):\n        super().__init__()\n\n        self.lang = lang\n        self.lang_id = lang2id[self.lang]\n\n        self.fine_tuning = fine_tuning\n        self.batch_size = batch_size\n\n        lang_map = get_lang_map(lang)\n        normilize_text_lang = lang_map.nemo\n\n        self.tokenizer = TokenizerIPA(lang)\n        self.normilize_text = NormalizeText(normilize_text_lang)\n\n        self.train_config_acoustic: AcousticTrainingConfig\n\n        if self.fine_tuning:\n            self.train_config_acoustic = AcousticFinetuningConfig()\n        else:\n            self.train_config_acoustic = AcousticPretrainingConfig()\n\n        self.preprocess_config = preprocess_config\n\n        # TODO: fix the arguments!\n        self.acoustic_model = AcousticModel(\n            preprocess_config=self.preprocess_config,\n            model_config=model_config,\n            # NOTE: this parameter may be hyperparameter that you can define based on the demands\n            n_speakers=n_speakers,\n        )\n\n        # NOTE: in case of training from 0 bin_warmup should be True!\n        self.loss_acoustic = FastSpeech2LossGen(\n            bin_warmup=bin_warmup,\n        )\n\n    def forward(\n        self,\n        text: str,\n        speaker_idx: Tensor,\n    ) -&gt; Tensor:\n        r\"\"\"Performs a forward pass through the AcousticModel.\n        This code must be run only with the loaded weights from the checkpoint!\n\n        Args:\n            text (str): The input text.\n            speaker_idx (Tensor): The index of the speaker\n\n        Returns:\n            Tensor: The generated waveform with hifi-gan.\n        \"\"\"\n        normalized_text = self.normilize_text(text)\n        _, phones = self.tokenizer(normalized_text)\n\n        # Convert to tensor\n        x = torch.tensor(\n            phones,\n            dtype=torch.int,\n            device=speaker_idx.device,\n        ).unsqueeze(0)\n\n        speakers = speaker_idx.repeat(x.shape[1]).unsqueeze(0)\n\n        langs = (\n            torch.tensor(\n                [self.lang_id],\n                dtype=torch.int,\n                device=speaker_idx.device,\n            )\n            .repeat(x.shape[1])\n            .unsqueeze(0)\n        )\n\n        mel_pred = self.acoustic_model.forward(\n            x=x,\n            speakers=speakers,\n            langs=langs,\n        )\n\n        return mel_pred\n\n    def training_step(self, batch: List, _: int):\n        r\"\"\"Performs a training step for the model.\n\n        Args:\n        batch (List): The batch of data for training. The batch should contain:\n            - ids: List of indexes.\n            - raw_texts: Raw text inputs.\n            - speakers: Speaker identities.\n            - texts: Text inputs.\n            - src_lens: Lengths of the source sequences.\n            - mels: Mel spectrogram targets.\n            - pitches: Pitch targets.\n            - pitches_stat: Statistics of the pitches.\n            - mel_lens: Lengths of the mel spectrograms.\n            - langs: Language identities.\n            - attn_priors: Prior attention weights.\n            - wavs: Waveform targets.\n            - energies: Energy targets.\n        batch_idx (int): Index of the batch.\n\n        Returns:\n            - 'loss': The total loss for the training step.\n        \"\"\"\n        (\n            _,\n            _,\n            speakers,\n            texts,\n            src_lens,\n            mels,\n            pitches,\n            _,\n            mel_lens,\n            langs,\n            attn_priors,\n            _,\n            energies,\n        ) = batch\n\n        outputs = self.acoustic_model.forward_train(\n            x=texts,\n            speakers=speakers,\n            src_lens=src_lens,\n            mels=mels,\n            mel_lens=mel_lens,\n            pitches=pitches,\n            langs=langs,\n            attn_priors=attn_priors,\n            energies=energies,\n        )\n\n        y_pred = outputs[\"y_pred\"]\n        log_duration_prediction = outputs[\"log_duration_prediction\"]\n        p_prosody_ref = outputs[\"p_prosody_ref\"]\n        p_prosody_pred = outputs[\"p_prosody_pred\"]\n        pitch_prediction = outputs[\"pitch_prediction\"]\n        energy_pred = outputs[\"energy_pred\"]\n        energy_target = outputs[\"energy_target\"]\n\n        src_mask = get_mask_from_lengths(src_lens)\n        mel_mask = get_mask_from_lengths(mel_lens)\n\n        (\n            total_loss,\n            mel_loss,\n            ssim_loss,\n            duration_loss,\n            u_prosody_loss,\n            p_prosody_loss,\n            pitch_loss,\n            ctc_loss,\n            bin_loss,\n            energy_loss,\n        ) = self.loss_acoustic.forward(\n            src_masks=src_mask,\n            mel_masks=mel_mask,\n            mel_targets=mels,\n            mel_predictions=y_pred,\n            log_duration_predictions=log_duration_prediction,\n            u_prosody_ref=outputs[\"u_prosody_ref\"],\n            u_prosody_pred=outputs[\"u_prosody_pred\"],\n            p_prosody_ref=p_prosody_ref,\n            p_prosody_pred=p_prosody_pred,\n            pitch_predictions=pitch_prediction,\n            p_targets=outputs[\"pitch_target\"],\n            durations=outputs[\"attn_hard_dur\"],\n            attn_logprob=outputs[\"attn_logprob\"],\n            attn_soft=outputs[\"attn_soft\"],\n            attn_hard=outputs[\"attn_hard\"],\n            src_lens=src_lens,\n            mel_lens=mel_lens,\n            energy_pred=energy_pred,\n            energy_target=energy_target,\n            step=self.trainer.global_step,\n        )\n\n        self.log(\n            \"train_total_loss\",\n            total_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\"train_mel_loss\", mel_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\n            \"train_ssim_loss\",\n            ssim_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\n            \"train_duration_loss\",\n            duration_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\n            \"train_u_prosody_loss\",\n            u_prosody_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\n            \"train_p_prosody_loss\",\n            p_prosody_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\n            \"train_pitch_loss\",\n            pitch_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\"train_ctc_loss\", ctc_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\"train_bin_loss\", bin_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\n            \"train_energy_loss\",\n            energy_loss,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n\n        return total_loss\n\n    def configure_optimizers(self):\n        r\"\"\"Configures the optimizer used for training.\n\n        Returns\n            tuple: A tuple containing three dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.\n        \"\"\"\n        lr_decay = self.train_config_acoustic.optimizer_config.lr_decay\n        default_lr = self.train_config_acoustic.optimizer_config.learning_rate\n\n        init_lr = (\n            default_lr\n            if self.trainer.global_step == 0\n            else default_lr * (lr_decay**self.trainer.global_step)\n        )\n\n        optimizer_acoustic = AdamW(\n            self.acoustic_model.parameters(),\n            lr=init_lr,\n            betas=self.train_config_acoustic.optimizer_config.betas,\n            eps=self.train_config_acoustic.optimizer_config.eps,\n            weight_decay=self.train_config_acoustic.optimizer_config.weight_decay,\n        )\n\n        scheduler_acoustic = ExponentialLR(optimizer_acoustic, gamma=lr_decay)\n\n        return {\n            \"optimizer\": optimizer_acoustic,\n            \"lr_scheduler\": scheduler_acoustic,\n        }\n\n    def train_dataloader(\n        self,\n        root: str = \"datasets_cache\",\n        cache: bool = True,\n        cache_dir: str = \"/dev/shm\",\n        include_libri: bool = False,\n        libri_speakers: List[str] = speakers_libri_ids,\n        hifi_speakers: List[str] = speakers_hifi_ids,\n    ) -&gt; DataLoader:\n        r\"\"\"Returns the training dataloader, that is using the LibriTTS dataset.\n\n        Args:\n            root (str): The root directory of the dataset.\n            cache (bool): Whether to cache the preprocessed data.\n            cache_dir (str): The directory for the cache. Defaults to \"/dev/shm\".\n            include_libri (bool): Whether to include the LibriTTS dataset or not.\n            libri_speakers (List[str]): The list of LibriTTS speakers to include.\n            hifi_speakers (List[str]): The list of HiFi-GAN speakers to include.\n\n        Returns:\n            Tupple[DataLoader, DataLoader]: The training and validation dataloaders.\n        \"\"\"\n        return train_dataloader(\n            batch_size=self.batch_size,\n            num_workers=self.preprocess_config.workers,\n            sampling_rate=self.preprocess_config.sampling_rate,\n            root=root,\n            cache=cache,\n            cache_dir=cache_dir,\n            lang=self.lang,\n            include_libri=include_libri,\n            libri_speakers=libri_speakers,\n            hifi_speakers=hifi_speakers,\n        )\n</code></pre>"},{"location":"models/tts/delightful_tts/delightful_tts/#models.tts.delightful_tts.delightful_tts.DelightfulTTS.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer used for training.</p> <p>Returns     tuple: A tuple containing three dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.</p> Source code in <code>models/tts/delightful_tts/delightful_tts.py</code> <pre><code>def configure_optimizers(self):\n    r\"\"\"Configures the optimizer used for training.\n\n    Returns\n        tuple: A tuple containing three dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.\n    \"\"\"\n    lr_decay = self.train_config_acoustic.optimizer_config.lr_decay\n    default_lr = self.train_config_acoustic.optimizer_config.learning_rate\n\n    init_lr = (\n        default_lr\n        if self.trainer.global_step == 0\n        else default_lr * (lr_decay**self.trainer.global_step)\n    )\n\n    optimizer_acoustic = AdamW(\n        self.acoustic_model.parameters(),\n        lr=init_lr,\n        betas=self.train_config_acoustic.optimizer_config.betas,\n        eps=self.train_config_acoustic.optimizer_config.eps,\n        weight_decay=self.train_config_acoustic.optimizer_config.weight_decay,\n    )\n\n    scheduler_acoustic = ExponentialLR(optimizer_acoustic, gamma=lr_decay)\n\n    return {\n        \"optimizer\": optimizer_acoustic,\n        \"lr_scheduler\": scheduler_acoustic,\n    }\n</code></pre>"},{"location":"models/tts/delightful_tts/delightful_tts/#models.tts.delightful_tts.delightful_tts.DelightfulTTS.forward","title":"<code>forward(text, speaker_idx)</code>","text":"<p>Performs a forward pass through the AcousticModel. This code must be run only with the loaded weights from the checkpoint!</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>speaker_idx</code> <code>Tensor</code> <p>The index of the speaker</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The generated waveform with hifi-gan.</p> Source code in <code>models/tts/delightful_tts/delightful_tts.py</code> <pre><code>def forward(\n    self,\n    text: str,\n    speaker_idx: Tensor,\n) -&gt; Tensor:\n    r\"\"\"Performs a forward pass through the AcousticModel.\n    This code must be run only with the loaded weights from the checkpoint!\n\n    Args:\n        text (str): The input text.\n        speaker_idx (Tensor): The index of the speaker\n\n    Returns:\n        Tensor: The generated waveform with hifi-gan.\n    \"\"\"\n    normalized_text = self.normilize_text(text)\n    _, phones = self.tokenizer(normalized_text)\n\n    # Convert to tensor\n    x = torch.tensor(\n        phones,\n        dtype=torch.int,\n        device=speaker_idx.device,\n    ).unsqueeze(0)\n\n    speakers = speaker_idx.repeat(x.shape[1]).unsqueeze(0)\n\n    langs = (\n        torch.tensor(\n            [self.lang_id],\n            dtype=torch.int,\n            device=speaker_idx.device,\n        )\n        .repeat(x.shape[1])\n        .unsqueeze(0)\n    )\n\n    mel_pred = self.acoustic_model.forward(\n        x=x,\n        speakers=speakers,\n        langs=langs,\n    )\n\n    return mel_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/delightful_tts/#models.tts.delightful_tts.delightful_tts.DelightfulTTS.train_dataloader","title":"<code>train_dataloader(root='datasets_cache', cache=True, cache_dir='/dev/shm', include_libri=False, libri_speakers=speakers_libri_ids, hifi_speakers=speakers_hifi_ids)</code>","text":"<p>Returns the training dataloader, that is using the LibriTTS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>The root directory of the dataset.</p> <code>'datasets_cache'</code> <code>cache</code> <code>bool</code> <p>Whether to cache the preprocessed data.</p> <code>True</code> <code>cache_dir</code> <code>str</code> <p>The directory for the cache. Defaults to \"/dev/shm\".</p> <code>'/dev/shm'</code> <code>include_libri</code> <code>bool</code> <p>Whether to include the LibriTTS dataset or not.</p> <code>False</code> <code>libri_speakers</code> <code>List[str]</code> <p>The list of LibriTTS speakers to include.</p> <code>speakers_libri_ids</code> <code>hifi_speakers</code> <code>List[str]</code> <p>The list of HiFi-GAN speakers to include.</p> <code>speakers_hifi_ids</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Tupple[DataLoader, DataLoader]: The training and validation dataloaders.</p> Source code in <code>models/tts/delightful_tts/delightful_tts.py</code> <pre><code>def train_dataloader(\n    self,\n    root: str = \"datasets_cache\",\n    cache: bool = True,\n    cache_dir: str = \"/dev/shm\",\n    include_libri: bool = False,\n    libri_speakers: List[str] = speakers_libri_ids,\n    hifi_speakers: List[str] = speakers_hifi_ids,\n) -&gt; DataLoader:\n    r\"\"\"Returns the training dataloader, that is using the LibriTTS dataset.\n\n    Args:\n        root (str): The root directory of the dataset.\n        cache (bool): Whether to cache the preprocessed data.\n        cache_dir (str): The directory for the cache. Defaults to \"/dev/shm\".\n        include_libri (bool): Whether to include the LibriTTS dataset or not.\n        libri_speakers (List[str]): The list of LibriTTS speakers to include.\n        hifi_speakers (List[str]): The list of HiFi-GAN speakers to include.\n\n    Returns:\n        Tupple[DataLoader, DataLoader]: The training and validation dataloaders.\n    \"\"\"\n    return train_dataloader(\n        batch_size=self.batch_size,\n        num_workers=self.preprocess_config.workers,\n        sampling_rate=self.preprocess_config.sampling_rate,\n        root=root,\n        cache=cache,\n        cache_dir=cache_dir,\n        lang=self.lang,\n        include_libri=include_libri,\n        libri_speakers=libri_speakers,\n        hifi_speakers=hifi_speakers,\n    )\n</code></pre>"},{"location":"models/tts/delightful_tts/delightful_tts/#models.tts.delightful_tts.delightful_tts.DelightfulTTS.training_step","title":"<code>training_step(batch, _)</code>","text":"<p>Performs a training step for the model.</p> <p>batch (List): The batch of data for training. The batch should contain:     - ids: List of indexes.     - raw_texts: Raw text inputs.     - speakers: Speaker identities.     - texts: Text inputs.     - src_lens: Lengths of the source sequences.     - mels: Mel spectrogram targets.     - pitches: Pitch targets.     - pitches_stat: Statistics of the pitches.     - mel_lens: Lengths of the mel spectrograms.     - langs: Language identities.     - attn_priors: Prior attention weights.     - wavs: Waveform targets.     - energies: Energy targets. batch_idx (int): Index of the batch.</p> <p>Returns:</p> Type Description <ul> <li>'loss': The total loss for the training step.</li> </ul> Source code in <code>models/tts/delightful_tts/delightful_tts.py</code> <pre><code>def training_step(self, batch: List, _: int):\n    r\"\"\"Performs a training step for the model.\n\n    Args:\n    batch (List): The batch of data for training. The batch should contain:\n        - ids: List of indexes.\n        - raw_texts: Raw text inputs.\n        - speakers: Speaker identities.\n        - texts: Text inputs.\n        - src_lens: Lengths of the source sequences.\n        - mels: Mel spectrogram targets.\n        - pitches: Pitch targets.\n        - pitches_stat: Statistics of the pitches.\n        - mel_lens: Lengths of the mel spectrograms.\n        - langs: Language identities.\n        - attn_priors: Prior attention weights.\n        - wavs: Waveform targets.\n        - energies: Energy targets.\n    batch_idx (int): Index of the batch.\n\n    Returns:\n        - 'loss': The total loss for the training step.\n    \"\"\"\n    (\n        _,\n        _,\n        speakers,\n        texts,\n        src_lens,\n        mels,\n        pitches,\n        _,\n        mel_lens,\n        langs,\n        attn_priors,\n        _,\n        energies,\n    ) = batch\n\n    outputs = self.acoustic_model.forward_train(\n        x=texts,\n        speakers=speakers,\n        src_lens=src_lens,\n        mels=mels,\n        mel_lens=mel_lens,\n        pitches=pitches,\n        langs=langs,\n        attn_priors=attn_priors,\n        energies=energies,\n    )\n\n    y_pred = outputs[\"y_pred\"]\n    log_duration_prediction = outputs[\"log_duration_prediction\"]\n    p_prosody_ref = outputs[\"p_prosody_ref\"]\n    p_prosody_pred = outputs[\"p_prosody_pred\"]\n    pitch_prediction = outputs[\"pitch_prediction\"]\n    energy_pred = outputs[\"energy_pred\"]\n    energy_target = outputs[\"energy_target\"]\n\n    src_mask = get_mask_from_lengths(src_lens)\n    mel_mask = get_mask_from_lengths(mel_lens)\n\n    (\n        total_loss,\n        mel_loss,\n        ssim_loss,\n        duration_loss,\n        u_prosody_loss,\n        p_prosody_loss,\n        pitch_loss,\n        ctc_loss,\n        bin_loss,\n        energy_loss,\n    ) = self.loss_acoustic.forward(\n        src_masks=src_mask,\n        mel_masks=mel_mask,\n        mel_targets=mels,\n        mel_predictions=y_pred,\n        log_duration_predictions=log_duration_prediction,\n        u_prosody_ref=outputs[\"u_prosody_ref\"],\n        u_prosody_pred=outputs[\"u_prosody_pred\"],\n        p_prosody_ref=p_prosody_ref,\n        p_prosody_pred=p_prosody_pred,\n        pitch_predictions=pitch_prediction,\n        p_targets=outputs[\"pitch_target\"],\n        durations=outputs[\"attn_hard_dur\"],\n        attn_logprob=outputs[\"attn_logprob\"],\n        attn_soft=outputs[\"attn_soft\"],\n        attn_hard=outputs[\"attn_hard\"],\n        src_lens=src_lens,\n        mel_lens=mel_lens,\n        energy_pred=energy_pred,\n        energy_target=energy_target,\n        step=self.trainer.global_step,\n    )\n\n    self.log(\n        \"train_total_loss\",\n        total_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\"train_mel_loss\", mel_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\n        \"train_ssim_loss\",\n        ssim_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\n        \"train_duration_loss\",\n        duration_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\n        \"train_u_prosody_loss\",\n        u_prosody_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\n        \"train_p_prosody_loss\",\n        p_prosody_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\n        \"train_pitch_loss\",\n        pitch_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\"train_ctc_loss\", ctc_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\"train_bin_loss\", bin_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\n        \"train_energy_loss\",\n        energy_loss,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n\n    return total_loss\n</code></pre>"},{"location":"models/tts/delightful_tts/readme/","title":"References","text":""},{"location":"models/tts/delightful_tts/readme/#delightfultts","title":"DelightfulTTS","text":"<p>The DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021 </p>"},{"location":"models/tts/delightful_tts/readme/#delightfultts-model","title":"DelightfulTTS Model","text":"<p>The core module for the training.</p>"},{"location":"models/tts/delightful_tts/readme/#acoustic-model","title":"Acoustic Model","text":"<p>AcousticModel class represents a PyTorch module for an acoustic model in text-to-speech (TTS). The acoustic model is responsible for predicting speech signals from phoneme sequences.</p> <p>The model comprises multiple sub-modules including encoder, decoder and various prosody encoders and predictors. Additionally, a pitch and length adaptor are instantiated.</p>"},{"location":"models/tts/delightful_tts/readme/#reference-encoder","title":"Reference Encoder","text":"<p>Similar to Tacotron model, the reference encoder is used to extract the high-level features from the reference</p>"},{"location":"models/tts/delightful_tts/readme/#convolution-blocks","title":"Convolution Blocks","text":"<p>This part of the code responsible for the convolution blocks used in the model. Based on the FastSpeech models from FastSpeech: Fast, Robust and Controllable Text to Speech by Yi Ren et al and FastSpeech 2: Fast and High-Quality End-to-End Text to Speech by Yi Ren et al.</p>"},{"location":"models/tts/delightful_tts/readme/#attention","title":"Attention","text":"<p>Attention mechanizm used in the model. The concept of \"global style tokens\" (GST) was introduced in  Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis by Yuxuan Wang et al.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/","title":"Accoustic Model","text":""},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel","title":"<code>AcousticModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>The DelightfulTTS AcousticModel class represents a PyTorch module for an acoustic model in text-to-speech (TTS). The acoustic model is responsible for predicting speech signals from phoneme sequences.</p> <p>The model comprises multiple sub-modules including encoder, decoder and various prosody encoders and predictors. Additionally, a pitch and length adaptor are instantiated.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>Object containing the configuration used for preprocessing the data</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>Configuration object containing various model parameters</p> required <code>n_speakers</code> <code>int</code> <p>Total number of speakers in the dataset</p> required <code>leaky_relu_slope</code> <code>float</code> <p>Slope for the leaky relu. Defaults to LEAKY_RELU_SLOPE.</p> <code>LEAKY_RELU_SLOPE</code> Note <p>For more specific details on the implementation of sub-modules please refer to their individual respective modules.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>class AcousticModel(Module):\n    r\"\"\"The DelightfulTTS AcousticModel class represents a PyTorch module for an acoustic model in text-to-speech (TTS).\n    The acoustic model is responsible for predicting speech signals from phoneme sequences.\n\n    The model comprises multiple sub-modules including encoder, decoder and various prosody encoders and predictors.\n    Additionally, a pitch and length adaptor are instantiated.\n\n    Args:\n        preprocess_config (PreprocessingConfig): Object containing the configuration used for preprocessing the data\n        model_config (AcousticModelConfigType): Configuration object containing various model parameters\n        n_speakers (int): Total number of speakers in the dataset\n        leaky_relu_slope (float, optional): Slope for the leaky relu. Defaults to LEAKY_RELU_SLOPE.\n\n    Note:\n        For more specific details on the implementation of sub-modules please refer to their individual respective modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        model_config: AcousticModelConfigType,\n        n_speakers: int,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n        self.emb_dim = model_config.encoder.n_hidden\n\n        self.encoder = Conformer(\n            dim=model_config.encoder.n_hidden,\n            n_layers=model_config.encoder.n_layers,\n            n_heads=model_config.encoder.n_heads,\n            embedding_dim=model_config.speaker_embed_dim + model_config.lang_embed_dim,\n            p_dropout=model_config.encoder.p_dropout,\n            kernel_size_conv_mod=model_config.encoder.kernel_size_conv_mod,\n            with_ff=model_config.encoder.with_ff,\n        )\n\n        self.pitch_adaptor_conv = PitchAdaptorConv(\n            channels_in=model_config.encoder.n_hidden,\n            channels_hidden=model_config.variance_adaptor.n_hidden,\n            channels_out=1,\n            kernel_size=model_config.variance_adaptor.kernel_size,\n            emb_kernel_size=model_config.variance_adaptor.emb_kernel_size,\n            dropout=model_config.variance_adaptor.p_dropout,\n            leaky_relu_slope=leaky_relu_slope,\n        )\n\n        self.energy_adaptor = EnergyAdaptor(\n            channels_in=model_config.encoder.n_hidden,\n            channels_hidden=model_config.variance_adaptor.n_hidden,\n            channels_out=1,\n            kernel_size=model_config.variance_adaptor.kernel_size,\n            emb_kernel_size=model_config.variance_adaptor.emb_kernel_size,\n            dropout=model_config.variance_adaptor.p_dropout,\n            leaky_relu_slope=leaky_relu_slope,\n        )\n\n        self.length_regulator = LengthAdaptor(model_config)\n\n        self.utterance_prosody_encoder = UtteranceLevelProsodyEncoder(\n            preprocess_config,\n            model_config,\n        )\n\n        self.utterance_prosody_predictor = PhonemeProsodyPredictor(\n            model_config=model_config,\n            phoneme_level=False,\n        )\n\n        self.phoneme_prosody_encoder = PhonemeLevelProsodyEncoder(\n            preprocess_config,\n            model_config,\n        )\n\n        self.phoneme_prosody_predictor = PhonemeProsodyPredictor(\n            model_config=model_config,\n            phoneme_level=True,\n        )\n\n        self.u_bottle_out = nn.Linear(\n            model_config.reference_encoder.bottleneck_size_u,\n            model_config.encoder.n_hidden,\n        )\n\n        self.u_norm = nn.LayerNorm(\n            model_config.reference_encoder.bottleneck_size_u,\n            elementwise_affine=False,\n        )\n\n        self.p_bottle_out = nn.Linear(\n            model_config.reference_encoder.bottleneck_size_p,\n            model_config.encoder.n_hidden,\n        )\n\n        self.p_norm = nn.LayerNorm(\n            model_config.reference_encoder.bottleneck_size_p,\n            elementwise_affine=False,\n        )\n\n        self.aligner = Aligner(\n            d_enc_in=model_config.encoder.n_hidden,\n            d_dec_in=preprocess_config.stft.n_mel_channels,\n            d_hidden=model_config.encoder.n_hidden,\n        )\n\n        self.decoder = Conformer(\n            dim=model_config.decoder.n_hidden,\n            n_layers=model_config.decoder.n_layers,\n            n_heads=model_config.decoder.n_heads,\n            embedding_dim=model_config.speaker_embed_dim + model_config.lang_embed_dim,\n            p_dropout=model_config.decoder.p_dropout,\n            kernel_size_conv_mod=model_config.decoder.kernel_size_conv_mod,\n            with_ff=model_config.decoder.with_ff,\n        )\n\n        self.src_word_emb = Parameter(\n            tools.initialize_embeddings(\n                (len(symbols), model_config.encoder.n_hidden),\n            ),\n        )\n\n        self.to_mel = nn.Linear(\n            model_config.decoder.n_hidden,\n            preprocess_config.stft.n_mel_channels,\n        )\n\n        # NOTE: here you can manage the speaker embeddings, can be used for the voice export ?\n        # NOTE: flexibility of the model binded by the n_speaker parameter, maybe I can find another way?\n        # NOTE: in LIBRITTS there are 2477 speakers, we can add more, just extend the speaker_embed matrix\n        # Need to think about it more\n        self.speaker_embed = Parameter(\n            tools.initialize_embeddings(\n                (n_speakers, model_config.speaker_embed_dim),\n            ),\n        )\n\n        self.lang_embed = Parameter(\n            tools.initialize_embeddings(\n                (len(SUPPORTED_LANGUAGES), model_config.lang_embed_dim),\n            ),\n        )\n\n    def get_embeddings(\n        self,\n        token_idx: torch.Tensor,\n        speaker_idx: torch.Tensor,\n        src_mask: torch.Tensor,\n        lang_idx: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Given the tokens, speakers, source mask, and language indices, compute\n        the embeddings for tokens, speakers and languages and return the\n        token_embeddings and combined speaker and language embeddings\n\n        Args:\n            token_idx (torch.Tensor): Tensor of token indices.\n            speaker_idx (torch.Tensor): Tensor of speaker identities.\n            src_mask (torch.Tensor): Mask tensor for source sequences.\n            lang_idx (torch.Tensor): Tensor of language indices.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Token embeddings tensor,\n            and combined speaker and language embeddings tensor.\n        \"\"\"\n        token_embeddings = F.embedding(token_idx, self.src_word_emb)\n        # NOTE: here you can manage the speaker embeddings, can be used for the voice export ?\n        speaker_embeds = F.embedding(speaker_idx, self.speaker_embed)\n        lang_embeds = F.embedding(lang_idx, self.lang_embed)\n\n        # Merge the speaker and language embeddings\n        embeddings = torch.cat([speaker_embeds, lang_embeds], dim=2)\n\n        # Apply the mask to the embeddings and token embeddings\n        embeddings = embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n        token_embeddings = token_embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n\n        return token_embeddings, embeddings\n\n    def prepare_for_export(self) -&gt; None:\n        r\"\"\"Prepare the model for export.\n\n        This method is called when the model is about to be exported, such as for deployment\n        or serializing for later use. The method removes unnecessary components that are\n        not needed during inference. Specifically, it removes the phoneme and utterance\n        prosody encoders for this acoustic model. These components are typically used during\n        training and are not needed when the model is used for making predictions.\n\n        Returns\n            None\n        \"\"\"\n        del self.phoneme_prosody_encoder\n        del self.utterance_prosody_encoder\n\n    # NOTE: freeze/unfreeze params changed, because of the conflict with the lightning module\n    def freeze_params(self) -&gt; None:\n        r\"\"\"Freeze the trainable parameters in the model.\n\n        By freezing, the parameters are no longer updated by gradient descent.\n        This is typically done when you want to keep parts of your model fixed while training other parts.\n        For this model, it freezes all parameters and then selectively unfreezes the\n        speaker embeddings and the pitch adaptor's pitch embeddings to allow these components to update during training.\n\n        Returns\n            None\n        \"\"\"\n        for par in self.parameters():\n            par.requires_grad = False\n        self.speaker_embed.requires_grad = True\n\n    # NOTE: freeze/unfreeze params changed, because of the conflict with the lightning module\n    def unfreeze_params(self, freeze_text_embed: bool, freeze_lang_embed: bool) -&gt; None:\n        r\"\"\"Unfreeze the trainable parameters in the model, allowing them to be updated during training.\n\n        This method is typically used to 'unfreeze' previously 'frozen' parameters, making them trainable again.\n        For this model, it unfreezes all parameters and then selectively freezes the\n        text embeddings and language embeddings, if required.\n\n        Args:\n            freeze_text_embed (bool): Flag to indicate if text embeddings should remain frozen.\n            freeze_lang_embed (bool): Flag to indicate if language embeddings should remain frozen.\n\n        Returns:\n            None\n        \"\"\"\n        # Iterate through all model parameters and make them trainable\n        for par in self.parameters():\n            par.requires_grad = True\n\n        # If freeze_text_embed flag is True, keep the source word embeddings frozen\n        if freeze_text_embed:\n            # @fixed self.src_word_emb.parameters has no parameters() method!\n            # for par in self.src_word_emb.parameters():\n            self.src_word_emb.requires_grad = False\n\n        # If freeze_lang_embed flag is True, keep the language embeddings frozen\n        if freeze_lang_embed:\n            self.lang_embed.requires_grad = False\n\n    def average_utterance_prosody(\n        self,\n        u_prosody_pred: torch.Tensor,\n        src_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Compute the average utterance prosody over the length of non-masked elements.\n\n        This method averages the output of the utterance prosody predictor over\n        the sequence lengths (non-masked elements). This function will return\n        a tensor with the same first dimension but singleton trailing dimensions.\n\n        Args:\n            u_prosody_pred (torch.Tensor): Tensor containing the predicted utterance prosody of dimension (batch_size, T, n_features).\n            src_mask (torch.Tensor): Tensor of dimension (batch_size, T) acting as a mask where masked entries are set to False.\n\n        Returns:\n            torch.Tensor: Tensor of dimension (batch_size, 1, n_features) containing average utterance prosody over non-masked sequence length.\n        \"\"\"\n        # Compute the real sequence lengths by negating the mask and summing along the sequence dimension\n        lengths = ((~src_mask) * 1.0).sum(1)\n\n        # Compute the sum of u_prosody_pred across the sequence length dimension,\n        #  then divide by the sequence lengths tensor to calculate the average.\n        #  This performs a broadcasting operation to account for the third dimension (n_features).\n        # Return the averaged prosody prediction\n        return u_prosody_pred.sum(1, keepdim=True) / lengths.view(-1, 1, 1)\n\n    def forward_train(\n        self,\n        x: torch.Tensor,\n        speakers: torch.Tensor,\n        src_lens: torch.Tensor,\n        mels: torch.Tensor,\n        mel_lens: torch.Tensor,\n        pitches: torch.Tensor,\n        langs: torch.Tensor,\n        attn_priors: Union[torch.Tensor, None],\n        energies: torch.Tensor,\n    ) -&gt; Dict[str, torch.Tensor]:\n        r\"\"\"Forward pass during training phase.\n\n        For a given phoneme sequence, speaker identities, sequence lengths, mels,\n        mel lengths, pitches, language, and attention priors, the forward pass\n        processes these inputs through the defined architecture.\n\n        Args:\n            x (torch.Tensor): Tensor of phoneme sequence.\n            speakers (torch.Tensor): Tensor of speaker identities.\n            src_lens (torch.Tensor): Long tensor representing the lengths of source sequences.\n            mels (torch.Tensor): Tensor of mel spectrograms.\n            mel_lens (torch.Tensor): Long tensor representing the lengths of mel sequences.\n            pitches (torch.Tensor): Tensor of pitch values.\n            langs (torch.Tensor): Tensor of language identities.\n            attn_priors (torch.Tensor): Prior attention values.\n            energies (torch.Tensor): Tensor of energy values.\n\n        Returns:\n            Dict[str, torch.Tensor]: Returns the prediction outputs as a dictionary.\n        \"\"\"\n        # Generate masks for padding positions in the source sequences and mel sequences\n        src_mask = tools.get_mask_from_lengths(src_lens)\n        mel_mask = tools.get_mask_from_lengths(mel_lens)\n\n        x, embeddings = self.get_embeddings(\n            token_idx=x,\n            speaker_idx=speakers,\n            src_mask=src_mask,\n            lang_idx=langs,\n        )\n\n        encoding = positional_encoding(\n            self.emb_dim,\n            max(x.shape[1], int(mel_lens.max().item())),\n        )\n        x = x.to(src_mask.device)\n        encoding = encoding.to(src_mask.device)\n        embeddings = embeddings.to(src_mask.device)\n\n        x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n\n        u_prosody_ref = self.u_norm(\n            self.utterance_prosody_encoder(mels=mels, mel_lens=mel_lens),\n        )\n        u_prosody_pred = self.u_norm(\n            self.average_utterance_prosody(\n                u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n                src_mask=src_mask,\n            ),\n        )\n\n        p_prosody_ref = self.p_norm(\n            self.phoneme_prosody_encoder(\n                x=x,\n                src_mask=src_mask,\n                mels=mels,\n                mel_lens=mel_lens,\n                encoding=encoding,\n            ),\n        )\n        p_prosody_pred = self.p_norm(\n            self.phoneme_prosody_predictor(\n                x=x,\n                mask=src_mask,\n            ),\n        )\n\n        x = x + self.u_bottle_out(u_prosody_pred)\n        x = x + self.p_bottle_out(p_prosody_pred)\n\n        # Save the residual for later use\n        x_res = x\n\n        attn_logprob, attn_soft, attn_hard, attn_hard_dur = self.aligner(\n            enc_in=x_res.permute((0, 2, 1)),\n            dec_in=mels,\n            enc_len=src_lens,\n            dec_len=mel_lens,\n            enc_mask=src_mask,\n            attn_prior=attn_priors,\n        )\n\n        attn_hard_dur = attn_hard_dur.to(src_mask.device)\n\n        x, pitch_prediction, avg_pitch_target = (\n            self.pitch_adaptor_conv.add_pitch_embedding_train(\n                x=x,\n                target=pitches,\n                dr=attn_hard_dur,\n                mask=src_mask,\n            )\n        )\n\n        energies = energies.to(src_mask.device)\n\n        x, energy_pred, avg_energy_target = (\n            self.energy_adaptor.add_energy_embedding_train(\n                x=x,\n                target=energies,\n                dr=attn_hard_dur,\n                mask=src_mask,\n            )\n        )\n\n        x, log_duration_prediction, embeddings = self.length_regulator.upsample_train(\n            x=x,\n            x_res=x_res,\n            duration_target=attn_hard_dur,\n            src_mask=src_mask,\n            embeddings=embeddings,\n        )\n\n        # Decode the encoder output to pred mel spectrogram\n        decoder_output = self.decoder(\n            x,\n            mel_mask,\n            embeddings=embeddings,\n            encoding=encoding,\n        )\n\n        y_pred = self.to_mel(decoder_output)\n        y_pred = y_pred.permute((0, 2, 1))\n\n        return {\n            \"y_pred\": y_pred,\n            \"pitch_prediction\": pitch_prediction,\n            \"pitch_target\": avg_pitch_target,\n            \"energy_pred\": energy_pred,\n            \"energy_target\": avg_energy_target,\n            \"log_duration_prediction\": log_duration_prediction,\n            \"u_prosody_pred\": u_prosody_pred,\n            \"u_prosody_ref\": u_prosody_ref,\n            \"p_prosody_pred\": p_prosody_pred,\n            \"p_prosody_ref\": p_prosody_ref,\n            \"attn_logprob\": attn_logprob,\n            \"attn_soft\": attn_soft,\n            \"attn_hard\": attn_hard,\n            \"attn_hard_dur\": attn_hard_dur,\n        }\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        speakers: torch.Tensor,\n        langs: torch.Tensor,\n        d_control: float = 1.0,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Forward pass during model inference.\n\n        The forward pass receives phoneme sequence, speaker identities, languages, pitch control and\n        duration control, conducts a series of operations on these inputs and returns the predicted mel\n        spectrogram.\n\n        Args:\n            x (torch.Tensor): Tensor of phoneme sequences.\n            speakers (torch.Tensor): Tensor of speaker identities.\n            langs (torch.Tensor): Tensor of language identities.\n            d_control (float): Duration control parameter. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor: Predicted mel spectrogram.\n        \"\"\"\n        # Generate masks for padding positions in the source sequences\n        src_mask = tools.get_mask_from_lengths(\n            torch.tensor([x.shape[1]], dtype=torch.int64),\n        ).to(x.device)\n\n        # Obtain the embeddings for the input\n        x, embeddings = self.get_embeddings(\n            token_idx=x,\n            speaker_idx=speakers,\n            src_mask=src_mask,\n            lang_idx=langs,\n        )\n\n        # Generate positional encodings\n        encoding = positional_encoding(\n            self.emb_dim,\n            x.shape[1],\n        ).to(x.device)\n\n        # Process the embeddings through the encoder\n        x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n\n        # Predict prosody at utterance level and phoneme level\n        u_prosody_pred = self.u_norm(\n            self.average_utterance_prosody(\n                u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n                src_mask=src_mask,\n            ),\n        )\n        p_prosody_pred = self.p_norm(\n            self.phoneme_prosody_predictor(\n                x=x,\n                mask=src_mask,\n            ),\n        )\n\n        x = x + self.u_bottle_out(u_prosody_pred)\n        x = x + self.p_bottle_out(p_prosody_pred)\n\n        x_res = x\n\n        x, _ = self.pitch_adaptor_conv.add_pitch_embedding(\n            x=x,\n            mask=src_mask,\n        )\n\n        x, _ = self.energy_adaptor.add_energy_embedding(\n            x=x,\n            mask=src_mask,\n        )\n\n        x, _, embeddings = self.length_regulator.upsample(\n            x=x,\n            x_res=x_res,\n            src_mask=src_mask,\n            control=d_control,\n            embeddings=embeddings,\n        )\n\n        mel_mask = tools.get_mask_from_lengths(\n            torch.tensor([x.shape[1]], dtype=torch.int64),\n        ).to(x.device)\n\n        if x.shape[1] &gt; encoding.shape[1]:\n            encoding = positional_encoding(self.emb_dim, x.shape[1]).to(x.device)\n\n        decoder_output = self.decoder(\n            x,\n            mel_mask,\n            embeddings=embeddings,\n            encoding=encoding,\n        )\n\n        x = self.to_mel(decoder_output)\n        x = x.permute((0, 2, 1))\n\n        return x\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.average_utterance_prosody","title":"<code>average_utterance_prosody(u_prosody_pred, src_mask)</code>","text":"<p>Compute the average utterance prosody over the length of non-masked elements.</p> <p>This method averages the output of the utterance prosody predictor over the sequence lengths (non-masked elements). This function will return a tensor with the same first dimension but singleton trailing dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>u_prosody_pred</code> <code>Tensor</code> <p>Tensor containing the predicted utterance prosody of dimension (batch_size, T, n_features).</p> required <code>src_mask</code> <code>Tensor</code> <p>Tensor of dimension (batch_size, T) acting as a mask where masked entries are set to False.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor of dimension (batch_size, 1, n_features) containing average utterance prosody over non-masked sequence length.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def average_utterance_prosody(\n    self,\n    u_prosody_pred: torch.Tensor,\n    src_mask: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"Compute the average utterance prosody over the length of non-masked elements.\n\n    This method averages the output of the utterance prosody predictor over\n    the sequence lengths (non-masked elements). This function will return\n    a tensor with the same first dimension but singleton trailing dimensions.\n\n    Args:\n        u_prosody_pred (torch.Tensor): Tensor containing the predicted utterance prosody of dimension (batch_size, T, n_features).\n        src_mask (torch.Tensor): Tensor of dimension (batch_size, T) acting as a mask where masked entries are set to False.\n\n    Returns:\n        torch.Tensor: Tensor of dimension (batch_size, 1, n_features) containing average utterance prosody over non-masked sequence length.\n    \"\"\"\n    # Compute the real sequence lengths by negating the mask and summing along the sequence dimension\n    lengths = ((~src_mask) * 1.0).sum(1)\n\n    # Compute the sum of u_prosody_pred across the sequence length dimension,\n    #  then divide by the sequence lengths tensor to calculate the average.\n    #  This performs a broadcasting operation to account for the third dimension (n_features).\n    # Return the averaged prosody prediction\n    return u_prosody_pred.sum(1, keepdim=True) / lengths.view(-1, 1, 1)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.forward","title":"<code>forward(x, speakers, langs, d_control=1.0)</code>","text":"<p>Forward pass during model inference.</p> <p>The forward pass receives phoneme sequence, speaker identities, languages, pitch control and duration control, conducts a series of operations on these inputs and returns the predicted mel spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor of phoneme sequences.</p> required <code>speakers</code> <code>Tensor</code> <p>Tensor of speaker identities.</p> required <code>langs</code> <code>Tensor</code> <p>Tensor of language identities.</p> required <code>d_control</code> <code>float</code> <p>Duration control parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Predicted mel spectrogram.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    speakers: torch.Tensor,\n    langs: torch.Tensor,\n    d_control: float = 1.0,\n) -&gt; torch.Tensor:\n    r\"\"\"Forward pass during model inference.\n\n    The forward pass receives phoneme sequence, speaker identities, languages, pitch control and\n    duration control, conducts a series of operations on these inputs and returns the predicted mel\n    spectrogram.\n\n    Args:\n        x (torch.Tensor): Tensor of phoneme sequences.\n        speakers (torch.Tensor): Tensor of speaker identities.\n        langs (torch.Tensor): Tensor of language identities.\n        d_control (float): Duration control parameter. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: Predicted mel spectrogram.\n    \"\"\"\n    # Generate masks for padding positions in the source sequences\n    src_mask = tools.get_mask_from_lengths(\n        torch.tensor([x.shape[1]], dtype=torch.int64),\n    ).to(x.device)\n\n    # Obtain the embeddings for the input\n    x, embeddings = self.get_embeddings(\n        token_idx=x,\n        speaker_idx=speakers,\n        src_mask=src_mask,\n        lang_idx=langs,\n    )\n\n    # Generate positional encodings\n    encoding = positional_encoding(\n        self.emb_dim,\n        x.shape[1],\n    ).to(x.device)\n\n    # Process the embeddings through the encoder\n    x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n\n    # Predict prosody at utterance level and phoneme level\n    u_prosody_pred = self.u_norm(\n        self.average_utterance_prosody(\n            u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n            src_mask=src_mask,\n        ),\n    )\n    p_prosody_pred = self.p_norm(\n        self.phoneme_prosody_predictor(\n            x=x,\n            mask=src_mask,\n        ),\n    )\n\n    x = x + self.u_bottle_out(u_prosody_pred)\n    x = x + self.p_bottle_out(p_prosody_pred)\n\n    x_res = x\n\n    x, _ = self.pitch_adaptor_conv.add_pitch_embedding(\n        x=x,\n        mask=src_mask,\n    )\n\n    x, _ = self.energy_adaptor.add_energy_embedding(\n        x=x,\n        mask=src_mask,\n    )\n\n    x, _, embeddings = self.length_regulator.upsample(\n        x=x,\n        x_res=x_res,\n        src_mask=src_mask,\n        control=d_control,\n        embeddings=embeddings,\n    )\n\n    mel_mask = tools.get_mask_from_lengths(\n        torch.tensor([x.shape[1]], dtype=torch.int64),\n    ).to(x.device)\n\n    if x.shape[1] &gt; encoding.shape[1]:\n        encoding = positional_encoding(self.emb_dim, x.shape[1]).to(x.device)\n\n    decoder_output = self.decoder(\n        x,\n        mel_mask,\n        embeddings=embeddings,\n        encoding=encoding,\n    )\n\n    x = self.to_mel(decoder_output)\n    x = x.permute((0, 2, 1))\n\n    return x\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.forward_train","title":"<code>forward_train(x, speakers, src_lens, mels, mel_lens, pitches, langs, attn_priors, energies)</code>","text":"<p>Forward pass during training phase.</p> <p>For a given phoneme sequence, speaker identities, sequence lengths, mels, mel lengths, pitches, language, and attention priors, the forward pass processes these inputs through the defined architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor of phoneme sequence.</p> required <code>speakers</code> <code>Tensor</code> <p>Tensor of speaker identities.</p> required <code>src_lens</code> <code>Tensor</code> <p>Long tensor representing the lengths of source sequences.</p> required <code>mels</code> <code>Tensor</code> <p>Tensor of mel spectrograms.</p> required <code>mel_lens</code> <code>Tensor</code> <p>Long tensor representing the lengths of mel sequences.</p> required <code>pitches</code> <code>Tensor</code> <p>Tensor of pitch values.</p> required <code>langs</code> <code>Tensor</code> <p>Tensor of language identities.</p> required <code>attn_priors</code> <code>Tensor</code> <p>Prior attention values.</p> required <code>energies</code> <code>Tensor</code> <p>Tensor of energy values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor]: Returns the prediction outputs as a dictionary.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def forward_train(\n    self,\n    x: torch.Tensor,\n    speakers: torch.Tensor,\n    src_lens: torch.Tensor,\n    mels: torch.Tensor,\n    mel_lens: torch.Tensor,\n    pitches: torch.Tensor,\n    langs: torch.Tensor,\n    attn_priors: Union[torch.Tensor, None],\n    energies: torch.Tensor,\n) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"Forward pass during training phase.\n\n    For a given phoneme sequence, speaker identities, sequence lengths, mels,\n    mel lengths, pitches, language, and attention priors, the forward pass\n    processes these inputs through the defined architecture.\n\n    Args:\n        x (torch.Tensor): Tensor of phoneme sequence.\n        speakers (torch.Tensor): Tensor of speaker identities.\n        src_lens (torch.Tensor): Long tensor representing the lengths of source sequences.\n        mels (torch.Tensor): Tensor of mel spectrograms.\n        mel_lens (torch.Tensor): Long tensor representing the lengths of mel sequences.\n        pitches (torch.Tensor): Tensor of pitch values.\n        langs (torch.Tensor): Tensor of language identities.\n        attn_priors (torch.Tensor): Prior attention values.\n        energies (torch.Tensor): Tensor of energy values.\n\n    Returns:\n        Dict[str, torch.Tensor]: Returns the prediction outputs as a dictionary.\n    \"\"\"\n    # Generate masks for padding positions in the source sequences and mel sequences\n    src_mask = tools.get_mask_from_lengths(src_lens)\n    mel_mask = tools.get_mask_from_lengths(mel_lens)\n\n    x, embeddings = self.get_embeddings(\n        token_idx=x,\n        speaker_idx=speakers,\n        src_mask=src_mask,\n        lang_idx=langs,\n    )\n\n    encoding = positional_encoding(\n        self.emb_dim,\n        max(x.shape[1], int(mel_lens.max().item())),\n    )\n    x = x.to(src_mask.device)\n    encoding = encoding.to(src_mask.device)\n    embeddings = embeddings.to(src_mask.device)\n\n    x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n\n    u_prosody_ref = self.u_norm(\n        self.utterance_prosody_encoder(mels=mels, mel_lens=mel_lens),\n    )\n    u_prosody_pred = self.u_norm(\n        self.average_utterance_prosody(\n            u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n            src_mask=src_mask,\n        ),\n    )\n\n    p_prosody_ref = self.p_norm(\n        self.phoneme_prosody_encoder(\n            x=x,\n            src_mask=src_mask,\n            mels=mels,\n            mel_lens=mel_lens,\n            encoding=encoding,\n        ),\n    )\n    p_prosody_pred = self.p_norm(\n        self.phoneme_prosody_predictor(\n            x=x,\n            mask=src_mask,\n        ),\n    )\n\n    x = x + self.u_bottle_out(u_prosody_pred)\n    x = x + self.p_bottle_out(p_prosody_pred)\n\n    # Save the residual for later use\n    x_res = x\n\n    attn_logprob, attn_soft, attn_hard, attn_hard_dur = self.aligner(\n        enc_in=x_res.permute((0, 2, 1)),\n        dec_in=mels,\n        enc_len=src_lens,\n        dec_len=mel_lens,\n        enc_mask=src_mask,\n        attn_prior=attn_priors,\n    )\n\n    attn_hard_dur = attn_hard_dur.to(src_mask.device)\n\n    x, pitch_prediction, avg_pitch_target = (\n        self.pitch_adaptor_conv.add_pitch_embedding_train(\n            x=x,\n            target=pitches,\n            dr=attn_hard_dur,\n            mask=src_mask,\n        )\n    )\n\n    energies = energies.to(src_mask.device)\n\n    x, energy_pred, avg_energy_target = (\n        self.energy_adaptor.add_energy_embedding_train(\n            x=x,\n            target=energies,\n            dr=attn_hard_dur,\n            mask=src_mask,\n        )\n    )\n\n    x, log_duration_prediction, embeddings = self.length_regulator.upsample_train(\n        x=x,\n        x_res=x_res,\n        duration_target=attn_hard_dur,\n        src_mask=src_mask,\n        embeddings=embeddings,\n    )\n\n    # Decode the encoder output to pred mel spectrogram\n    decoder_output = self.decoder(\n        x,\n        mel_mask,\n        embeddings=embeddings,\n        encoding=encoding,\n    )\n\n    y_pred = self.to_mel(decoder_output)\n    y_pred = y_pred.permute((0, 2, 1))\n\n    return {\n        \"y_pred\": y_pred,\n        \"pitch_prediction\": pitch_prediction,\n        \"pitch_target\": avg_pitch_target,\n        \"energy_pred\": energy_pred,\n        \"energy_target\": avg_energy_target,\n        \"log_duration_prediction\": log_duration_prediction,\n        \"u_prosody_pred\": u_prosody_pred,\n        \"u_prosody_ref\": u_prosody_ref,\n        \"p_prosody_pred\": p_prosody_pred,\n        \"p_prosody_ref\": p_prosody_ref,\n        \"attn_logprob\": attn_logprob,\n        \"attn_soft\": attn_soft,\n        \"attn_hard\": attn_hard,\n        \"attn_hard_dur\": attn_hard_dur,\n    }\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.freeze_params","title":"<code>freeze_params()</code>","text":"<p>Freeze the trainable parameters in the model.</p> <p>By freezing, the parameters are no longer updated by gradient descent. This is typically done when you want to keep parts of your model fixed while training other parts. For this model, it freezes all parameters and then selectively unfreezes the speaker embeddings and the pitch adaptor's pitch embeddings to allow these components to update during training.</p> <p>Returns     None</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def freeze_params(self) -&gt; None:\n    r\"\"\"Freeze the trainable parameters in the model.\n\n    By freezing, the parameters are no longer updated by gradient descent.\n    This is typically done when you want to keep parts of your model fixed while training other parts.\n    For this model, it freezes all parameters and then selectively unfreezes the\n    speaker embeddings and the pitch adaptor's pitch embeddings to allow these components to update during training.\n\n    Returns\n        None\n    \"\"\"\n    for par in self.parameters():\n        par.requires_grad = False\n    self.speaker_embed.requires_grad = True\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.get_embeddings","title":"<code>get_embeddings(token_idx, speaker_idx, src_mask, lang_idx)</code>","text":"<p>Given the tokens, speakers, source mask, and language indices, compute the embeddings for tokens, speakers and languages and return the token_embeddings and combined speaker and language embeddings</p> <p>Parameters:</p> Name Type Description Default <code>token_idx</code> <code>Tensor</code> <p>Tensor of token indices.</p> required <code>speaker_idx</code> <code>Tensor</code> <p>Tensor of speaker identities.</p> required <code>src_mask</code> <code>Tensor</code> <p>Mask tensor for source sequences.</p> required <code>lang_idx</code> <code>Tensor</code> <p>Tensor of language indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple[torch.Tensor, torch.Tensor]: Token embeddings tensor,</p> <code>Tensor</code> <p>and combined speaker and language embeddings tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def get_embeddings(\n    self,\n    token_idx: torch.Tensor,\n    speaker_idx: torch.Tensor,\n    src_mask: torch.Tensor,\n    lang_idx: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Given the tokens, speakers, source mask, and language indices, compute\n    the embeddings for tokens, speakers and languages and return the\n    token_embeddings and combined speaker and language embeddings\n\n    Args:\n        token_idx (torch.Tensor): Tensor of token indices.\n        speaker_idx (torch.Tensor): Tensor of speaker identities.\n        src_mask (torch.Tensor): Mask tensor for source sequences.\n        lang_idx (torch.Tensor): Tensor of language indices.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Token embeddings tensor,\n        and combined speaker and language embeddings tensor.\n    \"\"\"\n    token_embeddings = F.embedding(token_idx, self.src_word_emb)\n    # NOTE: here you can manage the speaker embeddings, can be used for the voice export ?\n    speaker_embeds = F.embedding(speaker_idx, self.speaker_embed)\n    lang_embeds = F.embedding(lang_idx, self.lang_embed)\n\n    # Merge the speaker and language embeddings\n    embeddings = torch.cat([speaker_embeds, lang_embeds], dim=2)\n\n    # Apply the mask to the embeddings and token embeddings\n    embeddings = embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n    token_embeddings = token_embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n\n    return token_embeddings, embeddings\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.prepare_for_export","title":"<code>prepare_for_export()</code>","text":"<p>Prepare the model for export.</p> <p>This method is called when the model is about to be exported, such as for deployment or serializing for later use. The method removes unnecessary components that are not needed during inference. Specifically, it removes the phoneme and utterance prosody encoders for this acoustic model. These components are typically used during training and are not needed when the model is used for making predictions.</p> <p>Returns     None</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def prepare_for_export(self) -&gt; None:\n    r\"\"\"Prepare the model for export.\n\n    This method is called when the model is about to be exported, such as for deployment\n    or serializing for later use. The method removes unnecessary components that are\n    not needed during inference. Specifically, it removes the phoneme and utterance\n    prosody encoders for this acoustic model. These components are typically used during\n    training and are not needed when the model is used for making predictions.\n\n    Returns\n        None\n    \"\"\"\n    del self.phoneme_prosody_encoder\n    del self.utterance_prosody_encoder\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/acoustic_model/#models.tts.delightful_tts.acoustic_model.acoustic_model.AcousticModel.unfreeze_params","title":"<code>unfreeze_params(freeze_text_embed, freeze_lang_embed)</code>","text":"<p>Unfreeze the trainable parameters in the model, allowing them to be updated during training.</p> <p>This method is typically used to 'unfreeze' previously 'frozen' parameters, making them trainable again. For this model, it unfreezes all parameters and then selectively freezes the text embeddings and language embeddings, if required.</p> <p>Parameters:</p> Name Type Description Default <code>freeze_text_embed</code> <code>bool</code> <p>Flag to indicate if text embeddings should remain frozen.</p> required <code>freeze_lang_embed</code> <code>bool</code> <p>Flag to indicate if language embeddings should remain frozen.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>models/tts/delightful_tts/acoustic_model/acoustic_model.py</code> <pre><code>def unfreeze_params(self, freeze_text_embed: bool, freeze_lang_embed: bool) -&gt; None:\n    r\"\"\"Unfreeze the trainable parameters in the model, allowing them to be updated during training.\n\n    This method is typically used to 'unfreeze' previously 'frozen' parameters, making them trainable again.\n    For this model, it unfreezes all parameters and then selectively freezes the\n    text embeddings and language embeddings, if required.\n\n    Args:\n        freeze_text_embed (bool): Flag to indicate if text embeddings should remain frozen.\n        freeze_lang_embed (bool): Flag to indicate if language embeddings should remain frozen.\n\n    Returns:\n        None\n    \"\"\"\n    # Iterate through all model parameters and make them trainable\n    for par in self.parameters():\n        par.requires_grad = True\n\n    # If freeze_text_embed flag is True, keep the source word embeddings frozen\n    if freeze_text_embed:\n        # @fixed self.src_word_emb.parameters has no parameters() method!\n        # for par in self.src_word_emb.parameters():\n        self.src_word_emb.requires_grad = False\n\n    # If freeze_lang_embed flag is True, keep the language embeddings frozen\n    if freeze_lang_embed:\n        self.lang_embed.requires_grad = False\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/aligner/","title":"Aligner","text":""},{"location":"models/tts/delightful_tts/acoustic_model/aligner/#models.tts.delightful_tts.acoustic_model.aligner.Aligner","title":"<code>Aligner</code>","text":"<p>             Bases: <code>Module</code></p> <p>DEPRECATED: Aligner class represents a PyTorch module responsible for alignment tasks in a sequence-to-sequence model. It uses convolutional layers combined with LeakyReLU activation functions to project inputs to a hidden representation. The class utilizes both softmax and log-softmax to calculate softmax along dimension 3.</p> <p>Parameters:</p> Name Type Description Default <code>d_enc_in</code> <code>int</code> <p>Number of channels in the input for the encoder.</p> required <code>d_dec_in</code> <code>int</code> <p>Number of channels in the input for the decoder.</p> required <code>d_hidden</code> <code>int</code> <p>Number of channels in the output (hidden layers).</p> required <code>kernel_size_enc</code> <code>int</code> <p>Size of the convolving kernel for encoder, default is 3.</p> <code>3</code> <code>kernel_size_dec</code> <code>int</code> <p>Size of the convolving kernel for decoder, default is 7.</p> <code>7</code> <code>temperature</code> <code>float</code> <p>The temperature value applied in Gaussian isotropic attention mechanism, default is 0.0005.</p> <code>0.0005</code> <code>leaky_relu_slope</code> <code>float</code> <p>Controls the angle of the negative slope of LeakyReLU activation, default is LEAKY_RELU_SLOPE.</p> <code>LEAKY_RELU_SLOPE</code> Source code in <code>models/tts/delightful_tts/acoustic_model/aligner.py</code> <pre><code>class Aligner(Module):\n    r\"\"\"DEPRECATED: Aligner class represents a PyTorch module responsible for alignment tasks\n    in a sequence-to-sequence model. It uses convolutional layers combined with\n    LeakyReLU activation functions to project inputs to a hidden representation.\n    The class utilizes both softmax and log-softmax to calculate softmax\n    along dimension 3.\n\n    Args:\n        d_enc_in (int): Number of channels in the input for the encoder.\n        d_dec_in (int): Number of channels in the input for the decoder.\n        d_hidden (int): Number of channels in the output (hidden layers).\n        kernel_size_enc (int, optional): Size of the convolving kernel for encoder, default is 3.\n        kernel_size_dec (int, optional): Size of the convolving kernel for decoder, default is 7.\n        temperature (float, optional): The temperature value applied in Gaussian isotropic\n            attention mechanism, default is 0.0005.\n        leaky_relu_slope (float, optional): Controls the angle of the negative slope of\n            LeakyReLU activation, default is LEAKY_RELU_SLOPE.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        d_enc_in: int,\n        d_dec_in: int,\n        d_hidden: int,\n        kernel_size_enc: int = 3,\n        kernel_size_dec: int = 7,\n        temperature: float = 0.0005,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n        self.temperature = temperature\n\n        self.softmax = torch.nn.Softmax(dim=3)\n        self.log_softmax = torch.nn.LogSoftmax(dim=3)\n\n        self.key_proj = nn.Sequential(\n            nn.Conv1d(\n                d_enc_in,\n                d_hidden,\n                kernel_size=kernel_size_enc,\n                padding=kernel_size_enc // 2,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n            nn.Conv1d(\n                d_hidden,\n                d_hidden,\n                kernel_size=kernel_size_enc,\n                padding=kernel_size_enc // 2,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n        )\n\n        self.query_proj = nn.Sequential(\n            nn.Conv1d(\n                d_dec_in,\n                d_hidden,\n                kernel_size=kernel_size_dec,\n                padding=kernel_size_dec // 2,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n            nn.Conv1d(\n                d_hidden,\n                d_hidden,\n                kernel_size=kernel_size_dec,\n                padding=kernel_size_dec // 2,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n            nn.Conv1d(\n                d_hidden,\n                d_hidden,\n                kernel_size=kernel_size_dec,\n                padding=kernel_size_dec // 2,\n            ),\n            nn.LeakyReLU(leaky_relu_slope),\n        )\n\n    def binarize_attention_parallel(\n        self,\n        attn: torch.Tensor,\n        in_lens: torch.Tensor,\n        out_lens: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"For training purposes only! Binarizes attention with MAS.\n        Binarizes the attention tensor using Maximum Attention Strategy (MAS).\n\n        This process is applied for training purposes only and the resulting\n        binarized attention tensor will no longer receive a gradient in the\n        backpropagation process.\n\n        Args:\n            attn (Tensor): The attention tensor. Must be of shape (B, 1, max_mel_len, max_text_len),\n                where B represents the batch size, max_mel_len represents the maximum length\n                of the mel spectrogram, and max_text_len represents the maximum length of the text.\n            in_lens (Tensor): A 1D tensor of shape (B,) that contains the input sequence lengths,\n                which likely corresponds to text sequence lengths.\n            out_lens (Tensor): A 1D tensor of shape (B,) that contains the output sequence lengths,\n                which likely corresponds to mel spectrogram lengths.\n\n        Returns:\n            Tensor: The binarized attention tensor. The output tensor has the same shape as the input `attn` tensor.\n        \"\"\"\n        with torch.no_grad():\n            attn_cpu = attn.data.cpu().numpy()\n            attn_out = b_mas(\n                attn_cpu,\n                in_lens.cpu().numpy(),\n                out_lens.cpu().numpy(),\n                width=1,\n            )\n        return torch.from_numpy(attn_out)\n\n    def forward(\n        self,\n        enc_in: torch.Tensor,\n        dec_in: torch.Tensor,\n        enc_len: torch.Tensor,\n        dec_len: torch.Tensor,\n        enc_mask: torch.Tensor,\n        attn_prior: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Performs the forward pass through the Aligner module.\n\n        Args:\n            enc_in (Tensor): The text encoder outputs.\n                Must be of shape (B, C_1, T_1), where B is the batch size, C_1 the number of\n                channels in encoder inputs,\n                and T_1 the sequence length of encoder inputs.\n            dec_in (Tensor): The data to align with encoder outputs.\n                Must be of shape (B, C_2, T_2), where C_2 is the number of channels in decoder inputs,\n                and T_2 the sequence length of decoder inputs.\n            enc_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `enc_in`.\n            dec_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `dec_in`.\n            enc_mask (Tensor): Binary mask tensor used to avoid attention to certain timesteps.\n            attn_prior (Tensor): Previous attention values for attention calculation.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor, Tensor]: Returns a tuple of Tensors representing the log-probability, soft attention, hard attention, and hard attention duration.\n        \"\"\"\n        queries = dec_in.float()\n        keys = enc_in.float()\n        keys_enc = self.key_proj(keys)  # B x n_attn_dims x T2\n        queries_enc = self.query_proj(queries)\n\n        # Simplistic Gaussian Isotopic Attention\n        attn = (\n            queries_enc[:, :, :, None] - keys_enc[:, :, None]\n        ) ** 2  # B x n_attn_dims x T1 x T2\n        attn = -self.temperature * attn.sum(1, keepdim=True)\n\n        if attn_prior is not None:\n            # print(f\"AlignmentEncoder \\t| mel: {queries.shape} phone: {keys.shape}\n            # mask: {mask.shape} attn: {attn.shape} attn_prior: {attn_prior.shape}\")\n            attn = self.log_softmax(attn) + torch.log(\n                attn_prior.permute((0, 2, 1))[:, None] + 1e-8,\n            )\n            # print(f\"AlignmentEncoder \\t| After prior sum attn: {attn.shape}\")\"\"\"\n\n        attn_logprob = attn.clone()\n\n        if enc_mask is not None:\n            attn.masked_fill(enc_mask.unsqueeze(1).unsqueeze(1), -float(\"inf\"))\n\n        attn_soft = self.softmax(attn)  # softmax along T2\n        attn_hard = self.binarize_attention_parallel(attn_soft, enc_len, dec_len)\n        attn_hard_dur = attn_hard.sum(2)[:, 0, :]\n        return attn_logprob, attn_soft, attn_hard, attn_hard_dur\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/aligner/#models.tts.delightful_tts.acoustic_model.aligner.Aligner.binarize_attention_parallel","title":"<code>binarize_attention_parallel(attn, in_lens, out_lens)</code>","text":"<p>For training purposes only! Binarizes attention with MAS. Binarizes the attention tensor using Maximum Attention Strategy (MAS).</p> <p>This process is applied for training purposes only and the resulting binarized attention tensor will no longer receive a gradient in the backpropagation process.</p> <p>Parameters:</p> Name Type Description Default <code>attn</code> <code>Tensor</code> <p>The attention tensor. Must be of shape (B, 1, max_mel_len, max_text_len), where B represents the batch size, max_mel_len represents the maximum length of the mel spectrogram, and max_text_len represents the maximum length of the text.</p> required <code>in_lens</code> <code>Tensor</code> <p>A 1D tensor of shape (B,) that contains the input sequence lengths, which likely corresponds to text sequence lengths.</p> required <code>out_lens</code> <code>Tensor</code> <p>A 1D tensor of shape (B,) that contains the output sequence lengths, which likely corresponds to mel spectrogram lengths.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The binarized attention tensor. The output tensor has the same shape as the input <code>attn</code> tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/aligner.py</code> <pre><code>def binarize_attention_parallel(\n    self,\n    attn: torch.Tensor,\n    in_lens: torch.Tensor,\n    out_lens: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"For training purposes only! Binarizes attention with MAS.\n    Binarizes the attention tensor using Maximum Attention Strategy (MAS).\n\n    This process is applied for training purposes only and the resulting\n    binarized attention tensor will no longer receive a gradient in the\n    backpropagation process.\n\n    Args:\n        attn (Tensor): The attention tensor. Must be of shape (B, 1, max_mel_len, max_text_len),\n            where B represents the batch size, max_mel_len represents the maximum length\n            of the mel spectrogram, and max_text_len represents the maximum length of the text.\n        in_lens (Tensor): A 1D tensor of shape (B,) that contains the input sequence lengths,\n            which likely corresponds to text sequence lengths.\n        out_lens (Tensor): A 1D tensor of shape (B,) that contains the output sequence lengths,\n            which likely corresponds to mel spectrogram lengths.\n\n    Returns:\n        Tensor: The binarized attention tensor. The output tensor has the same shape as the input `attn` tensor.\n    \"\"\"\n    with torch.no_grad():\n        attn_cpu = attn.data.cpu().numpy()\n        attn_out = b_mas(\n            attn_cpu,\n            in_lens.cpu().numpy(),\n            out_lens.cpu().numpy(),\n            width=1,\n        )\n    return torch.from_numpy(attn_out)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/aligner/#models.tts.delightful_tts.acoustic_model.aligner.Aligner.forward","title":"<code>forward(enc_in, dec_in, enc_len, dec_len, enc_mask, attn_prior)</code>","text":"<p>Performs the forward pass through the Aligner module.</p> <p>Parameters:</p> Name Type Description Default <code>enc_in</code> <code>Tensor</code> <p>The text encoder outputs. Must be of shape (B, C_1, T_1), where B is the batch size, C_1 the number of channels in encoder inputs, and T_1 the sequence length of encoder inputs.</p> required <code>dec_in</code> <code>Tensor</code> <p>The data to align with encoder outputs. Must be of shape (B, C_2, T_2), where C_2 is the number of channels in decoder inputs, and T_2 the sequence length of decoder inputs.</p> required <code>enc_len</code> <code>Tensor</code> <p>1D tensor representing the lengths of each sequence in the batch in <code>enc_in</code>.</p> required <code>dec_len</code> <code>Tensor</code> <p>1D tensor representing the lengths of each sequence in the batch in <code>dec_in</code>.</p> required <code>enc_mask</code> <code>Tensor</code> <p>Binary mask tensor used to avoid attention to certain timesteps.</p> required <code>attn_prior</code> <code>Tensor</code> <p>Previous attention values for attention calculation.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor, Tensor, Tensor]: Returns a tuple of Tensors representing the log-probability, soft attention, hard attention, and hard attention duration.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/aligner.py</code> <pre><code>def forward(\n    self,\n    enc_in: torch.Tensor,\n    dec_in: torch.Tensor,\n    enc_len: torch.Tensor,\n    dec_len: torch.Tensor,\n    enc_mask: torch.Tensor,\n    attn_prior: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Performs the forward pass through the Aligner module.\n\n    Args:\n        enc_in (Tensor): The text encoder outputs.\n            Must be of shape (B, C_1, T_1), where B is the batch size, C_1 the number of\n            channels in encoder inputs,\n            and T_1 the sequence length of encoder inputs.\n        dec_in (Tensor): The data to align with encoder outputs.\n            Must be of shape (B, C_2, T_2), where C_2 is the number of channels in decoder inputs,\n            and T_2 the sequence length of decoder inputs.\n        enc_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `enc_in`.\n        dec_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `dec_in`.\n        enc_mask (Tensor): Binary mask tensor used to avoid attention to certain timesteps.\n        attn_prior (Tensor): Previous attention values for attention calculation.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor, Tensor]: Returns a tuple of Tensors representing the log-probability, soft attention, hard attention, and hard attention duration.\n    \"\"\"\n    queries = dec_in.float()\n    keys = enc_in.float()\n    keys_enc = self.key_proj(keys)  # B x n_attn_dims x T2\n    queries_enc = self.query_proj(queries)\n\n    # Simplistic Gaussian Isotopic Attention\n    attn = (\n        queries_enc[:, :, :, None] - keys_enc[:, :, None]\n    ) ** 2  # B x n_attn_dims x T1 x T2\n    attn = -self.temperature * attn.sum(1, keepdim=True)\n\n    if attn_prior is not None:\n        # print(f\"AlignmentEncoder \\t| mel: {queries.shape} phone: {keys.shape}\n        # mask: {mask.shape} attn: {attn.shape} attn_prior: {attn_prior.shape}\")\n        attn = self.log_softmax(attn) + torch.log(\n            attn_prior.permute((0, 2, 1))[:, None] + 1e-8,\n        )\n        # print(f\"AlignmentEncoder \\t| After prior sum attn: {attn.shape}\")\"\"\"\n\n    attn_logprob = attn.clone()\n\n    if enc_mask is not None:\n        attn.masked_fill(enc_mask.unsqueeze(1).unsqueeze(1), -float(\"inf\"))\n\n    attn_soft = self.softmax(attn)  # softmax along T2\n    attn_hard = self.binarize_attention_parallel(attn_soft, enc_len, dec_len)\n    attn_hard_dur = attn_hard.sum(2)[:, 0, :]\n    return attn_logprob, attn_soft, attn_hard, attn_hard_dur\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/embedding/","title":"Embedding","text":""},{"location":"models/tts/delightful_tts/acoustic_model/embedding/#models.tts.delightful_tts.acoustic_model.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Class represents a simple embedding layer but without any learning of the embeddings. The embeddings are initialized with random values and kept static throughout training (They are parameters, not model's state).</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the dictionary of embeddings, typically size of the vocabulary.</p> required <code>embedding_dim</code> <code>int</code> <p>The size of each embedding vector.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: An output tensor resulting from the lookup operation.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/embedding.py</code> <pre><code>class Embedding(Module):\n    r\"\"\"Class represents a simple embedding layer but without any learning of the embeddings.\n    The embeddings are initialized with random values and kept static throughout training (They are parameters, not model's state).\n\n    Args:\n        num_embeddings (int): Size of the dictionary of embeddings, typically size of the vocabulary.\n        embedding_dim (int): The size of each embedding vector.\n\n    Returns:\n        torch.Tensor: An output tensor resulting from the lookup operation.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n    ):\n        super().__init__()\n        self.embeddings = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n\n    def forward(self, idx: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward propagation for the Embedding implementation.\n\n        Args:\n            idx (torch.Tensor): A tensor containing the indices of the embeddings to be accessed.\n\n        Returns:\n            torch.Tensor: An output tensor resulting from the lookup operation.\n        \"\"\"\n        return F.embedding(idx, self.embeddings)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/embedding/#models.tts.delightful_tts.acoustic_model.embedding.Embedding.forward","title":"<code>forward(idx)</code>","text":"<p>Forward propagation for the Embedding implementation.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>Tensor</code> <p>A tensor containing the indices of the embeddings to be accessed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: An output tensor resulting from the lookup operation.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/embedding.py</code> <pre><code>def forward(self, idx: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward propagation for the Embedding implementation.\n\n    Args:\n        idx (torch.Tensor): A tensor containing the indices of the embeddings to be accessed.\n\n    Returns:\n        torch.Tensor: An output tensor resulting from the lookup operation.\n    \"\"\"\n    return F.embedding(idx, self.embeddings)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/","title":"Energy Adaptor","text":""},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/#models.tts.delightful_tts.acoustic_model.energy_adaptor.EnergyAdaptor","title":"<code>EnergyAdaptor</code>","text":"<p>             Bases: <code>Module</code></p> <p>Variance Adaptor with an added 1D conv layer. Used to get energy embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>channels_in</code> <code>int</code> <p>Number of in channels for conv layers.</p> required <code>channels_out</code> <code>int</code> <p>Number of out channels.</p> required <code>kernel_size</code> <code>int</code> <p>Size the kernel for the conv layers.</p> required <code>dropout</code> <code>float</code> <p>Probability of dropout.</p> required <code>leaky_relu_slope</code> <code>float</code> <p>Slope for the leaky relu.</p> required <code>emb_kernel_size</code> <code>int</code> <p>Size the kernel for the pitch embedding.</p> required inputs, mask <ul> <li>inputs (batch, time1, dim): Tensor containing input vector</li> <li>target (batch, 1, time2): Tensor containing the energy target</li> <li>dr (batch, time1): Tensor containing aligner durations vector</li> <li>mask (batch, time1): Tensor containing indices to be masked</li> </ul> <p>Returns:     - energy prediction (batch, 1, time1): Tensor produced by energy predictor     - energy embedding (batch, channels, time1): Tensor produced energy adaptor     - average energy target(train only) (batch, 1, time1): Tensor produced after averaging over durations</p> Source code in <code>models/tts/delightful_tts/acoustic_model/energy_adaptor.py</code> <pre><code>class EnergyAdaptor(nn.Module):\n    \"\"\"Variance Adaptor with an added 1D conv layer. Used to\n    get energy embeddings.\n\n    Args:\n        channels_in (int): Number of in channels for conv layers.\n        channels_out (int): Number of out channels.\n        kernel_size (int): Size the kernel for the conv layers.\n        dropout (float): Probability of dropout.\n        leaky_relu_slope (float): Slope for the leaky relu.\n        emb_kernel_size (int): Size the kernel for the pitch embedding.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time1, dim): Tensor containing input vector\n        - **target** (batch, 1, time2): Tensor containing the energy target\n        - **dr** (batch, time1): Tensor containing aligner durations vector\n        - **mask** (batch, time1): Tensor containing indices to be masked\n    Returns:\n        - **energy prediction** (batch, 1, time1): Tensor produced by energy predictor\n        - **energy embedding** (batch, channels, time1): Tensor produced energy adaptor\n        - **average energy target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels_hidden: int,\n        channels_out: int,\n        kernel_size: int,\n        dropout: float,\n        leaky_relu_slope: float,\n        emb_kernel_size: int,\n    ):\n        super().__init__()\n        self.energy_predictor = VariancePredictor(\n            channels_in=channels_in,\n            channels=channels_hidden,\n            channels_out=channels_out,\n            kernel_size=kernel_size,\n            p_dropout=dropout,\n            leaky_relu_slope=leaky_relu_slope,\n        )\n        self.energy_emb = nn.Conv1d(\n            1,\n            channels_hidden,\n            kernel_size=emb_kernel_size,\n            padding=int((emb_kernel_size - 1) / 2),\n        )\n\n    def get_energy_embedding_train(\n        self,\n        x: torch.Tensor,\n        target: torch.Tensor,\n        dr: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Function is used during training to get the energy prediction, average energy target, and energy embedding.\n\n        Args:\n            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                            T_src is the source sequence length, and C is the number of channels.\n            target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n                                T_max2 is the maximum target sequence length.\n            dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the durations.\n            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the mask.\n\n        Returns:\n            energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the energy prediction.\n            avg_energy_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                            T_src is the source sequence length. The values represent the average energy target.\n            energy_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                    C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n        Shapes:\n            x: :math: `[B, T_src, C]`\n            target: :math: `[B, 1, T_max2]`\n            dr: :math: `[B, T_src]`\n            mask: :math: `[B, T_src]`\n        \"\"\"\n        energy_pred = self.energy_predictor.forward(x, mask)\n        energy_pred = energy_pred.unsqueeze(1)\n\n        avg_energy_target = average_over_durations(target, dr)\n        energy_emb = self.energy_emb(avg_energy_target)\n\n        return energy_pred, avg_energy_target, energy_emb\n\n    def add_energy_embedding_train(\n        self,\n        x: torch.Tensor,\n        target: torch.Tensor,\n        dr: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Add energy embedding during training.\n\n        This method calculates the energy embedding and adds it to the input tensor 'x'.\n        It also returns the predicted energy and the average target energy.\n\n        Args:\n            x (torch.Tensor): The input tensor to which the energy embedding will be added.\n            target (torch.Tensor): The target tensor used in the energy embedding calculation.\n            dr (torch.Tensor): The duration tensor used in the energy embedding calculation.\n            mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n\n        Returns:\n            x (torch.Tensor): The input tensor with added energy embedding.\n            energy_pred (torch.Tensor): The predicted energy tensor.\n            avg_energy_target (torch.Tensor): The average target energy tensor.\n        \"\"\"\n        energy_pred, avg_energy_target, energy_emb = self.get_energy_embedding_train(\n            x=x,\n            target=target,\n            dr=dr,\n            mask=mask,\n        )\n        x_energy = x + energy_emb.transpose(1, 2)\n        return x_energy, energy_pred, avg_energy_target\n\n    def get_energy_embedding(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Function is used during inference to get the energy embedding and energy prediction.\n\n        Args:\n            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                            T_src is the source sequence length, and C is the number of channels.\n            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the mask.\n\n        Returns:\n            energy_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                            C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n            energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the energy prediction.\n        \"\"\"\n        energy_pred = self.energy_predictor.forward(x, mask)\n        energy_pred = energy_pred.unsqueeze(1)\n\n        energy_emb_pred = self.energy_emb(energy_pred)\n        return energy_emb_pred, energy_pred\n\n    def add_energy_embedding(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Add energy embedding during inference.\n\n        This method calculates the energy embedding and adds it to the input tensor 'x'.\n        It also returns the predicted energy.\n\n        Args:\n            x (torch.Tensor): The input tensor to which the energy embedding will be added.\n            mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n            energy_transform (Callable): A function to transform the energy prediction.\n\n        Returns:\n            x (torch.Tensor): The input tensor with added energy embedding.\n            energy_pred (torch.Tensor): The predicted energy tensor.\n        \"\"\"\n        energy_emb_pred, energy_pred = self.get_energy_embedding(x, mask)\n        x_energy = x + energy_emb_pred.transpose(1, 2)\n        return x_energy, energy_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/#models.tts.delightful_tts.acoustic_model.energy_adaptor.EnergyAdaptor.add_energy_embedding","title":"<code>add_energy_embedding(x, mask)</code>","text":"<p>Add energy embedding during inference.</p> <p>This method calculates the energy embedding and adds it to the input tensor 'x'. It also returns the predicted energy.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to which the energy embedding will be added.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor used in the energy embedding calculation.</p> required <code>energy_transform</code> <code>Callable</code> <p>A function to transform the energy prediction.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>The input tensor with added energy embedding.</p> <code>energy_pred</code> <code>Tensor</code> <p>The predicted energy tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/energy_adaptor.py</code> <pre><code>def add_energy_embedding(\n    self,\n    x: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Add energy embedding during inference.\n\n    This method calculates the energy embedding and adds it to the input tensor 'x'.\n    It also returns the predicted energy.\n\n    Args:\n        x (torch.Tensor): The input tensor to which the energy embedding will be added.\n        mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n        energy_transform (Callable): A function to transform the energy prediction.\n\n    Returns:\n        x (torch.Tensor): The input tensor with added energy embedding.\n        energy_pred (torch.Tensor): The predicted energy tensor.\n    \"\"\"\n    energy_emb_pred, energy_pred = self.get_energy_embedding(x, mask)\n    x_energy = x + energy_emb_pred.transpose(1, 2)\n    return x_energy, energy_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/#models.tts.delightful_tts.acoustic_model.energy_adaptor.EnergyAdaptor.add_energy_embedding_train","title":"<code>add_energy_embedding_train(x, target, dr, mask)</code>","text":"<p>Add energy embedding during training.</p> <p>This method calculates the energy embedding and adds it to the input tensor 'x'. It also returns the predicted energy and the average target energy.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to which the energy embedding will be added.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor used in the energy embedding calculation.</p> required <code>dr</code> <code>Tensor</code> <p>The duration tensor used in the energy embedding calculation.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor used in the energy embedding calculation.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>The input tensor with added energy embedding.</p> <code>energy_pred</code> <code>Tensor</code> <p>The predicted energy tensor.</p> <code>avg_energy_target</code> <code>Tensor</code> <p>The average target energy tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/energy_adaptor.py</code> <pre><code>def add_energy_embedding_train(\n    self,\n    x: torch.Tensor,\n    target: torch.Tensor,\n    dr: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Add energy embedding during training.\n\n    This method calculates the energy embedding and adds it to the input tensor 'x'.\n    It also returns the predicted energy and the average target energy.\n\n    Args:\n        x (torch.Tensor): The input tensor to which the energy embedding will be added.\n        target (torch.Tensor): The target tensor used in the energy embedding calculation.\n        dr (torch.Tensor): The duration tensor used in the energy embedding calculation.\n        mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n\n    Returns:\n        x (torch.Tensor): The input tensor with added energy embedding.\n        energy_pred (torch.Tensor): The predicted energy tensor.\n        avg_energy_target (torch.Tensor): The average target energy tensor.\n    \"\"\"\n    energy_pred, avg_energy_target, energy_emb = self.get_energy_embedding_train(\n        x=x,\n        target=target,\n        dr=dr,\n        mask=mask,\n    )\n    x_energy = x + energy_emb.transpose(1, 2)\n    return x_energy, energy_pred, avg_energy_target\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/#models.tts.delightful_tts.acoustic_model.energy_adaptor.EnergyAdaptor.get_energy_embedding","title":"<code>get_energy_embedding(x, mask)</code>","text":"<p>Function is used during inference to get the energy embedding and energy prediction.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3D tensor of shape [B, T_src, C] where B is the batch size,             T_src is the source sequence length, and C is the number of channels.</p> required <code>mask</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the mask.</p> required <p>Returns:</p> Name Type Description <code>energy_emb_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, C, T_src] where B is the batch size,                             C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.</p> <code>energy_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                         T_src is the source sequence length. The values represent the energy prediction.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/energy_adaptor.py</code> <pre><code>def get_energy_embedding(\n    self,\n    x: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Function is used during inference to get the energy embedding and energy prediction.\n\n    Args:\n        x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                        T_src is the source sequence length, and C is the number of channels.\n        mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the mask.\n\n    Returns:\n        energy_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                        C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n        energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                    T_src is the source sequence length. The values represent the energy prediction.\n    \"\"\"\n    energy_pred = self.energy_predictor.forward(x, mask)\n    energy_pred = energy_pred.unsqueeze(1)\n\n    energy_emb_pred = self.energy_emb(energy_pred)\n    return energy_emb_pred, energy_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/energy_adaptor/#models.tts.delightful_tts.acoustic_model.energy_adaptor.EnergyAdaptor.get_energy_embedding_train","title":"<code>get_energy_embedding_train(x, target, dr, mask)</code>","text":"<p>Function is used during training to get the energy prediction, average energy target, and energy embedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3D tensor of shape [B, T_src, C] where B is the batch size,             T_src is the source sequence length, and C is the number of channels.</p> required <code>target</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_max2] where B is the batch size,                 T_max2 is the maximum target sequence length.</p> required <code>dr</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the durations.</p> required <code>mask</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the mask.</p> required <p>Returns:</p> Name Type Description <code>energy_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                         T_src is the source sequence length. The values represent the energy prediction.</p> <code>avg_energy_target</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                             T_src is the source sequence length. The values represent the average energy target.</p> <code>energy_emb</code> <code>Tensor</code> <p>A 3D tensor of shape [B, C, T_src] where B is the batch size,                     C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.</p> <p>Shapes:     x: :math: <code>[B, T_src, C]</code>     target: :math: <code>[B, 1, T_max2]</code>     dr: :math: <code>[B, T_src]</code>     mask: :math: <code>[B, T_src]</code></p> Source code in <code>models/tts/delightful_tts/acoustic_model/energy_adaptor.py</code> <pre><code>def get_energy_embedding_train(\n    self,\n    x: torch.Tensor,\n    target: torch.Tensor,\n    dr: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Function is used during training to get the energy prediction, average energy target, and energy embedding.\n\n    Args:\n        x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                        T_src is the source sequence length, and C is the number of channels.\n        target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n                            T_max2 is the maximum target sequence length.\n        dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the durations.\n        mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the mask.\n\n    Returns:\n        energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                    T_src is the source sequence length. The values represent the energy prediction.\n        avg_energy_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the average energy target.\n        energy_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n    Shapes:\n        x: :math: `[B, T_src, C]`\n        target: :math: `[B, 1, T_max2]`\n        dr: :math: `[B, T_src]`\n        mask: :math: `[B, T_src]`\n    \"\"\"\n    energy_pred = self.energy_predictor.forward(x, mask)\n    energy_pred = energy_pred.unsqueeze(1)\n\n    avg_energy_target = average_over_durations(target, dr)\n    energy_emb = self.energy_emb(avg_energy_target)\n\n    return energy_pred, avg_energy_target, energy_emb\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/","title":"Length Adaptor","text":""},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/#models.tts.delightful_tts.acoustic_model.length_adaptor.LengthAdaptor","title":"<code>LengthAdaptor</code>","text":"<p>             Bases: <code>Module</code></p> <p>DEPRECATED: The LengthAdaptor module is used to adjust the duration of phonemes. It contains a dedicated duration predictor and methods to upsample the input features to match predicted durations.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>AcousticModelConfigType</code> <p>The model configuration object containing model parameters.</p> required Source code in <code>models/tts/delightful_tts/acoustic_model/length_adaptor.py</code> <pre><code>class LengthAdaptor(Module):\n    r\"\"\"DEPRECATED: The LengthAdaptor module is used to adjust the duration of phonemes.\n    It contains a dedicated duration predictor and methods to upsample the input features to match predicted durations.\n\n    Args:\n        model_config (AcousticModelConfigType): The model configuration object containing model parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: AcousticModelConfigType,\n    ):\n        super().__init__()\n        # Initialize the duration predictor\n        self.duration_predictor = VariancePredictor(\n            channels_in=model_config.encoder.n_hidden,\n            channels=model_config.variance_adaptor.n_hidden,\n            channels_out=1,\n            kernel_size=model_config.variance_adaptor.kernel_size,\n            p_dropout=model_config.variance_adaptor.p_dropout,\n        )\n\n    def length_regulate(\n        self,\n        x: torch.Tensor,\n        duration: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Regulates the length of the input tensor using the duration tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n            duration (torch.Tensor): The tensor containing duration for each time step in x.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: The regulated output tensor and the tensor containing the length of each sequence in the batch.\n        \"\"\"\n        output = torch.jit.annotate(List[torch.Tensor], [])\n        mel_len = torch.jit.annotate(List[int], [])\n        max_len = 0\n        for batch, expand_target in zip(x, duration):\n            expanded = self.expand(batch, expand_target)\n            if expanded.shape[0] &gt; max_len:\n                max_len = expanded.shape[0]\n            output.append(expanded)\n            mel_len.append(expanded.shape[0])\n        output = tools.pad(output, max_len)\n        return output, torch.tensor(mel_len, dtype=torch.int64)\n\n    def expand(self, batch: torch.Tensor, predicted: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Expands the input tensor based on the predicted values.\n\n        Args:\n            batch (torch.Tensor): The input tensor.\n            predicted (torch.Tensor): The tensor containing predicted expansion factors.\n\n        Returns:\n            torch.Tensor: The expanded tensor.\n        \"\"\"\n        out = torch.jit.annotate(List[torch.Tensor], [])\n        for i, vec in enumerate(batch):\n            expand_size = predicted[i].item()\n            out.append(vec.expand(max(int(expand_size), 0), -1))\n        return torch.cat(out, 0)\n\n    def upsample_train(\n        self,\n        x: torch.Tensor,\n        x_res: torch.Tensor,\n        duration_target: torch.Tensor,\n        embeddings: torch.Tensor,\n        src_mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Upsamples the input tensor during training using ground truth durations.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n            x_res (torch.Tensor): Another input tensor for duration prediction.\n            duration_target (torch.Tensor): The ground truth durations tensor.\n            embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n            src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, log duration prediction, and upsampled embeddings.\n        \"\"\"\n        x_res = x_res.detach()\n        log_duration_prediction = self.duration_predictor(\n            x_res,\n            src_mask,\n        )  # type: torch.Tensor\n        x, _ = self.length_regulate(x, duration_target)\n        embeddings, _ = self.length_regulate(embeddings, duration_target)\n        return x, log_duration_prediction, embeddings\n\n    def upsample(\n        self,\n        x: torch.Tensor,\n        x_res: torch.Tensor,\n        src_mask: torch.Tensor,\n        embeddings: torch.Tensor,\n        control: float,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Upsamples the input tensor during inference.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n            x_res (torch.Tensor): Another input tensor for duration prediction.\n            src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n            embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n            control (float): A control parameter for pitch regulation.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, approximated duration, and upsampled embeddings.\n        \"\"\"\n        log_duration_prediction = self.duration_predictor(\n            x_res,\n            src_mask,\n        )\n        duration_rounded = torch.clamp(\n            (torch.round(torch.exp(log_duration_prediction) - 1) * control),\n            min=0,\n        )\n        x, _ = self.length_regulate(x, duration_rounded)\n        embeddings, _ = self.length_regulate(embeddings, duration_rounded)\n        return x, duration_rounded, embeddings\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/#models.tts.delightful_tts.acoustic_model.length_adaptor.LengthAdaptor.expand","title":"<code>expand(batch, predicted)</code>","text":"<p>Expands the input tensor based on the predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>The input tensor.</p> required <code>predicted</code> <code>Tensor</code> <p>The tensor containing predicted expansion factors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The expanded tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/length_adaptor.py</code> <pre><code>def expand(self, batch: torch.Tensor, predicted: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Expands the input tensor based on the predicted values.\n\n    Args:\n        batch (torch.Tensor): The input tensor.\n        predicted (torch.Tensor): The tensor containing predicted expansion factors.\n\n    Returns:\n        torch.Tensor: The expanded tensor.\n    \"\"\"\n    out = torch.jit.annotate(List[torch.Tensor], [])\n    for i, vec in enumerate(batch):\n        expand_size = predicted[i].item()\n        out.append(vec.expand(max(int(expand_size), 0), -1))\n    return torch.cat(out, 0)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/#models.tts.delightful_tts.acoustic_model.length_adaptor.LengthAdaptor.length_regulate","title":"<code>length_regulate(x, duration)</code>","text":"<p>Regulates the length of the input tensor using the duration tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>duration</code> <code>Tensor</code> <p>The tensor containing duration for each time step in x.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: The regulated output tensor and the tensor containing the length of each sequence in the batch.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/length_adaptor.py</code> <pre><code>def length_regulate(\n    self,\n    x: torch.Tensor,\n    duration: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Regulates the length of the input tensor using the duration tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        duration (torch.Tensor): The tensor containing duration for each time step in x.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: The regulated output tensor and the tensor containing the length of each sequence in the batch.\n    \"\"\"\n    output = torch.jit.annotate(List[torch.Tensor], [])\n    mel_len = torch.jit.annotate(List[int], [])\n    max_len = 0\n    for batch, expand_target in zip(x, duration):\n        expanded = self.expand(batch, expand_target)\n        if expanded.shape[0] &gt; max_len:\n            max_len = expanded.shape[0]\n        output.append(expanded)\n        mel_len.append(expanded.shape[0])\n    output = tools.pad(output, max_len)\n    return output, torch.tensor(mel_len, dtype=torch.int64)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/#models.tts.delightful_tts.acoustic_model.length_adaptor.LengthAdaptor.upsample","title":"<code>upsample(x, x_res, src_mask, embeddings, control)</code>","text":"<p>Upsamples the input tensor during inference.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>x_res</code> <code>Tensor</code> <p>Another input tensor for duration prediction.</p> required <code>src_mask</code> <code>Tensor</code> <p>The mask tensor indicating valid entries in x and x_res.</p> required <code>embeddings</code> <code>Tensor</code> <p>The tensor containing phoneme embeddings.</p> required <code>control</code> <code>float</code> <p>A control parameter for pitch regulation.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, approximated duration, and upsampled embeddings.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/length_adaptor.py</code> <pre><code>def upsample(\n    self,\n    x: torch.Tensor,\n    x_res: torch.Tensor,\n    src_mask: torch.Tensor,\n    embeddings: torch.Tensor,\n    control: float,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Upsamples the input tensor during inference.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        x_res (torch.Tensor): Another input tensor for duration prediction.\n        src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n        embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n        control (float): A control parameter for pitch regulation.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, approximated duration, and upsampled embeddings.\n    \"\"\"\n    log_duration_prediction = self.duration_predictor(\n        x_res,\n        src_mask,\n    )\n    duration_rounded = torch.clamp(\n        (torch.round(torch.exp(log_duration_prediction) - 1) * control),\n        min=0,\n    )\n    x, _ = self.length_regulate(x, duration_rounded)\n    embeddings, _ = self.length_regulate(embeddings, duration_rounded)\n    return x, duration_rounded, embeddings\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/length_adaptor/#models.tts.delightful_tts.acoustic_model.length_adaptor.LengthAdaptor.upsample_train","title":"<code>upsample_train(x, x_res, duration_target, embeddings, src_mask)</code>","text":"<p>Upsamples the input tensor during training using ground truth durations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>x_res</code> <code>Tensor</code> <p>Another input tensor for duration prediction.</p> required <code>duration_target</code> <code>Tensor</code> <p>The ground truth durations tensor.</p> required <code>embeddings</code> <code>Tensor</code> <p>The tensor containing phoneme embeddings.</p> required <code>src_mask</code> <code>Tensor</code> <p>The mask tensor indicating valid entries in x and x_res.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, log duration prediction, and upsampled embeddings.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/length_adaptor.py</code> <pre><code>def upsample_train(\n    self,\n    x: torch.Tensor,\n    x_res: torch.Tensor,\n    duration_target: torch.Tensor,\n    embeddings: torch.Tensor,\n    src_mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Upsamples the input tensor during training using ground truth durations.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        x_res (torch.Tensor): Another input tensor for duration prediction.\n        duration_target (torch.Tensor): The ground truth durations tensor.\n        embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n        src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, log duration prediction, and upsampled embeddings.\n    \"\"\"\n    x_res = x_res.detach()\n    log_duration_prediction = self.duration_predictor(\n        x_res,\n        src_mask,\n    )  # type: torch.Tensor\n    x, _ = self.length_regulate(x, duration_target)\n    embeddings, _ = self.length_regulate(embeddings, duration_target)\n    return x, log_duration_prediction, embeddings\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/mas/","title":"Monotonic Alignments Shrink","text":""},{"location":"models/tts/delightful_tts/acoustic_model/mas/#models.tts.delightful_tts.acoustic_model.mas.b_mas","title":"<code>b_mas(b_attn_map, in_lens, out_lens, width=1)</code>","text":"<p>Applies Monotonic Alignments Shrink (MAS) operation in parallel to the batches of an attention map. It uses the <code>mas_width1</code> function internally to perform MAS operation.</p> <p>Parameters:</p> Name Type Description Default <code>b_attn_map</code> <code>ndarray</code> <p>The batched attention map; a 3D array where the first dimension is the batch size, second dimension corresponds to source length, and third dimension corresponds to target length.</p> required <code>in_lens</code> <code>ndarray</code> <p>Lengths of sequences in the input batch.</p> required <code>out_lens</code> <code>ndarray</code> <p>Lengths of sequences in the output batch.</p> required <code>width</code> <code>int</code> <p>The width for the MAS operation. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If width is not equal to 1. This function currently supports only width of 1.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The batched attention map after applying the MAS operation. It has the same dimensions as <code>b_attn_map</code>.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/mas.py</code> <pre><code>@njit(parallel=True)\ndef b_mas(\n    b_attn_map: np.ndarray,\n    in_lens: np.ndarray,\n    out_lens: np.ndarray,\n    width: int=1) -&gt; np.ndarray:\n    r\"\"\"Applies Monotonic Alignments Shrink (MAS) operation in parallel to the batches of an attention map.\n    It uses the `mas_width1` function internally to perform MAS operation.\n\n    Args:\n        b_attn_map (np.ndarray): The batched attention map; a 3D array where the first dimension is the batch size, second dimension corresponds to source length, and third dimension corresponds to target length.\n        in_lens (np.ndarray): Lengths of sequences in the input batch.\n        out_lens (np.ndarray): Lengths of sequences in the output batch.\n        width (int, optional): The width for the MAS operation. Defaults to 1.\n\n    Raises:\n        AssertionError: If width is not equal to 1. This function currently supports only width of 1.\n\n    Returns:\n        np.ndarray: The batched attention map after applying the MAS operation. It has the same dimensions as `b_attn_map`.\n    \"\"\"\n    # Assert that the width is 1. This function currently supports only width of 1\n    assert width == 1\n    attn_out = np.zeros_like(b_attn_map)\n\n    # Loop over each attention map in the batch in parallel\n    for b in prange(b_attn_map.shape[0]):\n        # Apply Monotonic Alignments Shrink operation to the b-th attention map in the batch\n        out = mas_width1(b_attn_map[b, 0, : out_lens[b], : in_lens[b]])\n\n        # Update the b-th attention map in the output with the result of MAS operation\n        attn_out[b, 0, : out_lens[b], : in_lens[b]] = out\n\n    # Return the batched attention map after applying the MAS operation\n    return attn_out\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/mas/#models.tts.delightful_tts.acoustic_model.mas.mas_width1","title":"<code>mas_width1(attn_map)</code>","text":"<p>Applies a Monotonic Alignments Shrink (MAS) operation with a hard-coded width of 1 to an attention map. Mas with hardcoded width=1 Essentially, it produces optimal alignments based on previous attention distribution.</p> <p>Parameters:</p> Name Type Description Default <code>attn_map</code> <code>ndarray</code> <p>The original attention map, a 2D numpy array where rows correspond to mel bins and columns to text bins.</p> required <p>Returns:</p> Name Type Description <code>opt</code> <code>ndarray</code> <p>Returns the optimal attention map after applying the MAS operation.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/mas.py</code> <pre><code>@njit(fastmath=True)\ndef mas_width1(attn_map: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Applies a Monotonic Alignments Shrink (MAS) operation with a hard-coded width of 1 to an attention map.\n    Mas with hardcoded width=1\n    Essentially, it produces optimal alignments based on previous attention distribution.\n\n    Args:\n        attn_map (np.ndarray): The original attention map, a 2D numpy array where rows correspond to mel bins and columns to text bins.\n\n    Returns:\n        opt (np.ndarray): Returns the optimal attention map after applying the MAS operation.\n    \"\"\"\n    # assumes mel x text\n    # Create a placeholder for the output\n    opt = np.zeros_like(attn_map)\n\n    # Convert the attention map to log scale for stability\n    attn_map = np.log(attn_map)\n\n    # Initialize the first row of attention map appropriately\n    attn_map[0, 1:] = -np.inf\n\n    # Initialize log_p with the first row of attention map\n    log_p = np.zeros_like(attn_map)\n    log_p[0, :] = attn_map[0, :]\n\n    # Placeholder to remember the previous indices for backtracking later\n    prev_ind = np.zeros_like(attn_map, dtype=np.int64)\n\n    # Compute the log probabilities based on previous attention distribution\n    for i in range(1, attn_map.shape[0]):\n        for j in range(attn_map.shape[1]):  # for each text dim\n            prev_log = log_p[i - 1, j]\n            prev_j = j\n\n            # Compare with left (j-1) pixel and update if the left pixel has larger log probability\n            if j - 1 &gt;= 0 and log_p[i - 1, j - 1] &gt;= log_p[i - 1, j]:\n                prev_log = log_p[i - 1, j - 1]\n                prev_j = j - 1\n\n            log_p[i, j] = attn_map[i, j] + prev_log\n\n            # Store the position of maximum cumulative log probability\n            prev_ind[i, j] = prev_j\n\n    # Backtrack to retrieve the path of attention with maximum cumulative log probability\n    curr_text_idx = attn_map.shape[1] - 1\n    for i in range(attn_map.shape[0] - 1, -1, -1):\n        opt[i, curr_text_idx] = 1\n        curr_text_idx = prev_ind[i, curr_text_idx]\n\n    # Mark the first position of the optimal path\n    opt[0, curr_text_idx] = 1\n    return opt\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/phoneme_prosody_predictor/","title":"Phoneme Prosody Predictor","text":""},{"location":"models/tts/delightful_tts/acoustic_model/phoneme_prosody_predictor/#models.tts.delightful_tts.acoustic_model.phoneme_prosody_predictor.PhonemeProsodyPredictor","title":"<code>PhonemeProsodyPredictor</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class to define the Phoneme Prosody Predictor.</p> <p>In linguistics, prosody (/\u02c8pr\u0252s\u0259di, \u02c8pr\u0252z\u0259di/) is the study of elements of speech that are not individual phonetic segments (vowels and consonants) but which are properties of syllables and larger units of speech, including linguistic functions such as intonation, stress, and rhythm. Such elements are known as suprasegmentals.</p> <p>Wikipedia Prosody (linguistics)</p> <p>This prosody predictor is non-parallel and is inspired by the work of Du et al., 2021 ?. It consists of multiple convolution transpose, Leaky ReLU activation, LayerNorm, and dropout layers, followed by a linear transformation to generate the final output.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>AcousticModelConfigType</code> <p>Configuration object with model parameters.</p> required <code>phoneme_level</code> <code>bool</code> <p>A flag to decide whether to use phoneme level bottleneck size.</p> required <code>leaky_relu_slope</code> <code>float</code> <p>The negative slope of LeakyReLU activation function.</p> <code>LEAKY_RELU_SLOPE</code> Source code in <code>models/tts/delightful_tts/acoustic_model/phoneme_prosody_predictor.py</code> <pre><code>class PhonemeProsodyPredictor(Module):\n    r\"\"\"A class to define the Phoneme Prosody Predictor.\n\n    In linguistics, prosody (/\u02c8pr\u0252s\u0259di, \u02c8pr\u0252z\u0259di/) is the study of elements of speech that are not individual phonetic segments (vowels and consonants) but which are properties of syllables and larger units of speech, including linguistic functions such as intonation, stress, and rhythm. Such elements are known as suprasegmentals.\n\n    [Wikipedia Prosody (linguistics)](https://en.wikipedia.org/wiki/Prosody_(linguistics))\n\n    This prosody predictor is non-parallel and is inspired by the **work of Du et al., 2021 ?**. It consists of\n    multiple convolution transpose, Leaky ReLU activation, LayerNorm, and dropout layers, followed by a\n    linear transformation to generate the final output.\n\n    Args:\n        model_config (AcousticModelConfigType): Configuration object with model parameters.\n        phoneme_level (bool): A flag to decide whether to use phoneme level bottleneck size.\n        leaky_relu_slope (float): The negative slope of LeakyReLU activation function.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: AcousticModelConfigType,\n        phoneme_level: bool,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n\n        # Get the configuration\n        self.d_model = model_config.encoder.n_hidden\n        kernel_size = model_config.reference_encoder.predictor_kernel_size\n        dropout = model_config.encoder.p_dropout\n\n        # Decide on the bottleneck size based on phoneme level flag\n        bottleneck_size = (\n            model_config.reference_encoder.bottleneck_size_p\n            if phoneme_level\n            else model_config.reference_encoder.bottleneck_size_u\n        )\n\n        # Define the layers\n        self.layers = nn.ModuleList(\n            [\n                ConvTransposed(\n                    self.d_model,\n                    self.d_model,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(leaky_relu_slope),\n                nn.LayerNorm(\n                    self.d_model,\n                ),\n                nn.Dropout(dropout),\n                ConvTransposed(\n                    self.d_model,\n                    self.d_model,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(leaky_relu_slope),\n                nn.LayerNorm(\n                    self.d_model,\n                ),\n                nn.Dropout(dropout),\n            ],\n        )\n\n        # Output bottleneck layer\n        self.predictor_bottleneck = nn.Linear(\n            self.d_model,\n            bottleneck_size,\n        )\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the prosody predictor.\n\n        Args:\n            x (torch.Tensor): A 3-dimensional tensor `[B, src_len, d_model]`.\n            mask (torch.Tensor): A 2-dimensional tensor `[B, src_len]`.\n\n        Returns:\n            torch.Tensor: A 3-dimensional tensor `[B, src_len, 2 * d_model]`.\n        \"\"\"\n        # Expand the mask tensor's dimensions from [B, src_len] to [B, src_len, 1]\n        mask = mask.unsqueeze(2)\n\n        # Pass the input through the layers\n        for layer in self.layers:\n            x = layer(x)\n\n        # Apply mask\n        x = x.masked_fill(mask, 0.0)\n\n        # Final linear transformation\n        return self.predictor_bottleneck(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/phoneme_prosody_predictor/#models.tts.delightful_tts.acoustic_model.phoneme_prosody_predictor.PhonemeProsodyPredictor.forward","title":"<code>forward(x, mask)</code>","text":"<p>Forward pass of the prosody predictor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3-dimensional tensor <code>[B, src_len, d_model]</code>.</p> required <code>mask</code> <code>Tensor</code> <p>A 2-dimensional tensor <code>[B, src_len]</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 3-dimensional tensor <code>[B, src_len, 2 * d_model]</code>.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/phoneme_prosody_predictor.py</code> <pre><code>def forward(self, x: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the prosody predictor.\n\n    Args:\n        x (torch.Tensor): A 3-dimensional tensor `[B, src_len, d_model]`.\n        mask (torch.Tensor): A 2-dimensional tensor `[B, src_len]`.\n\n    Returns:\n        torch.Tensor: A 3-dimensional tensor `[B, src_len, 2 * d_model]`.\n    \"\"\"\n    # Expand the mask tensor's dimensions from [B, src_len] to [B, src_len, 1]\n    mask = mask.unsqueeze(2)\n\n    # Pass the input through the layers\n    for layer in self.layers:\n        x = layer(x)\n\n    # Apply mask\n    x = x.masked_fill(mask, 0.0)\n\n    # Final linear transformation\n    return self.predictor_bottleneck(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/","title":"Pitch Adaptor Conv","text":""},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/#models.tts.delightful_tts.acoustic_model.pitch_adaptor_conv.PitchAdaptorConv","title":"<code>PitchAdaptorConv</code>","text":"<p>             Bases: <code>Module</code></p> <p>The PitchAdaptorConv class is a pitch adaptor network in the model. Updated version of the PitchAdaptorConv uses the conv embeddings for the pitch.</p> <p>Parameters:</p> Name Type Description Default <code>channels_in</code> <code>int</code> <p>Number of in channels for conv layers.</p> required <code>channels_out</code> <code>int</code> <p>Number of out channels.</p> required <code>kernel_size</code> <code>int</code> <p>Size the kernel for the conv layers.</p> required <code>dropout</code> <code>float</code> <p>Probability of dropout.</p> required <code>leaky_relu_slope</code> <code>float</code> <p>Slope for the leaky relu.</p> required <code>emb_kernel_size</code> <code>int</code> <p>Size the kernel for the pitch embedding.</p> required inputs, mask <ul> <li>inputs (batch, time1, dim): Tensor containing input vector</li> <li>target (batch, 1, time2): Tensor containing the pitch target</li> <li>dr (batch, time1): Tensor containing aligner durations vector</li> <li>mask (batch, time1): Tensor containing indices to be masked</li> </ul> <p>Returns:     - pitch prediction (batch, 1, time1): Tensor produced by pitch predictor     - pitch embedding (batch, channels, time1): Tensor produced pitch adaptor     - average pitch target(train only) (batch, 1, time1): Tensor produced after averaging over durations</p> Source code in <code>models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv.py</code> <pre><code>class PitchAdaptorConv(nn.Module):\n    \"\"\"The PitchAdaptorConv class is a pitch adaptor network in the model.\n    Updated version of the PitchAdaptorConv uses the conv embeddings for the pitch.\n\n    Args:\n        channels_in (int): Number of in channels for conv layers.\n        channels_out (int): Number of out channels.\n        kernel_size (int): Size the kernel for the conv layers.\n        dropout (float): Probability of dropout.\n        leaky_relu_slope (float): Slope for the leaky relu.\n        emb_kernel_size (int): Size the kernel for the pitch embedding.\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time1, dim): Tensor containing input vector\n        - **target** (batch, 1, time2): Tensor containing the pitch target\n        - **dr** (batch, time1): Tensor containing aligner durations vector\n        - **mask** (batch, time1): Tensor containing indices to be masked\n    Returns:\n        - **pitch prediction** (batch, 1, time1): Tensor produced by pitch predictor\n        - **pitch embedding** (batch, channels, time1): Tensor produced pitch adaptor\n        - **average pitch target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels_hidden: int,\n        channels_out: int,\n        kernel_size: int,\n        dropout: float,\n        leaky_relu_slope: float,\n        emb_kernel_size: int,\n    ):\n        super().__init__()\n        self.pitch_predictor = VariancePredictor(\n            channels_in=channels_in,\n            channels=channels_hidden,\n            channels_out=channels_out,\n            kernel_size=kernel_size,\n            p_dropout=dropout,\n            leaky_relu_slope=leaky_relu_slope,\n        )\n        self.pitch_emb = nn.Conv1d(\n            1,\n            channels_hidden,\n            kernel_size=emb_kernel_size,\n            padding=int((emb_kernel_size - 1) / 2),\n        )\n\n    def get_pitch_embedding_train(\n        self,\n        x: torch.Tensor,\n        target: torch.Tensor,\n        dr: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Function is used during training to get the pitch prediction, average pitch target,\n        and pitch embedding.\n\n        Args:\n            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                            T_src is the source sequence length, and C is the number of channels.\n            target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n                                T_max2 is the maximum target sequence length.\n            dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the durations.\n            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the mask.\n\n        Returns:\n            pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the pitch prediction.\n            avg_pitch_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                            T_src is the source sequence length. The values represent the average pitch target.\n            pitch_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                    C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n        Shapes:\n            x: :math: `[B, T_src, C]`\n            target: :math: `[B, 1, T_max2]`\n            dr: :math: `[B, T_src]`\n            mask: :math: `[B, T_src]`\n        \"\"\"\n        pitch_pred = self.pitch_predictor.forward(x, mask)\n        pitch_pred = pitch_pred.unsqueeze(1)\n\n        avg_pitch_target = average_over_durations(target, dr)\n        pitch_emb = self.pitch_emb(avg_pitch_target)\n\n        return pitch_pred, avg_pitch_target, pitch_emb\n\n    def add_pitch_embedding_train(\n        self,\n        x: torch.Tensor,\n        target: torch.Tensor,\n        dr: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Add pitch embedding during training.\n\n        This method calculates the pitch embedding and adds it to the input tensor 'x'.\n        It also returns the predicted pitch and the average target pitch.\n\n        Args:\n            x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n            target (torch.Tensor): The target tensor used in the pitch embedding calculation.\n            dr (torch.Tensor): The duration tensor used in the pitch embedding calculation.\n            mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n\n        Returns:\n            x (torch.Tensor): The input tensor with added pitch embedding.\n            pitch_pred (torch.Tensor): The predicted pitch tensor.\n            avg_pitch_target (torch.Tensor): The average target pitch tensor.\n        \"\"\"\n        pitch_pred, avg_pitch_target, pitch_emb = self.get_pitch_embedding_train(\n            x=x,\n            target=target.unsqueeze(1),\n            dr=dr,\n            mask=mask,\n        )\n        x_pitch = x + pitch_emb.transpose(1, 2)\n        return x_pitch, pitch_pred, avg_pitch_target\n\n    def get_pitch_embedding(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Function is used during inference to get the pitch embedding and pitch prediction.\n\n        Args:\n            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                            T_src is the source sequence length, and C is the number of channels.\n            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                                T_src is the source sequence length. The values represent the mask.\n\n        Returns:\n            pitch_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                            C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n            pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the pitch prediction.\n        \"\"\"\n        pitch_pred = self.pitch_predictor.forward(x, mask)\n        pitch_pred = pitch_pred.unsqueeze(1)\n\n        pitch_emb_pred = self.pitch_emb(pitch_pred)\n        return pitch_emb_pred, pitch_pred\n\n    def add_pitch_embedding(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Add pitch embedding during inference.\n\n        This method calculates the pitch embedding and adds it to the input tensor 'x'.\n        It also returns the predicted pitch.\n\n        Args:\n            x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n            mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n            pitch_transform (Callable): A function to transform the pitch prediction.\n\n        Returns:\n            x (torch.Tensor): The input tensor with added pitch embedding.\n            pitch_pred (torch.Tensor): The predicted pitch tensor.\n        \"\"\"\n        pitch_emb_pred, pitch_pred = self.get_pitch_embedding(x, mask)\n        x_pitch = x + pitch_emb_pred.transpose(1, 2)\n        return x_pitch, pitch_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/#models.tts.delightful_tts.acoustic_model.pitch_adaptor_conv.PitchAdaptorConv.add_pitch_embedding","title":"<code>add_pitch_embedding(x, mask)</code>","text":"<p>Add pitch embedding during inference.</p> <p>This method calculates the pitch embedding and adds it to the input tensor 'x'. It also returns the predicted pitch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to which the pitch embedding will be added.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor used in the pitch embedding calculation.</p> required <code>pitch_transform</code> <code>Callable</code> <p>A function to transform the pitch prediction.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>The input tensor with added pitch embedding.</p> <code>pitch_pred</code> <code>Tensor</code> <p>The predicted pitch tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv.py</code> <pre><code>def add_pitch_embedding(\n    self,\n    x: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Add pitch embedding during inference.\n\n    This method calculates the pitch embedding and adds it to the input tensor 'x'.\n    It also returns the predicted pitch.\n\n    Args:\n        x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n        mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n        pitch_transform (Callable): A function to transform the pitch prediction.\n\n    Returns:\n        x (torch.Tensor): The input tensor with added pitch embedding.\n        pitch_pred (torch.Tensor): The predicted pitch tensor.\n    \"\"\"\n    pitch_emb_pred, pitch_pred = self.get_pitch_embedding(x, mask)\n    x_pitch = x + pitch_emb_pred.transpose(1, 2)\n    return x_pitch, pitch_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/#models.tts.delightful_tts.acoustic_model.pitch_adaptor_conv.PitchAdaptorConv.add_pitch_embedding_train","title":"<code>add_pitch_embedding_train(x, target, dr, mask)</code>","text":"<p>Add pitch embedding during training.</p> <p>This method calculates the pitch embedding and adds it to the input tensor 'x'. It also returns the predicted pitch and the average target pitch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to which the pitch embedding will be added.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor used in the pitch embedding calculation.</p> required <code>dr</code> <code>Tensor</code> <p>The duration tensor used in the pitch embedding calculation.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor used in the pitch embedding calculation.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>The input tensor with added pitch embedding.</p> <code>pitch_pred</code> <code>Tensor</code> <p>The predicted pitch tensor.</p> <code>avg_pitch_target</code> <code>Tensor</code> <p>The average target pitch tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv.py</code> <pre><code>def add_pitch_embedding_train(\n    self,\n    x: torch.Tensor,\n    target: torch.Tensor,\n    dr: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Add pitch embedding during training.\n\n    This method calculates the pitch embedding and adds it to the input tensor 'x'.\n    It also returns the predicted pitch and the average target pitch.\n\n    Args:\n        x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n        target (torch.Tensor): The target tensor used in the pitch embedding calculation.\n        dr (torch.Tensor): The duration tensor used in the pitch embedding calculation.\n        mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n\n    Returns:\n        x (torch.Tensor): The input tensor with added pitch embedding.\n        pitch_pred (torch.Tensor): The predicted pitch tensor.\n        avg_pitch_target (torch.Tensor): The average target pitch tensor.\n    \"\"\"\n    pitch_pred, avg_pitch_target, pitch_emb = self.get_pitch_embedding_train(\n        x=x,\n        target=target.unsqueeze(1),\n        dr=dr,\n        mask=mask,\n    )\n    x_pitch = x + pitch_emb.transpose(1, 2)\n    return x_pitch, pitch_pred, avg_pitch_target\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/#models.tts.delightful_tts.acoustic_model.pitch_adaptor_conv.PitchAdaptorConv.get_pitch_embedding","title":"<code>get_pitch_embedding(x, mask)</code>","text":"<p>Function is used during inference to get the pitch embedding and pitch prediction.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3D tensor of shape [B, T_src, C] where B is the batch size,             T_src is the source sequence length, and C is the number of channels.</p> required <code>mask</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the mask.</p> required <p>Returns:</p> Name Type Description <code>pitch_emb_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, C, T_src] where B is the batch size,                             C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.</p> <code>pitch_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                         T_src is the source sequence length. The values represent the pitch prediction.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv.py</code> <pre><code>def get_pitch_embedding(\n    self,\n    x: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Function is used during inference to get the pitch embedding and pitch prediction.\n\n    Args:\n        x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                        T_src is the source sequence length, and C is the number of channels.\n        mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the mask.\n\n    Returns:\n        pitch_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                        C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n        pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                    T_src is the source sequence length. The values represent the pitch prediction.\n    \"\"\"\n    pitch_pred = self.pitch_predictor.forward(x, mask)\n    pitch_pred = pitch_pred.unsqueeze(1)\n\n    pitch_emb_pred = self.pitch_emb(pitch_pred)\n    return pitch_emb_pred, pitch_pred\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv/#models.tts.delightful_tts.acoustic_model.pitch_adaptor_conv.PitchAdaptorConv.get_pitch_embedding_train","title":"<code>get_pitch_embedding_train(x, target, dr, mask)</code>","text":"<p>Function is used during training to get the pitch prediction, average pitch target, and pitch embedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3D tensor of shape [B, T_src, C] where B is the batch size,             T_src is the source sequence length, and C is the number of channels.</p> required <code>target</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_max2] where B is the batch size,                 T_max2 is the maximum target sequence length.</p> required <code>dr</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the durations.</p> required <code>mask</code> <code>Tensor</code> <p>A 2D tensor of shape [B, T_src] where B is the batch size,                 T_src is the source sequence length. The values represent the mask.</p> required <p>Returns:</p> Name Type Description <code>pitch_pred</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                         T_src is the source sequence length. The values represent the pitch prediction.</p> <code>avg_pitch_target</code> <code>Tensor</code> <p>A 3D tensor of shape [B, 1, T_src] where B is the batch size,                             T_src is the source sequence length. The values represent the average pitch target.</p> <code>pitch_emb</code> <code>Tensor</code> <p>A 3D tensor of shape [B, C, T_src] where B is the batch size,                     C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.</p> <p>Shapes:     x: :math: <code>[B, T_src, C]</code>     target: :math: <code>[B, 1, T_max2]</code>     dr: :math: <code>[B, T_src]</code>     mask: :math: <code>[B, T_src]</code></p> Source code in <code>models/tts/delightful_tts/acoustic_model/pitch_adaptor_conv.py</code> <pre><code>def get_pitch_embedding_train(\n    self,\n    x: torch.Tensor,\n    target: torch.Tensor,\n    dr: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Function is used during training to get the pitch prediction, average pitch target,\n    and pitch embedding.\n\n    Args:\n        x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n                        T_src is the source sequence length, and C is the number of channels.\n        target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n                            T_max2 is the maximum target sequence length.\n        dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the durations.\n        mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n                            T_src is the source sequence length. The values represent the mask.\n\n    Returns:\n        pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                    T_src is the source sequence length. The values represent the pitch prediction.\n        avg_pitch_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n                                        T_src is the source sequence length. The values represent the average pitch target.\n        pitch_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n                                C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n    Shapes:\n        x: :math: `[B, T_src, C]`\n        target: :math: `[B, 1, T_max2]`\n        dr: :math: `[B, T_src]`\n        mask: :math: `[B, T_src]`\n    \"\"\"\n    pitch_pred = self.pitch_predictor.forward(x, mask)\n    pitch_pred = pitch_pred.unsqueeze(1)\n\n    avg_pitch_target = average_over_durations(target, dr)\n    pitch_emb = self.pitch_emb(avg_pitch_target)\n\n    return pitch_pred, avg_pitch_target, pitch_emb\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/","title":"References","text":""},{"location":"models/tts/delightful_tts/acoustic_model/readme/#references","title":"References","text":""},{"location":"models/tts/delightful_tts/acoustic_model/readme/#accoustic-model","title":"Accoustic Model","text":"<p>The DelightfulTTS AcousticModel class represents a PyTorch module for an acoustic model in text-to-speech (TTS). The acoustic model is responsible for predicting speech signals from phoneme sequences.</p> <p>The model comprises multiple sub-modules including encoder, decoder and various prosody encoders and predictors. Additionally, a pitch and length adaptor are instantiated.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#embedding","title":"Embedding","text":"<p>This class represents a simple embedding layer but without any learning of the embeddings.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#helpers","title":"Helpers","text":"<p>Acoustic model helpers methods</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#variance-predictor","title":"Variance Predictor","text":"<p>This is a Duration and Pitch predictor neural network module in PyTorch.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#pitch-adaptor-conv","title":"Pitch Adaptor Conv","text":"<p>Variance Adaptor with an added 1D conv layer. Used to get pitch embeddings.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#energy-adaptor","title":"Energy Adaptor","text":"<p>Variance Adaptor with an added 1D conv layer. Used to get energy embeddings.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#length-adaptor","title":"Length Adaptor","text":"<p>The LengthAdaptor module is used to adjust the duration of phonemes. Used in Tacotron 2 model.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#phoneme-prosody-predictor","title":"Phoneme Prosody Predictor","text":"<p>A class to define the Phoneme Prosody Predictor.  This prosody predictor is non-parallel and is inspired by the work of Du et al., 2021 ?.</p> <p>In linguistics, prosody (/\u02c8pr\u0252s\u0259di, \u02c8pr\u0252z\u0259di/)is the study of elements of speech that are not individual phonetic segments (vowels and consonants) but which are properties of syllables and larger units of speech, including linguistic functions such as intonation, stress, and rhythm. Such elements are known as suprasegmentals.</p> <p>Wikipedia Prosody (linguistics)</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#aligner","title":"Aligner","text":"<p>Aligner class represents a PyTorch module responsible for alignment tasks in a sequence-to-sequence model. It uses convolutional layers combined with LeakyReLU activation functions to project inputs to a hidden representation.</p> <p>Also, for training purposes, binarizes attention with MAS</p>"},{"location":"models/tts/delightful_tts/acoustic_model/readme/#monotonic-alignments-shrink","title":"Monotonic Alignments Shrink","text":"<p><code>mas_width1</code> Applies a Monotonic Alignments Shrink (MAS) operation with a hard-coded width of 1 to an attention map. Mas with hardcoded <code>width=1</code></p> <p><code>b_mas</code> Applies Monotonic Alignments Shrink (MAS) operation in parallel to the batches of an attention map. It uses the <code>mas_width1</code> function internally to perform MAS operation.</p>"},{"location":"models/tts/delightful_tts/acoustic_model/variance_predictor/","title":"Variance Predictor","text":""},{"location":"models/tts/delightful_tts/acoustic_model/variance_predictor/#models.tts.delightful_tts.acoustic_model.variance_predictor.VariancePredictor","title":"<code>VariancePredictor</code>","text":"<p>             Bases: <code>Module</code></p> <p>Duration and Pitch predictor neural network module in PyTorch.</p> <p>It consists of multiple layers, including <code>ConvTransposed</code> layers (custom convolution transpose layers from the <code>model.conv_blocks</code> module), LeakyReLU activation functions, Layer Normalization and Dropout layers.</p> <p>Constructor for <code>VariancePredictor</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>channels_in</code> <code>int</code> <p>Number of input channels.</p> required <code>channels</code> <code>int</code> <p>Number of output channels for ConvTransposed layers and input channels for linear layer.</p> required <code>channels_out</code> <code>int</code> <p>Number of output channels for linear layer.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the kernel for ConvTransposed layers.</p> required <code>p_dropout</code> <code>float</code> <p>Probability of dropout.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/variance_predictor.py</code> <pre><code>class VariancePredictor(Module):\n    r\"\"\"Duration and Pitch predictor neural network module in PyTorch.\n\n    It consists of multiple layers, including `ConvTransposed` layers (custom convolution transpose layers from\n    the `model.conv_blocks` module), LeakyReLU activation functions, Layer Normalization and Dropout layers.\n\n    Constructor for `VariancePredictor` class.\n\n    Args:\n        channels_in (int): Number of input channels.\n        channels (int): Number of output channels for ConvTransposed layers and input channels for linear layer.\n        channels_out (int): Number of output channels for linear layer.\n        kernel_size (int): Size of the kernel for ConvTransposed layers.\n        p_dropout (float): Probability of dropout.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels: int,\n        channels_out: int,\n        kernel_size: int,\n        p_dropout: float,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n\n        self.layers = nn.ModuleList(\n            [\n                # Convolution transpose layer followed by LeakyReLU, LayerNorm and Dropout\n                ConvTransposed(\n                    channels_in,\n                    channels,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(leaky_relu_slope),\n                nn.LayerNorm(\n                    channels,\n                ),\n                nn.Dropout(p_dropout),\n                # Another \"block\" of ConvTransposed, LeakyReLU, LayerNorm, and Dropout\n                ConvTransposed(\n                    channels,\n                    channels,\n                    kernel_size=kernel_size,\n                    padding=(kernel_size - 1) // 2,\n                ),\n                nn.LeakyReLU(leaky_relu_slope),\n                nn.LayerNorm(\n                    channels,\n                ),\n                nn.Dropout(p_dropout),\n            ],\n        )\n\n        # Output linear layer\n        self.linear_layer = nn.Linear(\n            channels,\n            channels_out,\n        )\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass for `VariancePredictor`.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            mask (torch.Tensor): Mask tensor, has the same size as x.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        # Sequentially pass the input through all defined layers\n        # (ConvTransposed -&gt; LeakyReLU -&gt; LayerNorm -&gt; Dropout -&gt; ConvTransposed -&gt; LeakyReLU -&gt; LayerNorm -&gt; Dropout)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.linear_layer(x)\n        x = x.squeeze(-1)\n        return x.masked_fill(mask, 0.0)\n</code></pre>"},{"location":"models/tts/delightful_tts/acoustic_model/variance_predictor/#models.tts.delightful_tts.acoustic_model.variance_predictor.VariancePredictor.forward","title":"<code>forward(x, mask)</code>","text":"<p>Forward pass for <code>VariancePredictor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Mask tensor, has the same size as x.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>models/tts/delightful_tts/acoustic_model/variance_predictor.py</code> <pre><code>def forward(self, x: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass for `VariancePredictor`.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n        mask (torch.Tensor): Mask tensor, has the same size as x.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    # Sequentially pass the input through all defined layers\n    # (ConvTransposed -&gt; LeakyReLU -&gt; LayerNorm -&gt; Dropout -&gt; ConvTransposed -&gt; LeakyReLU -&gt; LayerNorm -&gt; Dropout)\n    for layer in self.layers:\n        x = layer(x)\n    x = self.linear_layer(x)\n    x = x.squeeze(-1)\n    return x.masked_fill(mask, 0.0)\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer/","title":"Conformer","text":""},{"location":"models/tts/delightful_tts/attention/conformer/#models.tts.delightful_tts.attention.conformer.Conformer","title":"<code>Conformer</code>","text":"<p>             Bases: <code>Module</code></p> <p><code>Conformer</code> class represents the <code>Conformer</code> model which is a sequence-to-sequence model used in some modern automated speech recognition systems. It is composed of several <code>ConformerBlocks</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The number of expected features in the input.</p> required <code>n_layers</code> <code>int</code> <p>The number of <code>ConformerBlocks</code> in the Conformer model.</p> required <code>n_heads</code> <code>int</code> <p>The number of heads in the multiheaded self-attention mechanism in each <code>ConformerBlock</code>.</p> required <code>embedding_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>p_dropout</code> <code>float</code> <p>The dropout probability to be used in each <code>ConformerBlock</code>.</p> required <code>kernel_size_conv_mod</code> <code>int</code> <p>The size of the convolving kernel in the convolution module of each <code>ConformerBlock</code>.</p> required <code>with_ff</code> <code>bool</code> <p>If True, each <code>ConformerBlock</code> uses FeedForward layer inside it.</p> required Source code in <code>models/tts/delightful_tts/attention/conformer.py</code> <pre><code>class Conformer(Module):\n    r\"\"\"`Conformer` class represents the `Conformer` model which is a sequence-to-sequence model\n    used in some modern automated speech recognition systems. It is composed of several `ConformerBlocks`.\n\n    Args:\n        dim (int): The number of expected features in the input.\n        n_layers (int): The number of `ConformerBlocks` in the Conformer model.\n        n_heads (int): The number of heads in the multiheaded self-attention mechanism in each `ConformerBlock`.\n        embedding_dim (int): The dimension of the embeddings.\n        p_dropout (float): The dropout probability to be used in each `ConformerBlock`.\n        kernel_size_conv_mod (int): The size of the convolving kernel in the convolution module of each `ConformerBlock`.\n        with_ff (bool): If True, each `ConformerBlock` uses FeedForward layer inside it.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        n_layers: int,\n        n_heads: int,\n        embedding_dim: int,\n        p_dropout: float,\n        kernel_size_conv_mod: int,\n        with_ff: bool,\n    ):\n        super().__init__()\n        self.layer_stack = nn.ModuleList(\n            [\n                ConformerBlock(\n                    dim,\n                    n_heads,\n                    kernel_size_conv_mod=kernel_size_conv_mod,\n                    dropout=p_dropout,\n                    embedding_dim=embedding_dim,\n                    with_ff=with_ff,\n                )\n                for _ in range(n_layers)\n            ],\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask: torch.Tensor,\n        embeddings: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Forward Pass of the Conformer block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n            mask (Tensor): The mask tensor.\n            embeddings (Tensor): Embeddings tensor.\n            encoding (Tensor): The positional encoding tensor.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n        \"\"\"\n        attn_mask = mask.view((mask.shape[0], 1, 1, mask.shape[1]))\n        attn_mask.to(x.device)\n        for enc_layer in self.layer_stack:\n            x = enc_layer(\n                x,\n                mask=mask,\n                slf_attn_mask=attn_mask,\n                embeddings=embeddings,\n                encoding=encoding,\n            )\n        return x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer/#models.tts.delightful_tts.attention.conformer.Conformer.forward","title":"<code>forward(x, mask, embeddings, encoding)</code>","text":"<p>Forward Pass of the Conformer block.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, num_features).</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor.</p> required <code>embeddings</code> <code>Tensor</code> <p>Embeddings tensor.</p> required <code>encoding</code> <code>Tensor</code> <p>The positional encoding tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor of shape (batch_size, seq_len, num_features).</p> Source code in <code>models/tts/delightful_tts/attention/conformer.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    mask: torch.Tensor,\n    embeddings: torch.Tensor,\n    encoding: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"Forward Pass of the Conformer block.\n\n    Args:\n        x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n        mask (Tensor): The mask tensor.\n        embeddings (Tensor): Embeddings tensor.\n        encoding (Tensor): The positional encoding tensor.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n    \"\"\"\n    attn_mask = mask.view((mask.shape[0], 1, 1, mask.shape[1]))\n    attn_mask.to(x.device)\n    for enc_layer in self.layer_stack:\n        x = enc_layer(\n            x,\n            mask=mask,\n            slf_attn_mask=attn_mask,\n            embeddings=embeddings,\n            encoding=encoding,\n        )\n    return x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer_block/","title":"Conformer Block","text":""},{"location":"models/tts/delightful_tts/attention/conformer_block/#models.tts.delightful_tts.attention.conformer_block.ConformerBlock","title":"<code>ConformerBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>ConformerBlock class represents a block in the Conformer model architecture. The block includes a pointwise convolution followed by Gated Linear Units (<code>GLU</code>) activation layer (<code>Conv1dGLU</code>), a Conformer self attention layer (<code>ConformerMultiHeadedSelfAttention</code>), and optional feed-forward layer (<code>FeedForward</code>).</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of expected features in the input.</p> required <code>n_head</code> <code>int</code> <p>The number of heads for the multiheaded attention mechanism.</p> required <code>kernel_size_conv_mod</code> <code>int</code> <p>The size of the convolving kernel for the convolution module.</p> required <code>embedding_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability.</p> required <code>with_ff</code> <code>bool</code> <p>If True, uses FeedForward layer inside ConformerBlock.</p> required Source code in <code>models/tts/delightful_tts/attention/conformer_block.py</code> <pre><code>class ConformerBlock(Module):\n    r\"\"\"ConformerBlock class represents a block in the Conformer model architecture.\n    The block includes a pointwise convolution followed by Gated Linear Units (`GLU`) activation layer (`Conv1dGLU`),\n    a Conformer self attention layer (`ConformerMultiHeadedSelfAttention`), and optional feed-forward layer (`FeedForward`).\n\n    Args:\n        d_model (int): The number of expected features in the input.\n        n_head (int): The number of heads for the multiheaded attention mechanism.\n        kernel_size_conv_mod (int): The size of the convolving kernel for the convolution module.\n        embedding_dim (int): The dimension of the embeddings.\n        dropout (float): The dropout probability.\n        with_ff (bool): If True, uses FeedForward layer inside ConformerBlock.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        kernel_size_conv_mod: int,\n        embedding_dim: int,\n        dropout: float,\n        with_ff: bool,\n    ):\n        super().__init__()\n        self.with_ff = with_ff\n        self.conditioning = Conv1dGLU(\n            d_model=d_model,\n            kernel_size=kernel_size_conv_mod,\n            padding=kernel_size_conv_mod // 2,\n            embedding_dim=embedding_dim,\n        )\n        if self.with_ff:\n            self.ff = FeedForward(\n                d_model=d_model,\n                dropout=dropout,\n                kernel_size=3,\n            )\n        self.conformer_conv_1 = ConformerConvModule(\n            d_model,\n            kernel_size=kernel_size_conv_mod,\n            dropout=dropout,\n        )\n        self.ln = nn.LayerNorm(\n            d_model,\n        )\n        self.slf_attn = ConformerMultiHeadedSelfAttention(\n            d_model=d_model,\n            num_heads=n_head,\n            dropout_p=dropout,\n        )\n        self.conformer_conv_2 = ConformerConvModule(\n            d_model,\n            kernel_size=kernel_size_conv_mod,\n            dropout=dropout,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        embeddings: torch.Tensor,\n        mask: torch.Tensor,\n        slf_attn_mask: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the Conformer block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n            embeddings (Tensor): Embeddings tensor.\n            mask (Tensor): The mask tensor.\n            slf_attn_mask (Tensor): The mask for self-attention layer.\n            encoding (Tensor): The positional encoding tensor.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n        \"\"\"\n        x = self.conditioning.forward(x, embeddings=embeddings)\n        if self.with_ff:\n            x = self.ff(x) + x\n        x = self.conformer_conv_1(x) + x\n        res = x\n        x = self.ln(x)\n        x, _ = self.slf_attn(\n            query=x,\n            key=x,\n            value=x,\n            mask=slf_attn_mask,\n            encoding=encoding,\n        )\n        x = x + res\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        return self.conformer_conv_2(x) + x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer_block/#models.tts.delightful_tts.attention.conformer_block.ConformerBlock.forward","title":"<code>forward(x, embeddings, mask, slf_attn_mask, encoding)</code>","text":"<p>Forward pass of the Conformer block.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, num_features).</p> required <code>embeddings</code> <code>Tensor</code> <p>Embeddings tensor.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor.</p> required <code>slf_attn_mask</code> <code>Tensor</code> <p>The mask for self-attention layer.</p> required <code>encoding</code> <code>Tensor</code> <p>The positional encoding tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor of shape (batch_size, seq_len, num_features).</p> Source code in <code>models/tts/delightful_tts/attention/conformer_block.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    embeddings: torch.Tensor,\n    mask: torch.Tensor,\n    slf_attn_mask: torch.Tensor,\n    encoding: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the Conformer block.\n\n    Args:\n        x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n        embeddings (Tensor): Embeddings tensor.\n        mask (Tensor): The mask tensor.\n        slf_attn_mask (Tensor): The mask for self-attention layer.\n        encoding (Tensor): The positional encoding tensor.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n    \"\"\"\n    x = self.conditioning.forward(x, embeddings=embeddings)\n    if self.with_ff:\n        x = self.ff(x) + x\n    x = self.conformer_conv_1(x) + x\n    res = x\n    x = self.ln(x)\n    x, _ = self.slf_attn(\n        query=x,\n        key=x,\n        value=x,\n        mask=slf_attn_mask,\n        encoding=encoding,\n    )\n    x = x + res\n    x = x.masked_fill(mask.unsqueeze(-1), 0)\n    return self.conformer_conv_2(x) + x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer_conv_module/","title":"Conformer Convolution Module","text":""},{"location":"models/tts/delightful_tts/attention/conformer_conv_module/#models.tts.delightful_tts.attention.conformer_conv_module.ConformerConvModule","title":"<code>ConformerConvModule</code>","text":"<p>             Bases: <code>Module</code></p> <p>Conformer Convolution Module class represents a module in the Conformer model architecture. The module includes a layer normalization, pointwise and depthwise convolutional layers, Gated Linear Units (GLU) activation, and dropout layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of expected features in the input.</p> required <code>expansion_factor</code> <code>int</code> <p>The expansion factor for the hidden layer size in the feed-forward network, default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>The size of the convolving kernel, default is 7.</p> <code>7</code> <code>dropout</code> <code>float</code> <p>The dropout probability, default is 0.1.</p> <code>0.1</code> <code>leaky_relu_slope</code> <code>float</code> <p>Controls the angle of the negative slope of the LeakyReLU activation, default is <code>LEAKY_RELU_SLOPE</code>.</p> <code>LEAKY_RELU_SLOPE</code> Source code in <code>models/tts/delightful_tts/attention/conformer_conv_module.py</code> <pre><code>class ConformerConvModule(Module):\n    r\"\"\"Conformer Convolution Module class represents a module in the Conformer model architecture.\n    The module includes a layer normalization, pointwise and depthwise convolutional layers,\n    Gated Linear Units (GLU) activation, and dropout layer.\n\n    Args:\n        d_model (int): The number of expected features in the input.\n        expansion_factor (int): The expansion factor for the hidden layer size in the feed-forward network, default is 2.\n        kernel_size (int): The size of the convolving kernel, default is 7.\n        dropout (float): The dropout probability, default is 0.1.\n        leaky_relu_slope (float): Controls the angle of the negative slope of the LeakyReLU activation, default is `LEAKY_RELU_SLOPE`.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        expansion_factor: int = 2,\n        kernel_size: int = 7,\n        dropout: float = 0.1,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n        inner_dim = d_model * expansion_factor\n        self.ln_1 = nn.LayerNorm(d_model)\n        self.conv_1 = PointwiseConv1d(\n            d_model,\n            inner_dim * 2,\n        )\n        self.conv_act = GLUActivation()\n        self.depthwise = DepthWiseConv1d(\n            inner_dim,\n            inner_dim,\n            kernel_size=kernel_size,\n            padding=tools.calc_same_padding(kernel_size)[0],\n        )\n        self.ln_2 = nn.GroupNorm(\n            1,\n            inner_dim,\n        )\n        self.activation = nn.LeakyReLU(leaky_relu_slope)\n        self.conv_2 = PointwiseConv1d(\n            inner_dim,\n            d_model,\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the Conformer conv module.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n        \"\"\"\n        x = self.ln_1(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv_1(x)\n        x = self.conv_act(x)\n        x = self.depthwise(x)\n        x = self.ln_2(x)\n        x = self.activation(x)\n        x = self.conv_2(x)\n        x = x.permute(0, 2, 1)\n        return self.dropout(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer_conv_module/#models.tts.delightful_tts.attention.conformer_conv_module.ConformerConvModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the Conformer conv module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, num_features).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor of shape (batch_size, seq_len, num_features).</p> Source code in <code>models/tts/delightful_tts/attention/conformer_conv_module.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the Conformer conv module.\n\n    Args:\n        x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n    \"\"\"\n    x = self.ln_1(x)\n    x = x.permute(0, 2, 1)\n    x = self.conv_1(x)\n    x = self.conv_act(x)\n    x = self.depthwise(x)\n    x = self.ln_2(x)\n    x = self.activation(x)\n    x = self.conv_2(x)\n    x = x.permute(0, 2, 1)\n    return self.dropout(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/conformer_multi_headed_self_attention/","title":"Conformer Multi-Headed Self Attention","text":""},{"location":"models/tts/delightful_tts/attention/conformer_multi_headed_self_attention/#models.tts.delightful_tts.attention.conformer_multi_headed_self_attention.ConformerMultiHeadedSelfAttention","title":"<code>ConformerMultiHeadedSelfAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL, the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention module to generalize better on different input length and the resulting encoder is more robust to the variance of the utterance length. Conformer use <code>prenorm</code> residual units with dropout which helps training and regularizing deeper models.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimension of model</p> required <code>num_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout_p</code> <code>float</code> <p>probability of dropout</p> required inputs, mask <ul> <li>inputs (batch, time, dim): Tensor containing input vector</li> <li>mask (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked</li> </ul> <p>Returns:</p> Type Description <code>(batch, time, dim)</code> <p>Tensor produces by relative multi headed self attention module.</p> Source code in <code>models/tts/delightful_tts/attention/conformer_multi_headed_self_attention.py</code> <pre><code>class ConformerMultiHeadedSelfAttention(Module):\n    \"\"\"Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL,\n    the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention\n    module to generalize better on different input length and the resulting encoder is more robust to the variance of\n    the utterance length. Conformer use `prenorm` residual units with dropout which helps training\n    and regularizing deeper models.\n\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.\n        dropout_p (float): probability of dropout\n\n    Inputs: inputs, mask\n        - **inputs** (batch, time, dim): Tensor containing input vector\n        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n\n    Returns:\n        (batch, time, dim): Tensor produces by relative multi headed self attention module.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        num_heads: int,\n        dropout_p: float,\n    ):\n        super().__init__()\n\n        # Initialize the RelativeMultiHeadAttention module passing the model dimension and number of attention heads\n        self.attention = RelativeMultiHeadAttention(\n            d_model=d_model, num_heads=num_heads,\n        )\n        self.dropout = nn.Dropout(p=dropout_p)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        batch_size, _, _ = key.size()\n\n        # Trim or extend the \"encoding\" to match the size of key, and repeat this for each input in the batch\n        encoding = encoding[:, : key.shape[1]]\n        encoding = encoding.repeat(batch_size, 1, 1)\n\n        # Pass inputs through the RelativeMultiHeadAttention layer, dropout the resulting outputs\n        outputs, attn = self.attention(\n            query, key, value, pos_embedding=encoding, mask=mask,\n        )\n\n        # Apply dropout to the attention outputs\n        outputs = self.dropout(outputs)\n        return outputs, attn\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/feed_forward/","title":"Feed Forward","text":""},{"location":"models/tts/delightful_tts/attention/feed_forward/#models.tts.delightful_tts.attention.feed_forward.FeedForward","title":"<code>FeedForward</code>","text":"<p>             Bases: <code>Module</code></p> <p>Creates a feed-forward neural network. The network includes a layer normalization, an activation function (LeakyReLU), and dropout layers.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of expected features in the input.</p> required <code>kernel_size</code> <code>int</code> <p>The size of the convolving kernel for the first conv1d layer.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability.</p> required <code>expansion_factor</code> <code>int</code> <p>The expansion factor for the hidden layer size in the feed-forward network, default is 4.</p> <code>4</code> <code>leaky_relu_slope</code> <code>float</code> <p>Controls the angle of the negative slope of LeakyReLU activation, default is <code>LEAKY_RELU_SLOPE</code>.</p> <code>LEAKY_RELU_SLOPE</code> Source code in <code>models/tts/delightful_tts/attention/feed_forward.py</code> <pre><code>class FeedForward(Module):\n    r\"\"\"Creates a feed-forward neural network.\n    The network includes a layer normalization, an activation function (LeakyReLU), and dropout layers.\n\n    Args:\n        d_model (int): The number of expected features in the input.\n        kernel_size (int): The size of the convolving kernel for the first conv1d layer.\n        dropout (float): The dropout probability.\n        expansion_factor (int, optional): The expansion factor for the hidden layer size in the feed-forward network, default is 4.\n        leaky_relu_slope (float, optional): Controls the angle of the negative slope of LeakyReLU activation, default is `LEAKY_RELU_SLOPE`.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        kernel_size: int,\n        dropout: float,\n        expansion_factor: int = 4,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(d_model)\n        self.conv_1 = nn.Conv1d(\n            d_model,\n            d_model * expansion_factor,\n            kernel_size=kernel_size,\n            padding=kernel_size // 2,\n        )\n        self.act = nn.LeakyReLU(leaky_relu_slope)\n        self.conv_2 = nn.Conv1d(d_model * expansion_factor, d_model, kernel_size=1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the feed-forward neural network.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n\n        Returns:\n            Tensor: Output tensor of shape (batch_size, seq_len, num_features).\n        \"\"\"\n        # Apply layer normalization\n        x = self.ln(x)\n\n        # Forward pass through the first convolution layer, activation layer and dropout layer\n        x = x.permute((0, 2, 1))\n        x = self.conv_1(x)\n        x = x.permute((0, 2, 1))\n        x = self.act(x)\n        x = self.dropout(x)\n\n        # Forward pass through the second convolution layer and dropout layer\n        x = x.permute((0, 2, 1))\n        x = self.conv_2(x)\n        x = x.permute((0, 2, 1))\n        x = self.dropout(x)\n\n        # Scale the output by 0.5 (this helps with training stability)\n        return 0.5 * x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/feed_forward/#models.tts.delightful_tts.attention.feed_forward.FeedForward.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the feed-forward neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, num_features).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Output tensor of shape (batch_size, seq_len, num_features).</p> Source code in <code>models/tts/delightful_tts/attention/feed_forward.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the feed-forward neural network.\n\n    Args:\n        x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n\n    Returns:\n        Tensor: Output tensor of shape (batch_size, seq_len, num_features).\n    \"\"\"\n    # Apply layer normalization\n    x = self.ln(x)\n\n    # Forward pass through the first convolution layer, activation layer and dropout layer\n    x = x.permute((0, 2, 1))\n    x = self.conv_1(x)\n    x = x.permute((0, 2, 1))\n    x = self.act(x)\n    x = self.dropout(x)\n\n    # Forward pass through the second convolution layer and dropout layer\n    x = x.permute((0, 2, 1))\n    x = self.conv_2(x)\n    x = x.permute((0, 2, 1))\n    x = self.dropout(x)\n\n    # Scale the output by 0.5 (this helps with training stability)\n    return 0.5 * x\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"models/tts/delightful_tts/attention/multi_head_attention/#models.tts.delightful_tts.attention.multi_head_attention.MultiHeadAttention","title":"<code>MultiHeadAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class that implements a Multi-head Attention mechanism. Multi-head attention allows the model to focus on different positions, capturing various aspects of the input.</p> <p>Parameters:</p> Name Type Description Default <code>query_dim</code> <code>int</code> <p>The dimensionality of the query.</p> required <code>key_dim</code> <code>int</code> <p>The dimensionality of the key.</p> required <code>num_units</code> <code>int</code> <p>The total number of dimensions of the output.</p> required <code>num_heads</code> <code>int</code> <p>The number of parallel attention layers (multi-heads).</p> required query, and key <ul> <li>query: Tensor of shape [N, T_q, query_dim]</li> <li>key: Tensor of shape [N, T_k, key_dim]</li> </ul> Outputs <ul> <li>An output tensor of shape [N, T_q, num_units]</li> </ul> Source code in <code>models/tts/delightful_tts/attention/multi_head_attention.py</code> <pre><code>class MultiHeadAttention(Module):\n    r\"\"\"A class that implements a Multi-head Attention mechanism.\n    Multi-head attention allows the model to focus on different positions,\n    capturing various aspects of the input.\n\n    Args:\n        query_dim (int): The dimensionality of the query.\n        key_dim (int): The dimensionality of the key.\n        num_units (int): The total number of dimensions of the output.\n        num_heads (int): The number of parallel attention layers (multi-heads).\n\n    Inputs: query, and key\n        - **query**: Tensor of shape [N, T_q, query_dim]\n        - **key**: Tensor of shape [N, T_k, key_dim]\n\n    Outputs:\n        - An output tensor of shape [N, T_q, num_units]\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim: int,\n        key_dim: int,\n        num_units: int,\n        num_heads: int,\n    ):\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(\n            in_features=query_dim,\n            out_features=num_units,\n            bias=False,\n        )\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(\n            in_features=key_dim, out_features=num_units, bias=False,\n        )\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Performs the forward pass over input tensors.\n\n        Args:\n            query (torch.Tensor): The input tensor containing query vectors.\n                It is expected to have the dimensions [N, T_q, query_dim]\n                where N is the batch size, T_q is the sequence length of queries,\n                and query_dim is the dimensionality of a single query vector.\n\n            key (torch.Tensor): The input tensor containing key vectors.\n                It is expected to have the dimensions [N, T_k, key_dim]\n                where N is the batch size, T_k is the sequence length of keys,\n                and key_dim is the dimensionality of a single key vector.\n\n        Returns:\n            torch.Tensor: The output tensor of shape [N, T_q, num_units] which\n                represents the results of the multi-head attention mechanism applied\n                on the provided queries and keys.\n        \"\"\"\n        querys = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key)  # [N, T_k, num_units]\n        values = self.W_value(key)\n        split_size = self.num_units // self.num_heads\n\n        querys = torch.stack(\n            torch.split(querys, split_size, dim=2), dim=0,\n        )  # [h, N, T_q, num_units/h]\n        keys = torch.stack(\n            torch.split(keys, split_size, dim=2), dim=0,\n        )  # [h, N, T_k, num_units/h]\n        values = torch.stack(\n            torch.split(values, split_size, dim=2), dim=0,\n        )  # [h, N, T_k, num_units/h]\n        # score = softmax(QK^T / (d_k ** 0.5))\n\n        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores = scores / (self.key_dim**0.5)\n        scores = F.softmax(scores, dim=3)\n        # out = score * V\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n        return torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(\n            0,\n        )  # [N, T_q, num_units]\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/multi_head_attention/#models.tts.delightful_tts.attention.multi_head_attention.MultiHeadAttention.forward","title":"<code>forward(query, key)</code>","text":"<p>Performs the forward pass over input tensors.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>The input tensor containing query vectors. It is expected to have the dimensions [N, T_q, query_dim] where N is the batch size, T_q is the sequence length of queries, and query_dim is the dimensionality of a single query vector.</p> required <code>key</code> <code>Tensor</code> <p>The input tensor containing key vectors. It is expected to have the dimensions [N, T_k, key_dim] where N is the batch size, T_k is the sequence length of keys, and key_dim is the dimensionality of a single key vector.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor of shape [N, T_q, num_units] which represents the results of the multi-head attention mechanism applied on the provided queries and keys.</p> Source code in <code>models/tts/delightful_tts/attention/multi_head_attention.py</code> <pre><code>def forward(self, query: torch.Tensor, key: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Performs the forward pass over input tensors.\n\n    Args:\n        query (torch.Tensor): The input tensor containing query vectors.\n            It is expected to have the dimensions [N, T_q, query_dim]\n            where N is the batch size, T_q is the sequence length of queries,\n            and query_dim is the dimensionality of a single query vector.\n\n        key (torch.Tensor): The input tensor containing key vectors.\n            It is expected to have the dimensions [N, T_k, key_dim]\n            where N is the batch size, T_k is the sequence length of keys,\n            and key_dim is the dimensionality of a single key vector.\n\n    Returns:\n        torch.Tensor: The output tensor of shape [N, T_q, num_units] which\n            represents the results of the multi-head attention mechanism applied\n            on the provided queries and keys.\n    \"\"\"\n    querys = self.W_query(query)  # [N, T_q, num_units]\n    keys = self.W_key(key)  # [N, T_k, num_units]\n    values = self.W_value(key)\n    split_size = self.num_units // self.num_heads\n\n    querys = torch.stack(\n        torch.split(querys, split_size, dim=2), dim=0,\n    )  # [h, N, T_q, num_units/h]\n    keys = torch.stack(\n        torch.split(keys, split_size, dim=2), dim=0,\n    )  # [h, N, T_k, num_units/h]\n    values = torch.stack(\n        torch.split(values, split_size, dim=2), dim=0,\n    )  # [h, N, T_k, num_units/h]\n    # score = softmax(QK^T / (d_k ** 0.5))\n\n    scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n    scores = scores / (self.key_dim**0.5)\n    scores = F.softmax(scores, dim=3)\n    # out = score * V\n    out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n    return torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(\n        0,\n    )  # [N, T_q, num_units]\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/readme/","title":"References","text":""},{"location":"models/tts/delightful_tts/attention/readme/#references","title":"References","text":""},{"location":"models/tts/delightful_tts/attention/readme/#style-embed-attention","title":"Style Embed Attention","text":"<p>This mechanism is being used to extract style features from audio data in the form of spectrograms.</p> <p>This technique is often used in text-to-speech synthesis (TTS) such as Tacotron-2, where the goal is to modulate the prosody, stress, and intonation of the synthesized speech based on the reference audio or some control parameters. The concept of \"global style tokens\" (GST) was introduced in </p> <p>Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis by Yuxuan Wang et al.</p>"},{"location":"models/tts/delightful_tts/attention/readme/#multi-head-attention","title":"Multi-Head Attention","text":"<p>I found great explanations with code implementation of Multi-Headed Attention (MHA) by labml.ai Deep Learning Paper Implementations.</p> <p>This is a tutorial/implementation of multi-headed attention from paper Attention Is All You Need in PyTorch. The implementation is inspired from Annotated Transformer.</p> <p>This computes scaled multi-headed attention for given <code>query</code>, <code>key</code> and <code>value</code> vectors.</p> \\[\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V\\] <p>In simple terms, it finds keys that matches the query, and gets the values of     those keys.</p> <p>It uses dot-product of query and key as the indicator of how matching they are. Before taking the \\(softmax\\) the dot-products are scaled by \\(\\frac{1}{\\sqrt{d_k}}\\). This is done to avoid large dot-product values causing softmax to give very small gradients when \\(d_k\\) is large.</p> <p>Softmax is calculated along the axis of of the sequence (or time).</p>"},{"location":"models/tts/delightful_tts/attention/readme/#relative-multi-head-attention","title":"Relative Multi-Head Attention","text":"<p>Explanations with code implementation of Relative Multi-Headed Attention by labml.ai Deep Learning Paper Implementations.</p> <p>Paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context in PyTorch</p>"},{"location":"models/tts/delightful_tts/attention/readme/#conformer-multi-headed-self-attention","title":"Conformer Multi-Headed Self Attention","text":"<p>Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL, the relative sinusoidal positional encoding scheme.</p>"},{"location":"models/tts/delightful_tts/attention/readme/#feed-forward","title":"Feed Forward","text":"<p>Creates a feed-forward neural network. The network includes a layer normalization, an activation function (<code>LeakyReLU</code>), and dropout layers.</p>"},{"location":"models/tts/delightful_tts/attention/readme/#conformer-conv-module","title":"Conformer Conv Module","text":"<p>Conformer Convolution Module class represents a module in the Conformer model architecture. The module includes a layer normalization, pointwise and depthwise convolutional layers, Gated Linear Units (GLU) activation, and dropout layer.</p>"},{"location":"models/tts/delightful_tts/attention/readme/#conformer-block","title":"Conformer Block","text":"<p><code>ConformerBlock</code> class represents a block in the Conformer model architecture. The block includes a pointwise convolution followed by Gated Linear Units (<code>GLU</code>) activation layer (<code>Conv1dGLU</code>), a Conformer self attention layer (<code>ConformerMultiHeadedSelfAttention</code>), and optional feed-forward layer (<code>FeedForward</code>).</p>"},{"location":"models/tts/delightful_tts/attention/readme/#conformer","title":"Conformer","text":"<p><code>Conformer</code> class represents the <code>Conformer</code> model which is a sequence-to-sequence model used in some modern automated speech recognition systems. It is composed of several <code>ConformerBlocks</code>.</p>"},{"location":"models/tts/delightful_tts/attention/relative_multi_head_attention/","title":"Relative Multi-Head Attention","text":""},{"location":"models/tts/delightful_tts/attention/relative_multi_head_attention/#models.tts.delightful_tts.attention.relative_multi_head_attention.RelativeMultiHeadAttention","title":"<code>RelativeMultiHeadAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Multi-head attention with relative positional encoding. This concept was proposed in the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimension of model</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>The number of attention heads.</p> <code>16</code> query, key, value, pos_embedding, mask <ul> <li>query (batch, time, dim): Tensor containing query vector</li> <li>key (batch, time, dim): Tensor containing key vector</li> <li>value (batch, time, dim): Tensor containing value vector</li> <li>pos_embedding (batch, time, dim): Positional embedding tensor</li> <li>mask (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked</li> </ul> <p>Returns:     - outputs: Tensor produces by relative multi head attention module.</p> <p>Note: <code>d_model</code> should be divisible by <code>num_heads</code> in other words <code>d_model % num_heads</code> should be zero.</p> Source code in <code>models/tts/delightful_tts/attention/relative_multi_head_attention.py</code> <pre><code>class RelativeMultiHeadAttention(Module):\n    r\"\"\"Multi-head attention with relative positional encoding.\n    This concept was proposed in the\n    [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.\n\n    Inputs: query, key, value, pos_embedding, mask\n        - **query** (batch, time, dim): Tensor containing query vector\n        - **key** (batch, time, dim): Tensor containing key vector\n        - **value** (batch, time, dim): Tensor containing value vector\n        - **pos_embedding** (batch, time, dim): Positional embedding tensor\n        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n    Returns:\n        - **outputs**: Tensor produces by relative multi head attention module.\n\n    Note: `d_model` should be divisible by `num_heads` in other words `d_model % num_heads` should be zero.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 512,\n        num_heads: int = 16,\n    ):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n        self.d_model = d_model\n        self.d_head = int(d_model / num_heads)\n        self.num_heads = num_heads\n        self.sqrt_dim = math.sqrt(d_model)\n\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.pos_proj = nn.Linear(d_model, d_model, bias=False)\n\n        self.u_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n        self.v_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n\n        torch.nn.init.xavier_uniform_(self.u_bias)\n        torch.nn.init.xavier_uniform_(self.v_bias)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        pos_embedding: torch.Tensor,\n        mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Function applies multi-head attention along with relative positional encoding to the inputs. It restructures the input queries, keys, and values according to individual attention heads, applies biases, calculates content and position scores, and combines these to get the final score. A softmax activation is applied over the final score, followed by the calculation of context (contextual representation of input).\n\n        Performs the forward pass on the queries, keys, values, and positional embeddings with a mask.\n\n        Args:\n            query (torch.Tensor): The input tensor containing query vectors.\n            key (torch.Tensor): The input tensor containing key vectors.\n            value (torch.Tensor): The input tensor containing value vectors.\n            pos_embedding (torch.Tensor): The positional embedding tensor.\n            mask (torch.Tensor): The mask tensor containing indices to be masked.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: The context and attention tensors.\n            Tensor produces by relative multi head attention module.\n        \"\"\"\n        batch_size = query.shape[0]\n        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)\n        key = (\n            self.key_proj(key)\n            .view(batch_size, -1, self.num_heads, self.d_head)\n            .permute(0, 2, 1, 3)\n        )\n        value = (\n            self.value_proj(value)\n            .view(batch_size, -1, self.num_heads, self.d_head)\n            .permute(0, 2, 1, 3)\n        )\n        pos_embedding = self.pos_proj(pos_embedding).view(\n            batch_size, -1, self.num_heads, self.d_head,\n        )\n        u_bias = self.u_bias.expand_as(query)\n        v_bias = self.v_bias.expand_as(query)\n        a = (query + u_bias).transpose(1, 2)\n        content_score = a @ key.transpose(2, 3)\n        b = (query + v_bias).transpose(1, 2)\n        pos_score = b @ pos_embedding.permute(0, 2, 3, 1)\n        pos_score = self._relative_shift(pos_score)\n\n        score = content_score + pos_score\n        score = score * (1.0 / self.sqrt_dim)\n\n        score.masked_fill_(mask, -1e9)\n\n        attn = F.softmax(score, -1)\n\n        context = (attn @ value).transpose(1, 2)\n        context = context.contiguous().view(batch_size, -1, self.d_model)\n\n        return self.out_proj(context), attn\n\n    def _relative_shift(self, pos_score: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"The main idea of relative positional encoding is that the attention score doesn't only depend on the query and the key, but also on the relative position of the key with respect to the query. This becomes particularly useful when working with sequences of tokens, like in NLP tasks, as it helps the model to be aware of the position of the words (or tokens) in the sentence.\n\n        Performs the relative shift operation on the positional scores.\n\n        Args:\n            pos_score (torch.Tensor): The positional scores tensor.\n\n        Returns:\n            torch.Tensor: The shifted positional scores tensor.\n        \"\"\"\n        batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n        zeros = torch.zeros(\n            (batch_size, num_heads, seq_length1, 1), device=pos_score.device,\n        )\n        padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n        padded_pos_score = padded_pos_score.view(\n            batch_size, num_heads, seq_length2 + 1, seq_length1,\n        )\n        return padded_pos_score[:, :, 1:].view_as(pos_score)\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/relative_multi_head_attention/#models.tts.delightful_tts.attention.relative_multi_head_attention.RelativeMultiHeadAttention.forward","title":"<code>forward(query, key, value, pos_embedding, mask)</code>","text":"<p>Function applies multi-head attention along with relative positional encoding to the inputs. It restructures the input queries, keys, and values according to individual attention heads, applies biases, calculates content and position scores, and combines these to get the final score. A softmax activation is applied over the final score, followed by the calculation of context (contextual representation of input).</p> <p>Performs the forward pass on the queries, keys, values, and positional embeddings with a mask.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>The input tensor containing query vectors.</p> required <code>key</code> <code>Tensor</code> <p>The input tensor containing key vectors.</p> required <code>value</code> <code>Tensor</code> <p>The input tensor containing value vectors.</p> required <code>pos_embedding</code> <code>Tensor</code> <p>The positional embedding tensor.</p> required <code>mask</code> <code>Tensor</code> <p>The mask tensor containing indices to be masked.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple[torch.Tensor, torch.Tensor]: The context and attention tensors.</p> <code>Tensor</code> <p>Tensor produces by relative multi head attention module.</p> Source code in <code>models/tts/delightful_tts/attention/relative_multi_head_attention.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    pos_embedding: torch.Tensor,\n    mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Function applies multi-head attention along with relative positional encoding to the inputs. It restructures the input queries, keys, and values according to individual attention heads, applies biases, calculates content and position scores, and combines these to get the final score. A softmax activation is applied over the final score, followed by the calculation of context (contextual representation of input).\n\n    Performs the forward pass on the queries, keys, values, and positional embeddings with a mask.\n\n    Args:\n        query (torch.Tensor): The input tensor containing query vectors.\n        key (torch.Tensor): The input tensor containing key vectors.\n        value (torch.Tensor): The input tensor containing value vectors.\n        pos_embedding (torch.Tensor): The positional embedding tensor.\n        mask (torch.Tensor): The mask tensor containing indices to be masked.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: The context and attention tensors.\n        Tensor produces by relative multi head attention module.\n    \"\"\"\n    batch_size = query.shape[0]\n    query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)\n    key = (\n        self.key_proj(key)\n        .view(batch_size, -1, self.num_heads, self.d_head)\n        .permute(0, 2, 1, 3)\n    )\n    value = (\n        self.value_proj(value)\n        .view(batch_size, -1, self.num_heads, self.d_head)\n        .permute(0, 2, 1, 3)\n    )\n    pos_embedding = self.pos_proj(pos_embedding).view(\n        batch_size, -1, self.num_heads, self.d_head,\n    )\n    u_bias = self.u_bias.expand_as(query)\n    v_bias = self.v_bias.expand_as(query)\n    a = (query + u_bias).transpose(1, 2)\n    content_score = a @ key.transpose(2, 3)\n    b = (query + v_bias).transpose(1, 2)\n    pos_score = b @ pos_embedding.permute(0, 2, 3, 1)\n    pos_score = self._relative_shift(pos_score)\n\n    score = content_score + pos_score\n    score = score * (1.0 / self.sqrt_dim)\n\n    score.masked_fill_(mask, -1e9)\n\n    attn = F.softmax(score, -1)\n\n    context = (attn @ value).transpose(1, 2)\n    context = context.contiguous().view(batch_size, -1, self.d_model)\n\n    return self.out_proj(context), attn\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/style_embed_attention/","title":"Style Embed Attention","text":""},{"location":"models/tts/delightful_tts/attention/style_embed_attention/#models.tts.delightful_tts.attention.style_embed_attention.StyleEmbedAttention","title":"<code>StyleEmbedAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Mechanism is being used to extract style features from audio data in the form of spectrograms.</p> <p>Each style token (parameterized by an embedding vector) represents a unique style feature. The model applies the <code>StyleEmbedAttention</code> mechanism to combine these style tokens (style features) in a weighted manner. The output of the attention module is a sum of style tokens, with each token weighted by its relevance to the input.</p> <p>This technique is often used in text-to-speech synthesis (TTS) such as Tacotron-2, where the goal is to modulate the prosody, stress, and intonation of the synthesized speech based on the reference audio or some control parameters. The concept of \"global style tokens\" (GST) was introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis by Yuxuan Wang et al.</p> <p>The <code>StyleEmbedAttention</code> class is a PyTorch module implementing the attention mechanism. This class is specifically designed for handling multiple attention heads. Attention here operates on a query and a set of key-value pairs to produce an output.</p> <p>Builds the <code>StyleEmbedAttention</code> network.</p> <p>Parameters:</p> Name Type Description Default <code>query_dim</code> <code>int</code> <p>Dimensionality of the query vectors.</p> required <code>key_dim</code> <code>int</code> <p>Dimensionality of the key vectors.</p> required <code>num_units</code> <code>int</code> <p>Total dimensionality of the query, key, and value vectors.</p> required <code>num_heads</code> <code>int</code> <p>Number of parallel attention layers (heads).</p> required <p>Note: <code>num_units</code> should be divisible by <code>num_heads</code>.</p> Source code in <code>models/tts/delightful_tts/attention/style_embed_attention.py</code> <pre><code>class StyleEmbedAttention(Module):\n    r\"\"\"Mechanism is being used to extract style features from audio data in the form of spectrograms.\n\n    Each style token (parameterized by an embedding vector) represents a unique style feature. The model applies the `StyleEmbedAttention` mechanism to combine these style tokens (style features) in a weighted manner. The output of the attention module is a sum of style tokens, with each token weighted by its relevance to the input.\n\n    This technique is often used in text-to-speech synthesis (TTS) such as Tacotron-2, where the goal is to modulate the prosody, stress, and intonation of the synthesized speech based on the reference audio or some control parameters. The concept of \"global style tokens\" (GST) was introduced in\n    [Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis](https://arxiv.org/abs/1803.09017) by Yuxuan Wang et al.\n\n    The `StyleEmbedAttention` class is a PyTorch module implementing the attention mechanism.\n    This class is specifically designed for handling multiple attention heads.\n    Attention here operates on a query and a set of key-value pairs to produce an output.\n\n    Builds the `StyleEmbedAttention` network.\n\n    Args:\n        query_dim (int): Dimensionality of the query vectors.\n        key_dim (int): Dimensionality of the key vectors.\n        num_units (int): Total dimensionality of the query, key, and value vectors.\n        num_heads (int): Number of parallel attention layers (heads).\n\n    Note: `num_units` should be divisible by `num_heads`.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim: int,\n        key_dim: int,\n        num_units: int,\n        num_heads: int,\n    ):\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(\n            in_features=query_dim,\n            out_features=num_units,\n            bias=False,\n        )\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(\n            in_features=key_dim, out_features=num_units, bias=False,\n        )\n\n    def forward(self, query: torch.Tensor, key_soft: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the StyleEmbedAttention module calculates the attention scores.\n\n        Args:\n            query (torch.Tensor): The input tensor for queries of shape `[N, T_q, query_dim]`\n            key_soft (torch.Tensor): The input tensor for keys of shape `[N, T_k, key_dim]`\n\n        Returns:\n            out (torch.Tensor): The output tensor of shape `[N, T_q, num_units]`\n        \"\"\"\n        values = self.W_value(key_soft)\n        split_size = self.num_units // self.num_heads\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)\n\n        # out_soft = scores_soft = None\n        queries = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key_soft)  # [N, T_k, num_units]\n\n        # [h, N, T_q, num_units/h]\n        queries = torch.stack(torch.split(queries, split_size, dim=2), dim=0)\n        # [h, N, T_k, num_units/h]\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)\n        # [h, N, T_k, num_units/h]\n\n        # score = softmax(QK^T / (d_k ** 0.5))\n        scores_soft = torch.matmul(queries, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores_soft = scores_soft / (self.key_dim**0.5)\n        scores_soft = F.softmax(scores_soft, dim=3)\n\n        # out = score * V\n        # [h, N, T_q, num_units/h]\n        out_soft = torch.matmul(scores_soft, values)\n        return torch.cat(torch.split(out_soft, 1, dim=0), dim=3).squeeze(\n            0,\n        )  # [N, T_q, num_units] scores_soft\n</code></pre>"},{"location":"models/tts/delightful_tts/attention/style_embed_attention/#models.tts.delightful_tts.attention.style_embed_attention.StyleEmbedAttention.forward","title":"<code>forward(query, key_soft)</code>","text":"<p>Forward pass of the StyleEmbedAttention module calculates the attention scores.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>The input tensor for queries of shape <code>[N, T_q, query_dim]</code></p> required <code>key_soft</code> <code>Tensor</code> <p>The input tensor for keys of shape <code>[N, T_k, key_dim]</code></p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <p>The output tensor of shape <code>[N, T_q, num_units]</code></p> Source code in <code>models/tts/delightful_tts/attention/style_embed_attention.py</code> <pre><code>def forward(self, query: torch.Tensor, key_soft: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the StyleEmbedAttention module calculates the attention scores.\n\n    Args:\n        query (torch.Tensor): The input tensor for queries of shape `[N, T_q, query_dim]`\n        key_soft (torch.Tensor): The input tensor for keys of shape `[N, T_k, key_dim]`\n\n    Returns:\n        out (torch.Tensor): The output tensor of shape `[N, T_q, num_units]`\n    \"\"\"\n    values = self.W_value(key_soft)\n    split_size = self.num_units // self.num_heads\n    values = torch.stack(torch.split(values, split_size, dim=2), dim=0)\n\n    # out_soft = scores_soft = None\n    queries = self.W_query(query)  # [N, T_q, num_units]\n    keys = self.W_key(key_soft)  # [N, T_k, num_units]\n\n    # [h, N, T_q, num_units/h]\n    queries = torch.stack(torch.split(queries, split_size, dim=2), dim=0)\n    # [h, N, T_k, num_units/h]\n    keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)\n    # [h, N, T_k, num_units/h]\n\n    # score = softmax(QK^T / (d_k ** 0.5))\n    scores_soft = torch.matmul(queries, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n    scores_soft = scores_soft / (self.key_dim**0.5)\n    scores_soft = F.softmax(scores_soft, dim=3)\n\n    # out = score * V\n    # [h, N, T_q, num_units/h]\n    out_soft = torch.matmul(scores_soft, values)\n    return torch.cat(torch.split(out_soft, 1, dim=0), dim=3).squeeze(\n        0,\n    )  # [N, T_q, num_units] scores_soft\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/activation/","title":"Activation","text":""},{"location":"models/tts/delightful_tts/conv_blocks/activation/#models.tts.delightful_tts.conv_blocks.activation.GLUActivation","title":"<code>GLUActivation</code>","text":"<p>             Bases: <code>Module</code></p> <p>Implements the Gated Linear Unit (GLU) activation function.</p> <p>The GLU activation splits the input in half across the channel dimension. One half is passed through a nonlinear activation function (like sigmoid or leaky ReLU), and the output from this activation function is used as a gate to control the amplitude of the other half of the input. An element-wise multiplication is then performed between the gating signal and the other half of the input.</p> <p>The GLU activation allows the model to dynamically choose which inputs to pass through and what information to suppress, which can help improving the model performance on certain tasks.</p> <p>Parameters:</p> Name Type Description Default <code>slope</code> <code>float</code> <p>Controls the slope for the leaky ReLU activation function. Default: 0.3 or see the const <code>LEAKY_RELU_SLOPE</code></p> <code>LEAKY_RELU_SLOPE</code> Shape <ul> <li>Input: (N, 2*C, L) where C is the number of input channels.</li> <li>Output: (N, C, L)</li> </ul> <p>Examples: <pre><code>m = GLUActivation(0.3)\ninput = torch.randn(16, 2*20, 44)\noutput = m(input)\n</code></pre></p> Source code in <code>models/tts/delightful_tts/conv_blocks/activation.py</code> <pre><code>class GLUActivation(Module):\n    r\"\"\"Implements the Gated Linear Unit (GLU) activation function.\n\n    The GLU activation splits the input in half across the channel dimension.\n    One half is passed through a nonlinear activation function (like sigmoid or leaky ReLU),\n    and the output from this activation function is used as a gate to control the\n    amplitude of the other half of the input. An element-wise multiplication is then performed\n    between the gating signal and the other half of the input.\n\n    The GLU activation allows the model to dynamically choose which inputs to pass through and\n    what information to suppress, which can help improving the model performance on certain tasks.\n\n    Args:\n        slope: Controls the slope for the leaky ReLU activation function. Default: 0.3 or see the const `LEAKY_RELU_SLOPE`\n\n    Shape:\n        - Input: (N, 2*C, L) where C is the number of input channels.\n        - Output: (N, C, L)\n\n    Examples:\n    ```python\n    m = GLUActivation(0.3)\n    input = torch.randn(16, 2*20, 44)\n    output = m(input)\n    ```\n\n    \"\"\"\n\n    def __init__(self, slope: float = LEAKY_RELU_SLOPE):\n        super().__init__()\n        self.lrelu = nn.LeakyReLU(slope)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Defines the computation performed at every call.\n\n        Args:\n            x: The input tensor of shape (batch_size, 2*channels, signal_length)\n\n        Returns:\n            x: The output tensor of shape (batch_size, channels, signal_length)\n        \"\"\"\n        # Split the input into two equal parts (chunks) along dimension 1\n        out, gate = x.chunk(2, dim=1)\n\n        # Perform element-wise multiplication of the first half (out)\n        # with the result of applying LeakyReLU on the second half (gate)\n        return out * self.lrelu(gate)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/activation/#models.tts.delightful_tts.conv_blocks.activation.GLUActivation.forward","title":"<code>forward(x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, 2*channels, signal_length)</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>The output tensor of shape (batch_size, channels, signal_length)</p> Source code in <code>models/tts/delightful_tts/conv_blocks/activation.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Defines the computation performed at every call.\n\n    Args:\n        x: The input tensor of shape (batch_size, 2*channels, signal_length)\n\n    Returns:\n        x: The output tensor of shape (batch_size, channels, signal_length)\n    \"\"\"\n    # Split the input into two equal parts (chunks) along dimension 1\n    out, gate = x.chunk(2, dim=1)\n\n    # Perform element-wise multiplication of the first half (out)\n    # with the result of applying LeakyReLU on the second half (gate)\n    return out * self.lrelu(gate)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/add_coords/","title":"AddCoords","text":""},{"location":"models/tts/delightful_tts/conv_blocks/add_coords/#models.tts.delightful_tts.conv_blocks.add_coords.AddCoords","title":"<code>AddCoords</code>","text":"<p>             Bases: <code>Module</code></p> <p>AddCoords is a PyTorch module that adds additional channels to the input tensor containing the relative (normalized to <code>[-1, 1]</code>) coordinates of each input element along the specified number of dimensions (<code>rank</code>). Essentially, it adds spatial context information to the tensor.</p> <p>Typically, these inputs are feature maps coming from some CNN, where the spatial organization of the input matters (such as an image or speech signal).</p> <p>This additional spatial context allows subsequent layers (such as convolutions) to learn position-dependent features. For example, in tasks where the absolute position of features matters (such as denoising and segmentation tasks), it helps the model to know where (in terms of relative position) the features are.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The dimensionality of the input tensor. That is to say, this tells us how many dimensions the         input tensor's spatial context has. It's assumed to be 1, 2, or 3 corresponding to some 1D, 2D,         or 3D data (like an image).</p> required <code>with_r</code> <code>bool</code> <p>Boolean indicating whether to add an extra radial distance channel or not. If True, an extra            channel is appended, which measures the Euclidean (L2) distance from the center of the image.            This might be useful when the proximity to the center of the image is important to the task.</p> <code>False</code> Source code in <code>models/tts/delightful_tts/conv_blocks/add_coords.py</code> <pre><code>class AddCoords(Module):\n    r\"\"\"AddCoords is a PyTorch module that adds additional channels to the input tensor containing the relative\n    (normalized to `[-1, 1]`) coordinates of each input element along the specified number of dimensions (`rank`).\n    Essentially, it adds spatial context information to the tensor.\n\n    Typically, these inputs are feature maps coming from some CNN, where the spatial organization of the input\n    matters (such as an image or speech signal).\n\n    This additional spatial context allows subsequent layers (such as convolutions) to learn position-dependent\n    features. For example, in tasks where the absolute position of features matters (such as denoising and\n    segmentation tasks), it helps the model to know where (in terms of relative position) the features are.\n\n    Args:\n        rank (int): The dimensionality of the input tensor. That is to say, this tells us how many dimensions the\n                    input tensor's spatial context has. It's assumed to be 1, 2, or 3 corresponding to some 1D, 2D,\n                    or 3D data (like an image).\n\n        with_r (bool): Boolean indicating whether to add an extra radial distance channel or not. If True, an extra\n                       channel is appended, which measures the Euclidean (L2) distance from the center of the image.\n                       This might be useful when the proximity to the center of the image is important to the task.\n    \"\"\"\n\n    def __init__(self, rank: int, with_r: bool = False):\n        super().__init__()\n        self.rank = rank\n        self.with_r = with_r\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the AddCoords module. Depending on the rank of the tensor, it adds one or more new channels\n        with relative coordinate values. If `with_r` is True, an extra radial channel is included.\n\n        For example, for an image (`rank=2`), two channels would be added which contain the normalized x and y\n        coordinates respectively of each pixel.\n\n        Calling the forward method updates the original tensor `x` with the added channels.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            out (torch.Tensor): The input tensor with added coordinate and possibly radial channels.\n        \"\"\"\n        if self.rank == 1:\n            batch_size_shape, _, dim_x = x.shape\n            xx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n            xx_channel = xx_range[None, None, :]\n\n            xx_channel = xx_channel.float() / (dim_x - 1)\n            xx_channel = xx_channel * 2 - 1\n            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1)\n\n            out = torch.cat([x, xx_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2))\n                out = torch.cat([out, rr], dim=1)\n\n        elif self.rank == 2:\n            batch_size_shape, _, dim_y, dim_x = x.shape\n            xx_ones = torch.ones([1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n            yy_ones = torch.ones([1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n\n            xx_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n            yy_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n            xx_range = xx_range[None, None, :, None]\n            yy_range = yy_range[None, None, :, None]\n\n            xx_channel = torch.matmul(xx_range, xx_ones)\n            yy_channel = torch.matmul(yy_range, yy_ones)\n\n            # transpose y\n            yy_channel = yy_channel.permute(0, 1, 3, 2)\n\n            xx_channel = xx_channel.float() / (dim_y - 1)\n            yy_channel = yy_channel.float() / (dim_x - 1)\n\n            xx_channel = xx_channel * 2 - 1\n            yy_channel = yy_channel * 2 - 1\n\n            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1)\n            yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1)\n\n            out = torch.cat([x, xx_channel, yy_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(\n                    torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2),\n                )\n                out = torch.cat([out, rr], dim=1)\n\n        elif self.rank == 3:\n            batch_size_shape, _, dim_z, dim_y, dim_x = x.shape\n            xx_ones = torch.ones([1, 1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n            yy_ones = torch.ones([1, 1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n            zz_ones = torch.ones([1, 1, 1, 1, dim_z], dtype=torch.int32, device=x.device)\n\n            xy_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n            xy_range = xy_range[None, None, None, :, None]\n\n            yz_range = torch.arange(dim_z, dtype=torch.int32, device=x.device)\n            yz_range = yz_range[None, None, None, :, None]\n\n            zx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n            zx_range = zx_range[None, None, None, :, None]\n\n            xy_channel = torch.matmul(xy_range, xx_ones)\n            xx_channel = torch.cat([xy_channel + i for i in range(dim_z)], dim=2)\n\n            yz_channel = torch.matmul(yz_range, yy_ones)\n            yz_channel = yz_channel.permute(0, 1, 3, 4, 2)\n            yy_channel = torch.cat([yz_channel + i for i in range(dim_x)], dim=4)\n\n            zx_channel = torch.matmul(zx_range, zz_ones)\n            zx_channel = zx_channel.permute(0, 1, 4, 2, 3)\n            zz_channel = torch.cat([zx_channel + i for i in range(dim_y)], dim=3)\n\n            out = torch.cat([x, xx_channel, yy_channel, zz_channel], dim=1)\n\n            if self.with_r:\n                rr = torch.sqrt(\n                    torch.pow(xx_channel - 0.5, 2)\n                    + torch.pow(yy_channel - 0.5, 2)\n                    + torch.pow(zz_channel - 0.5, 2),\n                )\n                out = torch.cat([out, rr], dim=1)\n        else:\n            raise NotImplementedError\n\n        return out\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/add_coords/#models.tts.delightful_tts.conv_blocks.add_coords.AddCoords.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the AddCoords module. Depending on the rank of the tensor, it adds one or more new channels with relative coordinate values. If <code>with_r</code> is True, an extra radial channel is included.</p> <p>For example, for an image (<code>rank=2</code>), two channels would be added which contain the normalized x and y coordinates respectively of each pixel.</p> <p>Calling the forward method updates the original tensor <code>x</code> with the added channels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <p>The input tensor with added coordinate and possibly radial channels.</p> Source code in <code>models/tts/delightful_tts/conv_blocks/add_coords.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the AddCoords module. Depending on the rank of the tensor, it adds one or more new channels\n    with relative coordinate values. If `with_r` is True, an extra radial channel is included.\n\n    For example, for an image (`rank=2`), two channels would be added which contain the normalized x and y\n    coordinates respectively of each pixel.\n\n    Calling the forward method updates the original tensor `x` with the added channels.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        out (torch.Tensor): The input tensor with added coordinate and possibly radial channels.\n    \"\"\"\n    if self.rank == 1:\n        batch_size_shape, _, dim_x = x.shape\n        xx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n        xx_channel = xx_range[None, None, :]\n\n        xx_channel = xx_channel.float() / (dim_x - 1)\n        xx_channel = xx_channel * 2 - 1\n        xx_channel = xx_channel.repeat(batch_size_shape, 1, 1)\n\n        out = torch.cat([x, xx_channel], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2))\n            out = torch.cat([out, rr], dim=1)\n\n    elif self.rank == 2:\n        batch_size_shape, _, dim_y, dim_x = x.shape\n        xx_ones = torch.ones([1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n        yy_ones = torch.ones([1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n\n        xx_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n        yy_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n        xx_range = xx_range[None, None, :, None]\n        yy_range = yy_range[None, None, :, None]\n\n        xx_channel = torch.matmul(xx_range, xx_ones)\n        yy_channel = torch.matmul(yy_range, yy_ones)\n\n        # transpose y\n        yy_channel = yy_channel.permute(0, 1, 3, 2)\n\n        xx_channel = xx_channel.float() / (dim_y - 1)\n        yy_channel = yy_channel.float() / (dim_x - 1)\n\n        xx_channel = xx_channel * 2 - 1\n        yy_channel = yy_channel * 2 - 1\n\n        xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1)\n        yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1)\n\n        out = torch.cat([x, xx_channel, yy_channel], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(\n                torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2),\n            )\n            out = torch.cat([out, rr], dim=1)\n\n    elif self.rank == 3:\n        batch_size_shape, _, dim_z, dim_y, dim_x = x.shape\n        xx_ones = torch.ones([1, 1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n        yy_ones = torch.ones([1, 1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n        zz_ones = torch.ones([1, 1, 1, 1, dim_z], dtype=torch.int32, device=x.device)\n\n        xy_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n        xy_range = xy_range[None, None, None, :, None]\n\n        yz_range = torch.arange(dim_z, dtype=torch.int32, device=x.device)\n        yz_range = yz_range[None, None, None, :, None]\n\n        zx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n        zx_range = zx_range[None, None, None, :, None]\n\n        xy_channel = torch.matmul(xy_range, xx_ones)\n        xx_channel = torch.cat([xy_channel + i for i in range(dim_z)], dim=2)\n\n        yz_channel = torch.matmul(yz_range, yy_ones)\n        yz_channel = yz_channel.permute(0, 1, 3, 4, 2)\n        yy_channel = torch.cat([yz_channel + i for i in range(dim_x)], dim=4)\n\n        zx_channel = torch.matmul(zx_range, zz_ones)\n        zx_channel = zx_channel.permute(0, 1, 4, 2, 3)\n        zz_channel = torch.cat([zx_channel + i for i in range(dim_y)], dim=3)\n\n        out = torch.cat([x, xx_channel, yy_channel, zz_channel], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(\n                torch.pow(xx_channel - 0.5, 2)\n                + torch.pow(yy_channel - 0.5, 2)\n                + torch.pow(zz_channel - 0.5, 2),\n            )\n            out = torch.cat([out, rr], dim=1)\n    else:\n        raise NotImplementedError\n\n    return out\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/bsconv/","title":"BSConv","text":""},{"location":"models/tts/delightful_tts/conv_blocks/bsconv/#models.tts.delightful_tts.conv_blocks.bsconv.BSConv1d","title":"<code>BSConv1d</code>","text":"<p>             Bases: <code>Module</code></p> <p><code>BSConv1d</code> implements the <code>BSConv</code> concept which is based on the paper BSConv: Binarized Separated Convolutional Neural Networks.</p> <p><code>BSConv</code> is an amalgamation of depthwise separable convolution and pointwise convolution. Depthwise separable convolution utilizes far fewer parameters by separating the spatial (depthwise) and channel-wise (pointwise) operations. Meanwhile, pointwise convolution helps in transforming the channel characteristics without considering the channel's context.</p> <p>Parameters:</p> Name Type Description Default <code>channels_in</code> <code>int</code> <p>Number of input channels</p> required <code>channels_out</code> <code>int</code> <p>Number of output channels produced by the convolution</p> required <code>kernel_size</code> <code>int</code> <p>Size of the kernel used in depthwise convolution</p> required <code>padding</code> <code>int</code> <p>Zeropadding added around the input tensor along the height and width directions</p> required <p>Attributes:</p> Name Type Description <code>pointwise</code> <code>PointwiseConv1d</code> <p>Pointwise convolution module</p> <code>depthwise</code> <code>DepthWiseConv1d</code> <p>Depthwise separable convolution module</p> Source code in <code>models/tts/delightful_tts/conv_blocks/bsconv.py</code> <pre><code>class BSConv1d(Module):\n    r\"\"\"`BSConv1d` implements the `BSConv` concept which is based on the paper [BSConv:\n    Binarized Separated Convolutional Neural Networks](https://arxiv.org/pdf/2003.13549.pdf).\n\n    `BSConv` is an amalgamation of depthwise separable convolution and pointwise convolution.\n    Depthwise separable convolution utilizes far fewer parameters by separating the spatial\n    (depthwise) and channel-wise (pointwise) operations. Meanwhile, pointwise convolution\n    helps in transforming the channel characteristics without considering the channel's context.\n\n    Args:\n        channels_in (int): Number of input channels\n        channels_out (int): Number of output channels produced by the convolution\n        kernel_size (int): Size of the kernel used in depthwise convolution\n        padding (int): Zeropadding added around the input tensor along the height and width directions\n\n    Attributes:\n        pointwise (PointwiseConv1d): Pointwise convolution module\n        depthwise (DepthWiseConv1d): Depthwise separable convolution module\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels_out: int,\n        kernel_size: int,\n        padding: int,\n    ):\n        super().__init__()\n\n        # Instantiate Pointwise Convolution Module:\n        # First operation in BSConv: the number of input channels is transformed to the number\n        # of output channels without taking into account the channel context.\n        self.pointwise = PointwiseConv1d(channels_in, channels_out)\n\n        # Instantiate Depthwise Convolution Module:\n        # Second operation in BSConv: A spatial convolution is performed independently over each output\n        # channel from the pointwise convolution.\n        self.depthwise = DepthWiseConv1d(\n            channels_out,\n            channels_out,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Propagate input tensor through pointwise convolution.\n        x1 = self.pointwise(x)\n\n        # Propagate the result of the previous pointwise convolution through the depthwise convolution.\n        # Return final output of the sequence of pointwise and depthwise convolutions\n        return self.depthwise(x1)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d/","title":"Conv1d","text":""},{"location":"models/tts/delightful_tts/conv_blocks/conv1d/#models.tts.delightful_tts.conv_blocks.conv1d.DepthWiseConv1d","title":"<code>DepthWiseConv1d</code>","text":"<p>             Bases: <code>Module</code></p> <p>Implements Depthwise 1D convolution. This module will apply a spatial convolution over inputs independently over each input channel in the style of depthwise convolutions.</p> <p>In a depthwise convolution, each input channel is convolved with its own set of filters, as opposed to standard convolutions where each input channel is convolved with all filters. At <code>groups=in_channels</code>, each input channel is convolved with its own set of filters. Filters in the DepthwiseConv1d are not shared among channels. This method can drastically reduce the number of parameters/learnable weights in the model, as each input channel gets its own filter.</p> <p>This technique is best suited to scenarios where the correlation between different channels is believed to be low. It is commonly employed in MobileNet models due to the reduced number of parameters, which is critical in mobile devices where computational resources are limited.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolving kernel</p> required <code>padding</code> <code>int</code> <p>Zero-padding added to both sides of the input</p> required Shape <ul> <li>Input: (N, C_in, L_in)</li> <li>Output: (N, C_out, L_out), where</li> </ul> <p><code>L_out = [L_in + 2*padding - (dilation*(kernel_size-1) + 1)]/stride + 1</code></p> <p>Attributes:</p> Name Type Description <code>weight</code> <code>Tensor</code> <p>the learnable weights of shape (<code>out_channels</code>, <code>in_channels</code>/<code>group</code>, <code>kernel_size</code>)</p> <code>bias</code> <code>Tensor</code> <p>the learnable bias of the module of shape (<code>out_channels</code>)</p> <p>Examples: <pre><code>m = DepthWiseConv1d(16, 33, 3, padding=1)\ninput = torch.randn(20, 16, 50)\noutput = m(input)\n</code></pre></p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d.py</code> <pre><code>class DepthWiseConv1d(Module):\n    r\"\"\"Implements Depthwise 1D convolution. This module will apply a spatial convolution over inputs\n    independently over each input channel in the style of depthwise convolutions.\n\n    In a depthwise convolution, each input channel is convolved with its own set of filters, as opposed\n    to standard convolutions where each input channel is convolved with all filters.\n    At `groups=in_channels`, each input channel is convolved with its own set of filters.\n    Filters in the\n    DepthwiseConv1d are not shared among channels. This method can drastically reduce the number of\n    parameters/learnable weights in the model, as each input channel gets its own filter.\n\n    This technique is best suited to scenarios where the correlation between different channels is\n    believed to be low. It is commonly employed in MobileNet models due to the reduced number of\n    parameters, which is critical in mobile devices where computational resources are limited.\n\n    Args:\n        in_channels (int): Number of channels in the input\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int): Size of the convolving kernel\n        padding (int): Zero-padding added to both sides of the input\n\n    Shape:\n        - Input: (N, C_in, L_in)\n        - Output: (N, C_out, L_out), where\n\n          `L_out = [L_in + 2*padding - (dilation*(kernel_size-1) + 1)]/stride + 1`\n\n    Attributes:\n        weight (Tensor): the learnable weights of shape (`out_channels`, `in_channels`/`group`, `kernel_size`)\n        bias (Tensor, optional): the learnable bias of the module of shape (`out_channels`)\n\n    Examples:\n    ```python\n    m = DepthWiseConv1d(16, 33, 3, padding=1)\n    input = torch.randn(20, 16, 50)\n    output = m(input)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        padding: int,\n    ):\n        super().__init__()\n\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=padding,\n            groups=in_channels,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Defines the computation performed at every call.\n\n        Args:\n            x: input tensor of shape (batch_size, in_channels, signal_length)\n\n        Returns:\n            output tensor of shape (batch_size, out_channels, signal_length)\n        \"\"\"\n        return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d/#models.tts.delightful_tts.conv_blocks.conv1d.DepthWiseConv1d.forward","title":"<code>forward(x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, in_channels, signal_length)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, out_channels, signal_length)</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Defines the computation performed at every call.\n\n    Args:\n        x: input tensor of shape (batch_size, in_channels, signal_length)\n\n    Returns:\n        output tensor of shape (batch_size, out_channels, signal_length)\n    \"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d/#models.tts.delightful_tts.conv_blocks.conv1d.PointwiseConv1d","title":"<code>PointwiseConv1d</code>","text":"<p>             Bases: <code>Module</code></p> <p>Applies a 1D pointwise (aka 1x1) convolution over an input signal composed of several input planes, officially known as channels in this context.</p> <p>The operation implemented is also known as a \"channel mixing\" operation, as each output channel can be seen as a linear combination of input channels.</p> <p>In the simplest case, the output value of the layer with input size (N, C_in, L) and output (N, C_out, L_out) can be precisely described as:</p> \\[out(N_i, C_{out_j}) = bias(C_{out_j}) +     weight(C_{out_j}, k) * input(N_i, k)\\] <p>where 'N' is a batch size, 'C' denotes a number of channels, 'L' is a length of signal sequence. The symbol '*' in the above indicates a 1D cross-correlation operation.</p> <p>The 1D cross correlation operation \"*\": Wikipedia Cross-correlation</p> <p>This module supports <code>TensorFloat32&lt;tf32_on_ampere&gt;</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution. Default: 1</p> <code>1</code> <code>padding</code> <code>int</code> <p>Zero-padding added to both sides of the input. Default: 0</p> <code>0</code> <code>bias</code> <code>bool</code> <p>If set to False, the layer will not learn an additive bias. Default: True</p> <code>True</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolving kernel. Default: 1</p> <code>1</code> Shape <ul> <li>Input: (N, C_in, L_in)</li> <li>Output: (N, C_out, L_out), where</li> </ul> <p>L_out = [L_in + 2padding - (dilation(kernel_size-1) + 1)]/stride + 1</p> <p>Attributes:</p> Name Type Description <code>weight</code> <code>Tensor</code> <p>the learnable weights of shape (out_channels, in_channels, kernel_size)</p> <code>bias</code> <code>Tensor</code> <p>the learnable bias of the module of shape (out_channels)</p> <p>Example: <pre><code>m = PointwiseConv1d(16, 33, 1, padding=0, bias=True)\ninput = torch.randn(20, 16, 50)\noutput = m(input)\n</code></pre></p> Description of parameters <p>stride (default 1): Controls the stride for the operation, which is the number of steps the convolutional kernel moves for each operation. A stride of 1 means that the kernel moves one step at a time and a stride of 2 means skipping every other step. Higher stride values can down sample the output and lead to smaller output shapes.</p> <p>padding (default 0): Controls the amount of padding applied to the input. By adding padding, the spatial size of the output can be controlled. If it is set to 0, no padding is applied. If it is set to 1, zero padding of one pixel width is added to the input data.</p> <p>bias (default True): Controls whether the layer uses a bias vector. By default, it is True, meaning that the layer has a learnable bias parameter.</p> <p>kernel_size (default 1): The size of the convolving kernel. In the case of 1D convolution, kernel_size is a single integer that specifies the number of elements the filter that convolves the input should have. In your PointwiseConv1d case, the default kernel size is 1, indicating a 1x1 convolution is applied which is commonly known as a pointwise convolution.</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d.py</code> <pre><code>class PointwiseConv1d(Module):\n    r\"\"\"Applies a 1D pointwise (aka 1x1) convolution over an input signal composed of several input\n    planes, officially known as channels in this context.\n\n    The operation implemented is also known as a \"channel mixing\" operation, as each output channel can be\n    seen as a linear combination of input channels.\n\n    In the simplest case, the output value of the layer with input size\n    (N, C_in, L) and output (N, C_out, L_out) can be\n    precisely described as:\n\n    $$out(N_i, C_{out_j}) = bias(C_{out_j}) +\n        weight(C_{out_j}, k) * input(N_i, k)$$\n\n    where 'N' is a batch size, 'C' denotes a number of channels,\n    'L' is a length of signal sequence.\n    The symbol '*' in the above indicates a 1D cross-correlation operation.\n\n    The 1D cross correlation operation \"*\": [Wikipedia Cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation)\n\n    This module supports `TensorFloat32&lt;tf32_on_ampere&gt;`.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input. Default: 0\n        bias (bool): If set to False, the layer will not learn an additive bias. Default: True\n        kernel_size (int): Size of the convolving kernel. Default: 1\n\n    Shape:\n        - Input: (N, C_in, L_in)\n        - Output: (N, C_out, L_out), where\n\n          L_out = [L_in + 2*padding - (dilation*(kernel_size-1) + 1)]/stride + 1\n\n    Attributes:\n        weight (Tensor): the learnable weights of shape (out_channels, in_channels, kernel_size)\n        bias (Tensor, optional): the learnable bias of the module of shape (out_channels)\n\n    Example:\n    ```python\n    m = PointwiseConv1d(16, 33, 1, padding=0, bias=True)\n    input = torch.randn(20, 16, 50)\n    output = m(input)\n    ```\n\n\n    Description of parameters:\n        stride (default 1): Controls the stride for the operation, which is the number of steps the convolutional\n        kernel moves for each operation. A stride of 1 means that the kernel moves one step at a time and a stride\n        of 2 means skipping every other step. Higher stride values can down sample the output and lead to smaller\n        output shapes.\n\n        padding (default 0): Controls the amount of padding applied to the input. By adding padding, the spatial\n        size of the output can be controlled. If it is set to 0, no padding is applied. If it is set to 1, zero\n        padding of one pixel width is added to the input data.\n\n        bias (default True): Controls whether the layer uses a bias vector. By default, it is True, meaning that\n        the layer has a learnable bias parameter.\n\n        kernel_size (default 1): The size of the convolving kernel. In the case of 1D convolution, kernel_size is\n        a single integer that specifies the number of elements the filter that convolves the input should have.\n        In your PointwiseConv1d case, the default kernel size is 1, indicating a 1x1 convolution is applied\n        which is commonly known as a pointwise convolution.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        padding: int = 0,\n        bias: bool = True,\n        kernel_size: int = 1,\n    ):\n        super().__init__()\n\n        self.conv = nn.Conv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): input tensor of shape (batch_size, in_channels, signal_length)\n\n        Returns:\n            output (torch.Tensor): tensor of shape (batch_size, out_channels, signal_length)\n        \"\"\"\n        return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d/#models.tts.delightful_tts.conv_blocks.conv1d.PointwiseConv1d.forward","title":"<code>forward(x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, in_channels, signal_length)</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>tensor of shape (batch_size, out_channels, signal_length)</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Defines the computation performed at every call.\n\n    Args:\n        x (torch.Tensor): input tensor of shape (batch_size, in_channels, signal_length)\n\n    Returns:\n        output (torch.Tensor): tensor of shape (batch_size, out_channels, signal_length)\n    \"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d_glu/","title":"Conv1dGLU","text":""},{"location":"models/tts/delightful_tts/conv_blocks/conv1d_glu/#models.tts.delightful_tts.conv_blocks.conv1d_glu.Conv1dGLU","title":"<code>Conv1dGLU</code>","text":"<p>             Bases: <code>Module</code></p> <p><code>Conv1dGLU</code> implements a variant of Convolutional Layer with a Gated Linear Unit (GLU). It's based on the Deep Voice 3 project.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>model dimension parameter.</p> required <code>kernel_size</code> <code>int</code> <p>kernel size for the convolution layer.</p> required <code>padding</code> <code>int</code> <p>padding size for the convolution layer.</p> required <code>embedding_dim</code> <code>int</code> <p>dimension of the embedding.</p> required <p>Attributes:</p> Name Type Description <code>bsconv1d</code> <code>BSConv1d) </code> <p>an instance of the Binarized Separated Convolution (1d)</p> <code>embedding_proj</code> <code>Linear</code> <p>linear transformation for embeddings.</p> <code>sqrt</code> <code>Tensor</code> <p>buffer that stores the square root of 0.5</p> <code>softsign</code> <code>SoftSign</code> <p>SoftSign Activation function</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d_glu.py</code> <pre><code>class Conv1dGLU(Module):\n    r\"\"\"`Conv1dGLU` implements a variant of Convolutional Layer with a Gated Linear Unit (GLU).\n    It's based on the Deep Voice 3 project.\n\n    Args:\n        d_model (int): model dimension parameter.\n        kernel_size (int): kernel size for the convolution layer.\n        padding (int): padding size for the convolution layer.\n        embedding_dim (int): dimension of the embedding.\n\n    Attributes:\n         bsconv1d (BSConv1d) : an instance of the Binarized Separated Convolution (1d)\n         embedding_proj (torch.nn.Modules.Linear): linear transformation for embeddings.\n         sqrt (torch.Tensor): buffer that stores the square root of 0.5\n         softsign (torch.nn.SoftSign): SoftSign Activation function\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        kernel_size: int,\n        padding: int,\n        embedding_dim: int,\n    ):\n        super().__init__()\n\n        self.bsconv1d = BSConv1d(\n            d_model,\n            2 * d_model,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n        self.embedding_proj = nn.Linear(\n            embedding_dim,\n            d_model,\n        )\n\n        self.register_buffer(\"sqrt\", torch.sqrt(torch.tensor([0.5])).squeeze(0))\n\n        self.softsign = torch.nn.Softsign()\n\n    def forward(self, x: torch.Tensor, embeddings: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward propagation method for the Conv1dGLU layer.\n\n        Args:\n            x (torch.Tensor): input tensor\n            embeddings (torch.Tensor): input embeddings\n\n        Returns:\n            x (torch.Tensor): output tensor after application of Conv1dGLU\n        \"\"\"\n        x = x.permute((0, 2, 1))\n        residual = x\n        x = self.bsconv1d(x)\n        splitdim = 1\n        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n        embeddings = self.embedding_proj(embeddings)\n        softsign = self.softsign(embeddings)\n        a = a + softsign.permute((0, 2, 1))\n        x = a * torch.sigmoid(b)\n        x = x + residual\n        x = x * self.sqrt\n        return x.permute((0, 2, 1))\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv1d_glu/#models.tts.delightful_tts.conv_blocks.conv1d_glu.Conv1dGLU.forward","title":"<code>forward(x, embeddings)</code>","text":"<p>Forward propagation method for the Conv1dGLU layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required <code>embeddings</code> <code>Tensor</code> <p>input embeddings</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>output tensor after application of Conv1dGLU</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv1d_glu.py</code> <pre><code>def forward(self, x: torch.Tensor, embeddings: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward propagation method for the Conv1dGLU layer.\n\n    Args:\n        x (torch.Tensor): input tensor\n        embeddings (torch.Tensor): input embeddings\n\n    Returns:\n        x (torch.Tensor): output tensor after application of Conv1dGLU\n    \"\"\"\n    x = x.permute((0, 2, 1))\n    residual = x\n    x = self.bsconv1d(x)\n    splitdim = 1\n    a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n    embeddings = self.embedding_proj(embeddings)\n    softsign = self.softsign(embeddings)\n    a = a + softsign.permute((0, 2, 1))\n    x = a * torch.sigmoid(b)\n    x = x + residual\n    x = x * self.sqrt\n    return x.permute((0, 2, 1))\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv_transposed/","title":"ConvTransposed","text":""},{"location":"models/tts/delightful_tts/conv_blocks/conv_transposed/#models.tts.delightful_tts.conv_blocks.conv_transposed.ConvTransposed","title":"<code>ConvTransposed</code>","text":"<p>             Bases: <code>Module</code></p> <p><code>ConvTransposed</code> applies a 1D convolution operation, with the main difference that it transposes the last two dimensions of the input tensor before and after applying the <code>BSConv1d</code> convolution operation. This can be useful in certain architectures where the tensor dimensions are processed in a different order.</p> <p>The <code>ConvTransposed</code> class performs a <code>BSConv</code> operation after transposing the input tensor dimensions. Specifically, it swaps the channels and width dimensions of a tensor, applies the convolution, and then swaps the dimensions back to their original order. The intuition behind swapping dimensions can depend on the specific use case in the larger architecture; typically, it's used when the operation or sequence of operations expected a different arrangement of dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution</p> required <code>kernel_size</code> <code>int</code> <p>Size of the kernel used in convolution</p> <code>1</code> <code>padding</code> <code>int</code> <p>Zero-padding added around the input tensor along the width direction</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>conv</code> <code>BSConv1d</code> <p><code>BSConv1d</code> module to apply convolution.</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv_transposed.py</code> <pre><code>class ConvTransposed(Module):\n    r\"\"\"`ConvTransposed` applies a 1D convolution operation, with the main difference that it transposes the\n    last two dimensions of the input tensor before and after applying the `BSConv1d` convolution operation.\n    This can be useful in certain architectures where the tensor dimensions are processed in a different order.\n\n    The `ConvTransposed` class performs a `BSConv` operation after transposing the input tensor dimensions. Specifically, it swaps the channels and width dimensions of a tensor, applies the convolution, and then swaps the dimensions back to their original order. The intuition behind swapping dimensions can depend on the specific use case in the larger architecture; typically, it's used when the operation or sequence of operations expected a different arrangement of dimensions.\n\n    Args:\n        in_channels (int): Number of channels in the input\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int): Size of the kernel used in convolution\n        padding (int): Zero-padding added around the input tensor along the width direction\n\n    Attributes:\n        conv (BSConv1d): `BSConv1d` module to apply convolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 1,\n        padding: int = 0,\n    ):\n        super().__init__()\n\n        # Define BSConv1d convolutional layer\n        self.conv = BSConv1d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward propagation method for the ConvTransposed layer.\n\n        Args:\n            x (torch.Tensor): input tensor\n\n        Returns:\n            x (torch.Tensor): output tensor after application of ConvTransposed\n        \"\"\"\n        # Transpose the last two dimensions (dimension 1 and 2 here). Now the tensor has shape (N, W, C)\n        x = x.contiguous().transpose(1, 2)\n\n        # Apply BSConv1d convolution.\n        x = self.conv(x)\n\n        # Transpose the last two dimensions back to their original order. Now the tensor has shape (N, C, W)\n        # Return final output tensor\n        return x.contiguous().transpose(1, 2)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/conv_transposed/#models.tts.delightful_tts.conv_blocks.conv_transposed.ConvTransposed.forward","title":"<code>forward(x)</code>","text":"<p>Forward propagation method for the ConvTransposed layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>output tensor after application of ConvTransposed</p> Source code in <code>models/tts/delightful_tts/conv_blocks/conv_transposed.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward propagation method for the ConvTransposed layer.\n\n    Args:\n        x (torch.Tensor): input tensor\n\n    Returns:\n        x (torch.Tensor): output tensor after application of ConvTransposed\n    \"\"\"\n    # Transpose the last two dimensions (dimension 1 and 2 here). Now the tensor has shape (N, W, C)\n    x = x.contiguous().transpose(1, 2)\n\n    # Apply BSConv1d convolution.\n    x = self.conv(x)\n\n    # Transpose the last two dimensions back to their original order. Now the tensor has shape (N, C, W)\n    # Return final output tensor\n    return x.contiguous().transpose(1, 2)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/coord_conv1d/","title":"CoordConv1d","text":""},{"location":"models/tts/delightful_tts/conv_blocks/coord_conv1d/#models.tts.delightful_tts.conv_blocks.coord_conv1d.CoordConv1d","title":"<code>CoordConv1d</code>","text":"<p>             Bases: <code>Conv1d</code>, <code>Module</code></p> <p><code>CoordConv1d</code> is an extension of the standard 1D convolution layer (<code>conv.Conv1d</code>), with the addition of extra coordinate channels. These extra channels encode positional coordinates, and optionally, the radial distance from the origin. This is inspired by the paper: An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution and is designed to help Convolution layers to pay attention to the absolute position of features in the input space.</p> <p>The responsibility of this class is to intercept the input tensor and append extra channels to it. These extra channels encode the positional coordinates (and optionally, the radial distance from the center). The enhanced tensor is then immediately passed through a standard Conv1D layer.</p> <p>In concrete terms, this means Convolution layer does not just process the color in an image-based task, but also 'knows' where in the overall image this color is located.</p> <p>In a typical Text-To-Speech (TTS) system like DelightfulTTS, the utterance is processed in a sequential manner. The importance of sequential data in such a use-case can benefit from <code>CoordConv</code> layer as it offers a way to draw more attention to the positioning of data. <code>CoordConv</code> is a drop-in replacement for standard convolution layers, enriches spatial representation in Convolutional Neural Networks (CNN) with additional positional information.</p> <p>Hence, the resultant Convolution does not only process the characteristics of the sound in the input speech signal, but also 'knows' where in the overall signal this particular sound is located, providing it with the spatial context. This can be particularly useful in TTS systems where the sequence of phonemes and their timing can be critical.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Zero-padding added to both sides of the input . Default: 0.</p> <code>0</code> <code>dilation</code> <code>int</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If True, adds a learnable bias to the output. Default: True.</p> <code>True</code> <code>with_r</code> <code>bool</code> <p>If True, adds a radial coordinate channel. Default: False.</p> <code>False</code> Source code in <code>models/tts/delightful_tts/conv_blocks/coord_conv1d.py</code> <pre><code>class CoordConv1d(conv.Conv1d, Module):\n    r\"\"\"`CoordConv1d` is an extension of the standard 1D convolution layer (`conv.Conv1d`), with the addition of extra coordinate\n    channels. These extra channels encode positional coordinates, and optionally, the radial distance from the origin.\n    This is inspired by the paper:\n    [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n    and is designed to help Convolution layers to pay attention to the absolute position of features in the input space.\n\n    The responsibility of this class is to intercept the input tensor and append extra channels to it. These extra channels\n    encode the positional coordinates (and optionally, the radial distance from the center). The enhanced tensor is then\n    immediately passed through a standard Conv1D layer.\n\n    In concrete terms, this means Convolution layer does not just process the color in an image-based task, but also 'knows'\n    where in the overall image this color is located.\n\n    In a typical Text-To-Speech (TTS) system like DelightfulTTS, the utterance is processed in a sequential manner.\n    The importance of sequential data in such a use-case can benefit from `CoordConv` layer as it offers a way to draw\n    more attention to the positioning of data. `CoordConv` is a drop-in replacement for standard convolution layers,\n    enriches spatial representation in Convolutional Neural Networks (CNN) with additional positional information.\n\n    Hence, the resultant Convolution does not only process the characteristics of the sound in the input speech signal,\n    but also 'knows' where in the overall signal this particular sound is located, providing it with the spatial context.\n    This can be particularly useful in TTS systems where the sequence of phonemes and their timing can be critical.\n\n    Args:\n        in_channels (int): Number of channels in the input.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1.\n        padding (int): Zero-padding added to both sides of the input . Default: 0.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        groups (int): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool): If True, adds a learnable bias to the output. Default: True.\n        with_r (bool): If True, adds a radial coordinate channel. Default: False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n        with_r: bool = False,\n    ):\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n\n        self.rank = 1\n        self.addcoords = AddCoords(self.rank, with_r)\n\n        self.conv = nn.Conv1d(\n            in_channels + self.rank + int(with_r),\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"The forward pass of the `CoordConv1d` module. It adds the coordinate channels to the input tensor with the `AddCoords`\n        module, and then immediately passes the result through a 1D convolution.\n\n        As a result, the subsequent Conv layers don't merely process sound characteristics of the speech signal, but are\n        also aware of their relative positioning, offering a notable improvement over traditional methods, particularly for\n        challenging TTS tasks where the sequence is critical.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, out_channels, length).\n        \"\"\"\n        # Apply AddCoords layer to add coordinate channels to the input tensor\n        x = self.addcoords(x)\n\n        # Apply convolution\n        return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/coord_conv1d/#models.tts.delightful_tts.conv_blocks.coord_conv1d.CoordConv1d.forward","title":"<code>forward(x)</code>","text":"<p>The forward pass of the <code>CoordConv1d</code> module. It adds the coordinate channels to the input tensor with the <code>AddCoords</code> module, and then immediately passes the result through a 1D convolution.</p> <p>As a result, the subsequent Conv layers don't merely process sound characteristics of the speech signal, but are also aware of their relative positioning, offering a notable improvement over traditional methods, particularly for challenging TTS tasks where the sequence is critical.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor of shape (batch_size, out_channels, length).</p> Source code in <code>models/tts/delightful_tts/conv_blocks/coord_conv1d.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"The forward pass of the `CoordConv1d` module. It adds the coordinate channels to the input tensor with the `AddCoords`\n    module, and then immediately passes the result through a 1D convolution.\n\n    As a result, the subsequent Conv layers don't merely process sound characteristics of the speech signal, but are\n    also aware of their relative positioning, offering a notable improvement over traditional methods, particularly for\n    challenging TTS tasks where the sequence is critical.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor of shape (batch_size, out_channels, length).\n    \"\"\"\n    # Apply AddCoords layer to add coordinate channels to the input tensor\n    x = self.addcoords(x)\n\n    # Apply convolution\n    return self.conv(x)\n</code></pre>"},{"location":"models/tts/delightful_tts/conv_blocks/readme/","title":"References","text":""},{"location":"models/tts/delightful_tts/conv_blocks/readme/#references","title":"References","text":""},{"location":"models/tts/delightful_tts/conv_blocks/readme/#activation-function-glu","title":"Activation Function GLU","text":"<p>Implements the Gated Linear Unit (GLU) activation function</p> <p>Paper: Language Modeling with Gated Convolutional Networks</p>"},{"location":"models/tts/delightful_tts/conv_blocks/readme/#bsconv","title":"BSConv","text":"<p><code>BSConv1d</code> implements the <code>BSConv</code> concept</p> <p>Paper: Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets</p>"},{"location":"models/tts/delightful_tts/conv_blocks/readme/#conv1d","title":"Conv1d","text":"<p>Implements Depthwise 1D convolution. This module will apply a spatial convolution over inputs  independently over each input channel in the style of depthwise convolutions.</p>"},{"location":"models/tts/delightful_tts/conv_blocks/readme/#conv1dglu","title":"Conv1dGLU","text":"<p>It's based on the Deep Voice 3 project</p> <p>Paper: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning</p> <ul> <li>ConvTransposed - <code>ConvTransposed</code> applies a 1D convolution operation, with the main difference that it transposes the  last two dimensions of the input tensor before and after applying the <code>BSConv1d</code> convolution operation.</li> </ul>"},{"location":"models/tts/delightful_tts/conv_blocks/readme/#coordconv1d","title":"CoordConv1d","text":"<p>Paper: An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</p> <ul> <li>AddCoords</li> </ul>"},{"location":"models/tts/delightful_tts/reference_encoder/STL/","title":"Style Token Layer (STL)","text":""},{"location":"models/tts/delightful_tts/reference_encoder/STL/#models.tts.delightful_tts.reference_encoder.STL.STL","title":"<code>STL</code>","text":"<p>             Bases: <code>Module</code></p> <p>Style Token Layer (STL). This layer helps to encapsulate different speaking styles in token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>AcousticModelConfigType</code> <p>An object containing the model's configuration parameters.</p> required <p>Attributes:</p> Name Type Description <code>embed</code> <code>Parameter</code> <p>The style token embedding tensor.</p> <code>attention</code> <code>StyleEmbedAttention</code> <p>The attention module used to compute a weighted sum of embeddings.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/STL.py</code> <pre><code>class STL(Module):\n    r\"\"\"Style Token Layer (STL).\n    This layer helps to encapsulate different speaking styles in token embeddings.\n\n    Args:\n        model_config (AcousticModelConfigType): An object containing the model's configuration parameters.\n\n    Attributes:\n        embed (nn.Parameter): The style token embedding tensor.\n        attention (StyleEmbedAttention): The attention module used to compute a weighted sum of embeddings.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: AcousticModelConfigType,\n    ):\n        super().__init__()\n\n        # Number of attention heads\n        num_heads = 1\n        # Dimension of encoder hidden states\n        n_hidden = model_config.encoder.n_hidden\n        # Number of style tokens\n        self.token_num = model_config.reference_encoder.token_num\n\n        # Define a learnable tensor for style tokens embedding\n        self.embed = nn.Parameter(\n            torch.FloatTensor(self.token_num, n_hidden // num_heads),\n        )\n\n        # Dimension of query in attention\n        d_q = n_hidden // 2\n        # Dimension of keys in attention\n        d_k = n_hidden // num_heads\n\n        # Style Embedding Attention module\n        self.attention = StyleEmbedAttention(\n            query_dim=d_q,\n            key_dim=d_k,\n            num_units=n_hidden,\n            num_heads=num_heads,\n        )\n\n        # Initialize the embedding with normal distribution\n        torch.nn.init.normal_(self.embed, mean=0, std=0.5)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the Style Token Layer\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns\n            torch.Tensor: The emotion embedded tensor after applying attention mechanism.\n        \"\"\"\n        N = x.size(0)\n\n        # Reshape input tensor to [N, 1, n_hidden // 2]\n        query = x.unsqueeze(1)\n\n        keys_soft = (\n            torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)\n        )  # [N, token_num, n_hidden // num_heads]\n\n        # Apply attention mechanism to get weighted sum of style token embeddings\n        return self.attention(query, keys_soft)\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/STL/#models.tts.delightful_tts.reference_encoder.STL.STL.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the Style Token Layer Args:     x (torch.Tensor): The input tensor.</p> <p>Returns     torch.Tensor: The emotion embedded tensor after applying attention mechanism.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/STL.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the Style Token Layer\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns\n        torch.Tensor: The emotion embedded tensor after applying attention mechanism.\n    \"\"\"\n    N = x.size(0)\n\n    # Reshape input tensor to [N, 1, n_hidden // 2]\n    query = x.unsqueeze(1)\n\n    keys_soft = (\n        torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)\n    )  # [N, token_num, n_hidden // num_heads]\n\n    # Apply attention mechanism to get weighted sum of style token embeddings\n    return self.attention(query, keys_soft)\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/phoneme_level_prosody_encoder/","title":"Phoneme Level Prosody Encoder","text":""},{"location":"models/tts/delightful_tts/reference_encoder/phoneme_level_prosody_encoder/#models.tts.delightful_tts.reference_encoder.phoneme_level_prosody_encoder.PhonemeLevelProsodyEncoder","title":"<code>PhonemeLevelProsodyEncoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Phoneme Level Prosody Encoder Module</p> <p>This Class is used to encode the phoneme level prosody in the speech synthesis pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>Configuration for preprocessing.</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>Acoustic model configuration.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The encoded tensor after applying masked fill.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/phoneme_level_prosody_encoder.py</code> <pre><code>class PhonemeLevelProsodyEncoder(Module):\n    r\"\"\"Phoneme Level Prosody Encoder Module\n\n    This Class is used to encode the phoneme level prosody in the speech synthesis pipeline.\n\n    Args:\n        preprocess_config (PreprocessingConfig): Configuration for preprocessing.\n        model_config (AcousticModelConfigType): Acoustic model configuration.\n\n    Returns:\n        torch.Tensor: The encoded tensor after applying masked fill.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        model_config: AcousticModelConfigType,\n    ):\n        super().__init__()\n\n        # Obtain the bottleneck size and reference encoder gru size from the model config.\n        bottleneck_size = model_config.reference_encoder.bottleneck_size_p\n        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n\n        # Initialize ReferenceEncoder, Linear layer and ConformerMultiHeadedSelfAttention for attention mechanism.\n        self.encoder = ReferenceEncoder(preprocess_config, model_config)\n        self.encoder_prj = nn.Linear(ref_enc_gru_size, model_config.encoder.n_hidden)\n        self.attention = ConformerMultiHeadedSelfAttention(\n            d_model=model_config.encoder.n_hidden,\n            num_heads=model_config.encoder.n_heads,\n            dropout_p=model_config.encoder.p_dropout,\n        )\n\n        # Bottleneck layer to transform the output of the attention mechanism.\n        self.encoder_bottleneck = nn.Linear(\n            model_config.encoder.n_hidden, bottleneck_size,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        src_mask: torch.Tensor,\n        mels: torch.Tensor,\n        mel_lens: torch.Tensor,\n        encoding: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"The forward pass of the PhonemeLevelProsodyEncoder. Input tensors are passed through the reference encoder,\n        attention mechanism, and a bottleneck.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape [N, seq_len, encoder_embedding_dim].\n            src_mask (torch.Tensor): The mask tensor which contains `True` at positions where the input x has been masked.\n            mels (torch.Tensor): The mel-spectrogram with shape [N, Ty/r, n_mels*r], where r=1.\n            mel_lens (torch.Tensor): The lengths of each sequence in mels.\n            encoding (torch.Tensor): The relative positional encoding tensor.\n\n        Returns:\n            torch.Tensor: Output tensor of shape [N, seq_len, bottleneck_size].\n        \"\"\"\n        # Use the reference encoder to embed prosody representation\n        embedded_prosody, _, mel_masks = self.encoder(mels, mel_lens)\n\n        # Pass the prosody representation through a bottleneck (dimension reduction)\n        embedded_prosody = self.encoder_prj(embedded_prosody)\n\n        # Flatten and apply attention mask\n        attn_mask = mel_masks.view((mel_masks.shape[0], 1, 1, -1))\n        x, _ = self.attention(\n            query=x,\n            key=embedded_prosody,\n            value=embedded_prosody,\n            mask=attn_mask,\n            encoding=encoding,\n        )\n\n        # Apply the bottleneck to the output and mask out irrelevant parts\n        x = self.encoder_bottleneck(x)\n        return x.masked_fill(src_mask.unsqueeze(-1), 0.0)\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/phoneme_level_prosody_encoder/#models.tts.delightful_tts.reference_encoder.phoneme_level_prosody_encoder.PhonemeLevelProsodyEncoder.forward","title":"<code>forward(x, src_mask, mels, mel_lens, encoding)</code>","text":"<p>The forward pass of the PhonemeLevelProsodyEncoder. Input tensors are passed through the reference encoder, attention mechanism, and a bottleneck.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [N, seq_len, encoder_embedding_dim].</p> required <code>src_mask</code> <code>Tensor</code> <p>The mask tensor which contains <code>True</code> at positions where the input x has been masked.</p> required <code>mels</code> <code>Tensor</code> <p>The mel-spectrogram with shape [N, Ty/r, n_mels*r], where r=1.</p> required <code>mel_lens</code> <code>Tensor</code> <p>The lengths of each sequence in mels.</p> required <code>encoding</code> <code>Tensor</code> <p>The relative positional encoding tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor of shape [N, seq_len, bottleneck_size].</p> Source code in <code>models/tts/delightful_tts/reference_encoder/phoneme_level_prosody_encoder.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    src_mask: torch.Tensor,\n    mels: torch.Tensor,\n    mel_lens: torch.Tensor,\n    encoding: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"The forward pass of the PhonemeLevelProsodyEncoder. Input tensors are passed through the reference encoder,\n    attention mechanism, and a bottleneck.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [N, seq_len, encoder_embedding_dim].\n        src_mask (torch.Tensor): The mask tensor which contains `True` at positions where the input x has been masked.\n        mels (torch.Tensor): The mel-spectrogram with shape [N, Ty/r, n_mels*r], where r=1.\n        mel_lens (torch.Tensor): The lengths of each sequence in mels.\n        encoding (torch.Tensor): The relative positional encoding tensor.\n\n    Returns:\n        torch.Tensor: Output tensor of shape [N, seq_len, bottleneck_size].\n    \"\"\"\n    # Use the reference encoder to embed prosody representation\n    embedded_prosody, _, mel_masks = self.encoder(mels, mel_lens)\n\n    # Pass the prosody representation through a bottleneck (dimension reduction)\n    embedded_prosody = self.encoder_prj(embedded_prosody)\n\n    # Flatten and apply attention mask\n    attn_mask = mel_masks.view((mel_masks.shape[0], 1, 1, -1))\n    x, _ = self.attention(\n        query=x,\n        key=embedded_prosody,\n        value=embedded_prosody,\n        mask=attn_mask,\n        encoding=encoding,\n    )\n\n    # Apply the bottleneck to the output and mask out irrelevant parts\n    x = self.encoder_bottleneck(x)\n    return x.masked_fill(src_mask.unsqueeze(-1), 0.0)\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/readme/","title":"References","text":""},{"location":"models/tts/delightful_tts/reference_encoder/readme/#references","title":"References","text":""},{"location":"models/tts/delightful_tts/reference_encoder/readme/#style-token-layer-stl","title":"Style Token Layer (STL)","text":"<p>This layer helps to encapsulate different speaking styles in token embeddings.</p>"},{"location":"models/tts/delightful_tts/reference_encoder/readme/#reference-encoder","title":"Reference Encoder","text":"<p>Similar to Tacotron model, the reference encoder is used to extract the high-level features from the reference</p> <p>It consists of a number of convolutional blocks (<code>CoordConv1d</code> for the first one and <code>nn.Conv1d</code> for the rest),  then followed by instance normalization and GRU layers. The <code>CoordConv1d</code> at the first layer to better preserve positional information, paper: Robust and fine-grained prosody control of end-to-end speech synthesis</p>"},{"location":"models/tts/delightful_tts/reference_encoder/readme/#utterance-level-prosody-encoder","title":"Utterance Level Prosody Encoder","text":"<p>A class to define the utterance level prosody encoder.</p> <p>The encoder uses a Reference encoder class to convert input sequences into high-level features, followed by prosody embedding, self attention on the embeddings, and a feedforward transformation to generate the final output.Initializes the encoder with given specifications and creates necessary layers.</p>"},{"location":"models/tts/delightful_tts/reference_encoder/readme/#phoneme-level-prosody-encoder","title":"Phoneme Level Prosody Encoder","text":"<p>This Class is used to encode the phoneme level prosody in the speech synthesis pipeline.</p>"},{"location":"models/tts/delightful_tts/reference_encoder/reference_encoder/","title":"Reference Encoder","text":""},{"location":"models/tts/delightful_tts/reference_encoder/reference_encoder/#models.tts.delightful_tts.reference_encoder.reference_encoder.ReferenceEncoder","title":"<code>ReferenceEncoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class to define the reference encoder. Similar to Tacotron model, the reference encoder is used to extract the high-level features from the reference</p> <p>It consists of a number of convolutional blocks (<code>CoordConv1d</code> for the first one and <code>nn.Conv1d</code> for the rest), then followed by instance normalization and GRU layers. The <code>CoordConv1d</code> at the first layer to better preserve positional information, paper: Robust and fine-grained prosody control of end-to-end speech synthesis</p> <p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>Configuration object with preprocessing parameters.</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>Configuration object with acoustic model parameters.</p> required <p>Returns:</p> Type Description <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. First: The sequence tensor produced by the last GRU layer after padding has been removed. Second: The GRU's final hidden state tensor. Third: The mask tensor, which has the same shape as x, and contains <code>True</code> at positions where the input x has been masked.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/reference_encoder.py</code> <pre><code>class ReferenceEncoder(Module):\n    r\"\"\"A class to define the reference encoder.\n    Similar to Tacotron model, the reference encoder is used to extract the high-level features from the reference\n\n    It consists of a number of convolutional blocks (`CoordConv1d` for the first one and `nn.Conv1d` for the rest),\n    then followed by instance normalization and GRU layers.\n    The `CoordConv1d` at the first layer to better preserve positional information, paper:\n    [Robust and fine-grained prosody control of end-to-end speech synthesis](https://arxiv.org/pdf/1811.02122.pdf)\n\n    Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n    Args:\n        preprocess_config (PreprocessingConfig): Configuration object with preprocessing parameters.\n        model_config (AcousticModelConfigType): Configuration object with acoustic model parameters.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. _First_: The sequence tensor\n            produced by the last GRU layer after padding has been removed. _Second_: The GRU's final hidden state tensor.\n            _Third_: The mask tensor, which has the same shape as x, and contains `True` at positions where the input x\n            has been masked.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        model_config: AcousticModelConfigType,\n    ):\n        super().__init__()\n\n        n_mel_channels = preprocess_config.stft.n_mel_channels\n        ref_enc_filters = model_config.reference_encoder.ref_enc_filters\n        ref_enc_size = model_config.reference_encoder.ref_enc_size\n        ref_enc_strides = model_config.reference_encoder.ref_enc_strides\n        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n\n        self.n_mel_channels = n_mel_channels\n        K = len(ref_enc_filters)\n        filters = [self.n_mel_channels, *ref_enc_filters]\n        strides = [1, *ref_enc_strides]\n\n        # Use CoordConv1d at the first layer to better preserve positional information: https://arxiv.org/pdf/1811.02122.pdf\n        convs = [\n            CoordConv1d(\n                in_channels=filters[0],\n                out_channels=filters[0 + 1],\n                kernel_size=ref_enc_size,\n                stride=strides[0],\n                padding=ref_enc_size // 2,\n                with_r=True,\n            ),\n            *[\n                nn.Conv1d(\n                    in_channels=filters[i],\n                    out_channels=filters[i + 1],\n                    kernel_size=ref_enc_size,\n                    stride=strides[i],\n                    padding=ref_enc_size // 2,\n                )\n                for i in range(1, K)\n            ],\n        ]\n        # Define convolution layers (ModuleList)\n        self.convs = nn.ModuleList(convs)\n\n        self.norms = nn.ModuleList(\n            [\n                nn.InstanceNorm1d(num_features=ref_enc_filters[i], affine=True)\n                for i in range(K)\n            ],\n        )\n\n        # Define GRU layer\n        self.gru = nn.GRU(\n            input_size=ref_enc_filters[-1],\n            hidden_size=ref_enc_gru_size,\n            batch_first=True,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mel_lens: torch.Tensor,\n        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"Forward pass of the ReferenceEncoder.\n\n        Args:\n            x (torch.Tensor): A 3-dimensional tensor containing the input sequences, its size is [N, n_mels, timesteps].\n            mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in x. Its length is N.\n            leaky_relu_slope (float): The slope of the leaky relu function.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. _First_: The sequence tensor\n                produced by the last GRU layer after padding has been removed. _Second_: The GRU's final hidden state tensor.\n                _Third_: The mask tensor, which has the same shape as x, and contains `True` at positions where the input x\n                has been masked.\n        \"\"\"\n        mel_masks = tools.get_mask_from_lengths(mel_lens).unsqueeze(1)\n        mel_masks = mel_masks.to(x.device)\n\n        x = x.masked_fill(mel_masks, 0)\n        for conv, norm in zip(self.convs, self.norms):\n            x = x.float()\n            x = conv(x)\n            x = F.leaky_relu(x, leaky_relu_slope)  # [N, 128, Ty//2^K, n_mels//2^K]\n            x = norm(x)\n\n        for _ in range(2):\n            mel_lens = tools.stride_lens_downsampling(mel_lens)\n\n        mel_masks = tools.get_mask_from_lengths(mel_lens)\n\n        x = x.masked_fill(mel_masks.unsqueeze(1), 0)\n        x = x.permute((0, 2, 1))\n\n        packed_sequence = torch.nn.utils.rnn.pack_padded_sequence(\n            x,\n            lengths=mel_lens.cpu().int(),\n            batch_first=True,\n            enforce_sorted=False,\n        )\n\n        self.gru.flatten_parameters()\n        # memory --- [N, Ty, E//2], out --- [1, N, E//2]\n        out, memory = self.gru(packed_sequence)\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n\n        return out, memory, mel_masks\n\n    def calculate_channels(\n        self,\n        L: int,\n        kernel_size: int,\n        stride: int,\n        pad: int,\n        n_convs: int,\n    ) -&gt; int:\n        r\"\"\"Calculate the number of channels after applying convolutions.\n\n        Args:\n            L (int): The original size.\n            kernel_size (int): The kernel size used in the convolutions.\n            stride (int): The stride used in the convolutions.\n            pad (int): The padding used in the convolutions.\n            n_convs (int): The number of convolutions.\n\n        Returns:\n            int: The size after the convolutions.\n        \"\"\"\n        # Loop through each convolution\n        for _ in range(n_convs):\n            # Calculate the size after each convolution\n            L = (L - kernel_size + 2 * pad) // stride + 1\n        return L\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/reference_encoder/#models.tts.delightful_tts.reference_encoder.reference_encoder.ReferenceEncoder.calculate_channels","title":"<code>calculate_channels(L, kernel_size, stride, pad, n_convs)</code>","text":"<p>Calculate the number of channels after applying convolutions.</p> <p>Parameters:</p> Name Type Description Default <code>L</code> <code>int</code> <p>The original size.</p> required <code>kernel_size</code> <code>int</code> <p>The kernel size used in the convolutions.</p> required <code>stride</code> <code>int</code> <p>The stride used in the convolutions.</p> required <code>pad</code> <code>int</code> <p>The padding used in the convolutions.</p> required <code>n_convs</code> <code>int</code> <p>The number of convolutions.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The size after the convolutions.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/reference_encoder.py</code> <pre><code>def calculate_channels(\n    self,\n    L: int,\n    kernel_size: int,\n    stride: int,\n    pad: int,\n    n_convs: int,\n) -&gt; int:\n    r\"\"\"Calculate the number of channels after applying convolutions.\n\n    Args:\n        L (int): The original size.\n        kernel_size (int): The kernel size used in the convolutions.\n        stride (int): The stride used in the convolutions.\n        pad (int): The padding used in the convolutions.\n        n_convs (int): The number of convolutions.\n\n    Returns:\n        int: The size after the convolutions.\n    \"\"\"\n    # Loop through each convolution\n    for _ in range(n_convs):\n        # Calculate the size after each convolution\n        L = (L - kernel_size + 2 * pad) // stride + 1\n    return L\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/reference_encoder/#models.tts.delightful_tts.reference_encoder.reference_encoder.ReferenceEncoder.forward","title":"<code>forward(x, mel_lens, leaky_relu_slope=LEAKY_RELU_SLOPE)</code>","text":"<p>Forward pass of the ReferenceEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A 3-dimensional tensor containing the input sequences, its size is [N, n_mels, timesteps].</p> required <code>mel_lens</code> <code>Tensor</code> <p>A 1-dimensional tensor containing the lengths of each sequence in x. Its length is N.</p> required <code>leaky_relu_slope</code> <code>float</code> <p>The slope of the leaky relu function.</p> <code>LEAKY_RELU_SLOPE</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. First: The sequence tensor produced by the last GRU layer after padding has been removed. Second: The GRU's final hidden state tensor. Third: The mask tensor, which has the same shape as x, and contains <code>True</code> at positions where the input x has been masked.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/reference_encoder.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    mel_lens: torch.Tensor,\n    leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r\"\"\"Forward pass of the ReferenceEncoder.\n\n    Args:\n        x (torch.Tensor): A 3-dimensional tensor containing the input sequences, its size is [N, n_mels, timesteps].\n        mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in x. Its length is N.\n        leaky_relu_slope (float): The slope of the leaky relu function.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. _First_: The sequence tensor\n            produced by the last GRU layer after padding has been removed. _Second_: The GRU's final hidden state tensor.\n            _Third_: The mask tensor, which has the same shape as x, and contains `True` at positions where the input x\n            has been masked.\n    \"\"\"\n    mel_masks = tools.get_mask_from_lengths(mel_lens).unsqueeze(1)\n    mel_masks = mel_masks.to(x.device)\n\n    x = x.masked_fill(mel_masks, 0)\n    for conv, norm in zip(self.convs, self.norms):\n        x = x.float()\n        x = conv(x)\n        x = F.leaky_relu(x, leaky_relu_slope)  # [N, 128, Ty//2^K, n_mels//2^K]\n        x = norm(x)\n\n    for _ in range(2):\n        mel_lens = tools.stride_lens_downsampling(mel_lens)\n\n    mel_masks = tools.get_mask_from_lengths(mel_lens)\n\n    x = x.masked_fill(mel_masks.unsqueeze(1), 0)\n    x = x.permute((0, 2, 1))\n\n    packed_sequence = torch.nn.utils.rnn.pack_padded_sequence(\n        x,\n        lengths=mel_lens.cpu().int(),\n        batch_first=True,\n        enforce_sorted=False,\n    )\n\n    self.gru.flatten_parameters()\n    # memory --- [N, Ty, E//2], out --- [1, N, E//2]\n    out, memory = self.gru(packed_sequence)\n    out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n\n    return out, memory, mel_masks\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/utterance_level_prosody_encoder/","title":"Utterance Level Prosody Encoder","text":""},{"location":"models/tts/delightful_tts/reference_encoder/utterance_level_prosody_encoder/#models.tts.delightful_tts.reference_encoder.utterance_level_prosody_encoder.UtteranceLevelProsodyEncoder","title":"<code>UtteranceLevelProsodyEncoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class to define the utterance level prosody encoder.</p> <p>The encoder uses a Reference encoder class to convert input sequences into high-level features, followed by prosody embedding, self attention on the embeddings, and a feedforward transformation to generate the final output.Initializes the encoder with given specifications and creates necessary layers.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>Configuration object with preprocessing parameters.</p> required <code>model_config</code> <code>AcousticModelConfigType</code> <p>Configuration object with acoustic model parameters.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A 3-dimensional tensor sized <code>[N, seq_len, E]</code>.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/utterance_level_prosody_encoder.py</code> <pre><code>class UtteranceLevelProsodyEncoder(Module):\n    r\"\"\"A class to define the utterance level prosody encoder.\n\n    The encoder uses a Reference encoder class to convert input sequences into high-level features,\n    followed by prosody embedding, self attention on the embeddings, and a feedforward transformation to generate the final output.Initializes the encoder with given specifications and creates necessary layers.\n\n    Args:\n        preprocess_config (PreprocessingConfig): Configuration object with preprocessing parameters.\n        model_config (AcousticModelConfigType): Configuration object with acoustic model parameters.\n\n    Returns:\n        torch.Tensor: A 3-dimensional tensor sized `[N, seq_len, E]`.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        model_config: AcousticModelConfigType,\n    ):\n        super().__init__()\n\n        self.E = model_config.encoder.n_hidden\n        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n        ref_attention_dropout = model_config.reference_encoder.ref_attention_dropout\n        bottleneck_size = model_config.reference_encoder.bottleneck_size_u\n\n        # Define important layers/modules for the encoder\n        self.encoder = ReferenceEncoder(preprocess_config, model_config)\n        self.encoder_prj = nn.Linear(ref_enc_gru_size, self.E // 2)\n        self.stl = STL(model_config)\n        self.encoder_bottleneck = nn.Linear(self.E, bottleneck_size)\n        self.dropout = nn.Dropout(ref_attention_dropout)\n\n    def forward(self, mels: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Defines the forward pass of the utterance level prosody encoder.\n\n        Args:\n            mels (torch.Tensor): A 3-dimensional tensor containing input sequences. Size is `[N, Ty/r, n_mels*r]`.\n            mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in mels. Length is N.\n\n        Returns:\n            torch.Tensor: A 3-dimensional tensor sized `[N, seq_len, E]`.\n        \"\"\"\n        # Use the reference encoder to get prosody embeddings\n        _, embedded_prosody, _ = self.encoder(mels, mel_lens)\n\n        # Bottleneck\n        # Use the linear projection layer on the prosody embeddings\n        embedded_prosody = self.encoder_prj(embedded_prosody)\n\n        # Apply the style token layer followed by the bottleneck layer\n        out = self.encoder_bottleneck(self.stl(embedded_prosody))\n\n        # Apply dropout for regularization\n        out = self.dropout(out)\n\n        # Reshape the output tensor before returning\n        return out.view((-1, 1, out.shape[3]))\n</code></pre>"},{"location":"models/tts/delightful_tts/reference_encoder/utterance_level_prosody_encoder/#models.tts.delightful_tts.reference_encoder.utterance_level_prosody_encoder.UtteranceLevelProsodyEncoder.forward","title":"<code>forward(mels, mel_lens)</code>","text":"<p>Defines the forward pass of the utterance level prosody encoder.</p> <p>Parameters:</p> Name Type Description Default <code>mels</code> <code>Tensor</code> <p>A 3-dimensional tensor containing input sequences. Size is <code>[N, Ty/r, n_mels*r]</code>.</p> required <code>mel_lens</code> <code>Tensor</code> <p>A 1-dimensional tensor containing the lengths of each sequence in mels. Length is N.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 3-dimensional tensor sized <code>[N, seq_len, E]</code>.</p> Source code in <code>models/tts/delightful_tts/reference_encoder/utterance_level_prosody_encoder.py</code> <pre><code>def forward(self, mels: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Defines the forward pass of the utterance level prosody encoder.\n\n    Args:\n        mels (torch.Tensor): A 3-dimensional tensor containing input sequences. Size is `[N, Ty/r, n_mels*r]`.\n        mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in mels. Length is N.\n\n    Returns:\n        torch.Tensor: A 3-dimensional tensor sized `[N, seq_len, E]`.\n    \"\"\"\n    # Use the reference encoder to get prosody embeddings\n    _, embedded_prosody, _ = self.encoder(mels, mel_lens)\n\n    # Bottleneck\n    # Use the linear projection layer on the prosody embeddings\n    embedded_prosody = self.encoder_prj(embedded_prosody)\n\n    # Apply the style token layer followed by the bottleneck layer\n    out = self.encoder_bottleneck(self.stl(embedded_prosody))\n\n    # Apply dropout for regularization\n    out = self.dropout(out)\n\n    # Reshape the output tensor before returning\n    return out.view((-1, 1, out.shape[3]))\n</code></pre>"},{"location":"models/tts/styledtts2/readme/","title":"References","text":""},{"location":"models/tts/styledtts2/readme/#wip","title":"WIP","text":""},{"location":"models/tts/styledtts2/diffusion/ada_layer_norm/","title":"AdaLayerNorm","text":""},{"location":"models/tts/styledtts2/diffusion/ada_layer_norm/#models.tts.styledtts2.diffusion.ada_layer_norm.AdaLayerNorm","title":"<code>AdaLayerNorm</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class used to represent an adaptive layer normalization module.</p> <p>Attributes:</p> Name Type Description <code>channels</code> <code>int</code> <p>The number of channels in the input data.</p> <code>eps</code> <code>float</code> <p>A small value added to the denominator for numerical stability.</p> <code>fc</code> <code>Linear</code> <p>A fully connected layer used to compute the scale and shift parameters.</p> <p>Parameters:</p> Name Type Description Default <code>style_dim</code> <code>int</code> <p>The dimension of the style vector.</p> required <code>channels</code> <code>int</code> <p>The number of channels in the input data.</p> required <code>eps</code> <code>float</code> <p>A small value added to the denominator for numerical stability. Defaults to 1e-5.</p> <code>1e-05</code> Source code in <code>models/tts/styledtts2/diffusion/ada_layer_norm.py</code> <pre><code>class AdaLayerNorm(nn.Module):\n    r\"\"\"A class used to represent an adaptive layer normalization module.\n\n    Attributes:\n        channels (int): The number of channels in the input data.\n        eps (float): A small value added to the denominator for numerical stability.\n        fc (nn.Linear): A fully connected layer used to compute the scale and shift parameters.\n\n    Args:\n        style_dim (int): The dimension of the style vector.\n        channels (int): The number of channels in the input data.\n        eps (float, optional): A small value added to the denominator for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, style_dim: int, channels: int, eps: float=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.fc = nn.Linear(style_dim, channels*2)\n\n    def forward(self, x: Tensor, s: Tensor) -&gt; Tensor:\n        r\"\"\"Applies adaptive layer normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, num_samples, num_channels).\n            s (torch.Tensor): The style tensor of shape (batch_size, style_dim).\n\n        Returns:\n            torch.Tensor: The normalized tensor of the same shape as the input tensor.\n        \"\"\"\n        x = x.transpose(-1, -2)\n        x = x.transpose(1, -1)\n\n        h = self.fc(s)\n        h = h.view(h.size(0), h.size(1), 1)\n\n        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n        gamma, beta = gamma.transpose(1, -1), beta.transpose(1, -1)\n\n        x = F.layer_norm(x, (self.channels,), eps=self.eps)\n        x = (1 + gamma) * x + beta\n        return x.transpose(1, -1).transpose(-1, -2)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/ada_layer_norm/#models.tts.styledtts2.diffusion.ada_layer_norm.AdaLayerNorm.forward","title":"<code>forward(x, s)</code>","text":"<p>Applies adaptive layer normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, num_samples, num_channels).</p> required <code>s</code> <code>Tensor</code> <p>The style tensor of shape (batch_size, style_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The normalized tensor of the same shape as the input tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/ada_layer_norm.py</code> <pre><code>def forward(self, x: Tensor, s: Tensor) -&gt; Tensor:\n    r\"\"\"Applies adaptive layer normalization to the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor of shape (batch_size, num_samples, num_channels).\n        s (torch.Tensor): The style tensor of shape (batch_size, style_dim).\n\n    Returns:\n        torch.Tensor: The normalized tensor of the same shape as the input tensor.\n    \"\"\"\n    x = x.transpose(-1, -2)\n    x = x.transpose(1, -1)\n\n    h = self.fc(s)\n    h = h.view(h.size(0), h.size(1), 1)\n\n    gamma, beta = torch.chunk(h, chunks=2, dim=1)\n    gamma, beta = gamma.transpose(1, -1), beta.transpose(1, -1)\n\n    x = F.layer_norm(x, (self.channels,), eps=self.eps)\n    x = (1 + gamma) * x + beta\n    return x.transpose(1, -1).transpose(-1, -2)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/","title":"Attention","text":""},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.Attention","title":"<code>Attention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Attention class that creates an attention mechanism with optional context.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>class Attention(nn.Module):\n    r\"\"\"Attention class that creates an attention mechanism with optional context.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        *,\n        head_features: int,\n        num_heads: int,\n        out_features: Optional[int] = None,\n        context_features: Optional[int] = None,\n        use_rel_pos: bool,\n        rel_pos_num_buckets: Optional[int] = None,\n        rel_pos_max_distance: Optional[int] = None,\n    ):\n        r\"\"\"Initialize the Attention with features, head features, number of heads, and relative position parameters.\n\n        Args:\n            features (int): The number of input features.\n            head_features (int): The number of features in each head.\n            num_heads (int): The number of heads.\n            out_features (Optional[int]): The number of output features. If None, it will be set to the number of input features.\n            context_features (Optional[int]): The number of context features. If None, it will be set to the number of input features.\n            use_rel_pos (bool): Whether to use relative position bias.\n            rel_pos_num_buckets (Optional[int]): The number of buckets for relative position bias. Required if use_rel_pos is True.\n            rel_pos_max_distance (Optional[int]): The maximum distance for relative position bias. Required if use_rel_pos is True.\n        \"\"\"\n        super().__init__()\n        self.context_features = context_features\n        mid_features = head_features * num_heads\n        context_features = default(context_features, features)\n\n        self.norm = nn.LayerNorm(features)\n        self.norm_context = nn.LayerNorm(context_features)\n        self.to_q = nn.Linear(\n            in_features=features, out_features=mid_features, bias=False,\n        )\n        self.to_kv = nn.Linear(\n            in_features=context_features, out_features=mid_features * 2, bias=False,\n        )\n\n        self.attention = AttentionBase(\n            features,\n            out_features=out_features,\n            num_heads=num_heads,\n            head_features=head_features,\n            use_rel_pos=use_rel_pos,\n            rel_pos_num_buckets=rel_pos_num_buckets,\n            rel_pos_max_distance=rel_pos_max_distance,\n        )\n\n    def forward(self, x: Tensor, *, context: Optional[Tensor] = None) -&gt; Tensor:\n        r\"\"\"Forward pass of the Attention.\n\n        Args:\n            x (Tensor): The input tensor.\n            context (Optional[Tensor]): The context tensor. If None, the input tensor will be used as the context.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        assert_message = \"You must provide a context when using context_features\"\n        assert not self.context_features or exists(context), assert_message\n\n        # Use context if provided\n        context = default(context, x)\n        # Normalize then compute q from input and k,v from context\n        x, context = self.norm(x), self.norm_context(context)\n        q, k, v = (self.to_q(x), *torch.chunk(self.to_kv(context), chunks=2, dim=-1))\n\n        # Compute and return attention\n        return self.attention(q, k, v)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.Attention.__init__","title":"<code>__init__(features, *, head_features, num_heads, out_features=None, context_features=None, use_rel_pos, rel_pos_num_buckets=None, rel_pos_max_distance=None)</code>","text":"<p>Initialize the Attention with features, head features, number of heads, and relative position parameters.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>The number of input features.</p> required <code>head_features</code> <code>int</code> <p>The number of features in each head.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads.</p> required <code>out_features</code> <code>Optional[int]</code> <p>The number of output features. If None, it will be set to the number of input features.</p> <code>None</code> <code>context_features</code> <code>Optional[int]</code> <p>The number of context features. If None, it will be set to the number of input features.</p> <code>None</code> <code>use_rel_pos</code> <code>bool</code> <p>Whether to use relative position bias.</p> required <code>rel_pos_num_buckets</code> <code>Optional[int]</code> <p>The number of buckets for relative position bias. Required if use_rel_pos is True.</p> <code>None</code> <code>rel_pos_max_distance</code> <code>Optional[int]</code> <p>The maximum distance for relative position bias. Required if use_rel_pos is True.</p> <code>None</code> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def __init__(\n    self,\n    features: int,\n    *,\n    head_features: int,\n    num_heads: int,\n    out_features: Optional[int] = None,\n    context_features: Optional[int] = None,\n    use_rel_pos: bool,\n    rel_pos_num_buckets: Optional[int] = None,\n    rel_pos_max_distance: Optional[int] = None,\n):\n    r\"\"\"Initialize the Attention with features, head features, number of heads, and relative position parameters.\n\n    Args:\n        features (int): The number of input features.\n        head_features (int): The number of features in each head.\n        num_heads (int): The number of heads.\n        out_features (Optional[int]): The number of output features. If None, it will be set to the number of input features.\n        context_features (Optional[int]): The number of context features. If None, it will be set to the number of input features.\n        use_rel_pos (bool): Whether to use relative position bias.\n        rel_pos_num_buckets (Optional[int]): The number of buckets for relative position bias. Required if use_rel_pos is True.\n        rel_pos_max_distance (Optional[int]): The maximum distance for relative position bias. Required if use_rel_pos is True.\n    \"\"\"\n    super().__init__()\n    self.context_features = context_features\n    mid_features = head_features * num_heads\n    context_features = default(context_features, features)\n\n    self.norm = nn.LayerNorm(features)\n    self.norm_context = nn.LayerNorm(context_features)\n    self.to_q = nn.Linear(\n        in_features=features, out_features=mid_features, bias=False,\n    )\n    self.to_kv = nn.Linear(\n        in_features=context_features, out_features=mid_features * 2, bias=False,\n    )\n\n    self.attention = AttentionBase(\n        features,\n        out_features=out_features,\n        num_heads=num_heads,\n        head_features=head_features,\n        use_rel_pos=use_rel_pos,\n        rel_pos_num_buckets=rel_pos_num_buckets,\n        rel_pos_max_distance=rel_pos_max_distance,\n    )\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.Attention.forward","title":"<code>forward(x, *, context=None)</code>","text":"<p>Forward pass of the Attention.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>context</code> <code>Optional[Tensor]</code> <p>The context tensor. If None, the input tensor will be used as the context.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def forward(self, x: Tensor, *, context: Optional[Tensor] = None) -&gt; Tensor:\n    r\"\"\"Forward pass of the Attention.\n\n    Args:\n        x (Tensor): The input tensor.\n        context (Optional[Tensor]): The context tensor. If None, the input tensor will be used as the context.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    assert_message = \"You must provide a context when using context_features\"\n    assert not self.context_features or exists(context), assert_message\n\n    # Use context if provided\n    context = default(context, x)\n    # Normalize then compute q from input and k,v from context\n    x, context = self.norm(x), self.norm_context(context)\n    q, k, v = (self.to_q(x), *torch.chunk(self.to_kv(context), chunks=2, dim=-1))\n\n    # Compute and return attention\n    return self.attention(q, k, v)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.AttentionBase","title":"<code>AttentionBase</code>","text":"<p>             Bases: <code>Module</code></p> <p>AttentionBase class that creates a base attention mechanism.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>class AttentionBase(nn.Module):\n    r\"\"\"AttentionBase class that creates a base attention mechanism.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        *,\n        head_features: int,\n        num_heads: int,\n        use_rel_pos: bool,\n        out_features: Optional[int] = None,\n        rel_pos_num_buckets: Optional[int] = None,\n        rel_pos_max_distance: Optional[int] = None,\n    ):\n        r\"\"\"Initialize the AttentionBase with features, head features, number of heads, and relative position parameters.\n\n        Args:\n            features (int): The number of input features.\n            head_features (int): The number of features in each head.\n            num_heads (int): The number of heads.\n            use_rel_pos (bool): Whether to use relative position bias.\n            out_features (Optional[int]): The number of output features. If None, it will be set to the number of input features.\n            rel_pos_num_buckets (Optional[int]): The number of buckets for relative position bias. Required if use_rel_pos is True.\n            rel_pos_max_distance (Optional[int]): The maximum distance for relative position bias. Required if use_rel_pos is True.\n        \"\"\"\n        super().__init__()\n        self.scale = head_features ** -0.5\n        self.num_heads = num_heads\n        self.use_rel_pos = use_rel_pos\n        mid_features = head_features * num_heads\n\n        if use_rel_pos:\n            if not exists(rel_pos_num_buckets):\n                raise ValueError(\"rel_pos_num_buckets must be provided.\")\n            if not exists(rel_pos_max_distance):\n                raise ValueError(\"rel_pos_max_distance must be provided.\")\n\n            self.rel_pos = RelativePositionBias(\n                num_buckets=rel_pos_num_buckets,\n                max_distance=rel_pos_max_distance,\n                num_heads=num_heads,\n            )\n        if out_features is None:\n            out_features = features\n\n        self.to_out = nn.Linear(in_features=mid_features, out_features=out_features)\n\n    def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass of the AttentionBase.\n\n        Args:\n            q (Tensor): The query tensor.\n            k (Tensor): The key tensor.\n            v (Tensor): The value tensor.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        # Split heads\n        q, k, v = rearrange_many((q, k, v), \"b n (h d) -&gt; b h n d\", h=self.num_heads)\n        # Compute similarity matrix\n        sim = einsum(\"... n d, ... m d -&gt; ... n m\", q, k)\n        sim = (sim + self.rel_pos(*sim.shape[-2:])) if self.use_rel_pos else sim\n        sim = sim * self.scale\n        # Get attention matrix with softmax\n        attn = sim.softmax(dim=-1)\n        # Compute values\n        out = einsum(\"... n m, ... m d -&gt; ... n d\", attn, v)\n        out = rearrange(out, \"b h n d -&gt; b n (h d)\")\n        return self.to_out(out)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.AttentionBase.__init__","title":"<code>__init__(features, *, head_features, num_heads, use_rel_pos, out_features=None, rel_pos_num_buckets=None, rel_pos_max_distance=None)</code>","text":"<p>Initialize the AttentionBase with features, head features, number of heads, and relative position parameters.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>The number of input features.</p> required <code>head_features</code> <code>int</code> <p>The number of features in each head.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads.</p> required <code>use_rel_pos</code> <code>bool</code> <p>Whether to use relative position bias.</p> required <code>out_features</code> <code>Optional[int]</code> <p>The number of output features. If None, it will be set to the number of input features.</p> <code>None</code> <code>rel_pos_num_buckets</code> <code>Optional[int]</code> <p>The number of buckets for relative position bias. Required if use_rel_pos is True.</p> <code>None</code> <code>rel_pos_max_distance</code> <code>Optional[int]</code> <p>The maximum distance for relative position bias. Required if use_rel_pos is True.</p> <code>None</code> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def __init__(\n    self,\n    features: int,\n    *,\n    head_features: int,\n    num_heads: int,\n    use_rel_pos: bool,\n    out_features: Optional[int] = None,\n    rel_pos_num_buckets: Optional[int] = None,\n    rel_pos_max_distance: Optional[int] = None,\n):\n    r\"\"\"Initialize the AttentionBase with features, head features, number of heads, and relative position parameters.\n\n    Args:\n        features (int): The number of input features.\n        head_features (int): The number of features in each head.\n        num_heads (int): The number of heads.\n        use_rel_pos (bool): Whether to use relative position bias.\n        out_features (Optional[int]): The number of output features. If None, it will be set to the number of input features.\n        rel_pos_num_buckets (Optional[int]): The number of buckets for relative position bias. Required if use_rel_pos is True.\n        rel_pos_max_distance (Optional[int]): The maximum distance for relative position bias. Required if use_rel_pos is True.\n    \"\"\"\n    super().__init__()\n    self.scale = head_features ** -0.5\n    self.num_heads = num_heads\n    self.use_rel_pos = use_rel_pos\n    mid_features = head_features * num_heads\n\n    if use_rel_pos:\n        if not exists(rel_pos_num_buckets):\n            raise ValueError(\"rel_pos_num_buckets must be provided.\")\n        if not exists(rel_pos_max_distance):\n            raise ValueError(\"rel_pos_max_distance must be provided.\")\n\n        self.rel_pos = RelativePositionBias(\n            num_buckets=rel_pos_num_buckets,\n            max_distance=rel_pos_max_distance,\n            num_heads=num_heads,\n        )\n    if out_features is None:\n        out_features = features\n\n    self.to_out = nn.Linear(in_features=mid_features, out_features=out_features)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.AttentionBase.forward","title":"<code>forward(q, k, v)</code>","text":"<p>Forward pass of the AttentionBase.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>The query tensor.</p> required <code>k</code> <code>Tensor</code> <p>The key tensor.</p> required <code>v</code> <code>Tensor</code> <p>The value tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass of the AttentionBase.\n\n    Args:\n        q (Tensor): The query tensor.\n        k (Tensor): The key tensor.\n        v (Tensor): The value tensor.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    # Split heads\n    q, k, v = rearrange_many((q, k, v), \"b n (h d) -&gt; b h n d\", h=self.num_heads)\n    # Compute similarity matrix\n    sim = einsum(\"... n d, ... m d -&gt; ... n m\", q, k)\n    sim = (sim + self.rel_pos(*sim.shape[-2:])) if self.use_rel_pos else sim\n    sim = sim * self.scale\n    # Get attention matrix with softmax\n    attn = sim.softmax(dim=-1)\n    # Compute values\n    out = einsum(\"... n m, ... m d -&gt; ... n d\", attn, v)\n    out = rearrange(out, \"b h n d -&gt; b n (h d)\")\n    return self.to_out(out)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.RelativePositionBias","title":"<code>RelativePositionBias</code>","text":"<p>             Bases: <code>Module</code></p> <p>RelativePositionBias class that creates a relative position bias for attention mechanisms.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>class RelativePositionBias(nn.Module):\n    r\"\"\"RelativePositionBias class that creates a relative position bias for attention mechanisms.\"\"\"\n\n    def __init__(self, num_buckets: int, max_distance: int, num_heads: int):\n        r\"\"\"Initialize the RelativePositionBias with a number of buckets, maximum distance, and number of heads.\n\n        Args:\n            num_buckets (int): The number of buckets for the relative position bias.\n            max_distance (int): The maximum distance for the relative position bias.\n            num_heads (int): The number of heads for the relative position bias.\n        \"\"\"\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.num_heads = num_heads\n        self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position: Tensor, num_buckets: int, max_distance: int,\n    ) -&gt; Tensor:\n        r\"\"\"Compute the relative position bucket.\n\n        Args:\n            relative_position (Tensor): The relative position tensor.\n            num_buckets (int): The number of buckets.\n            max_distance (int): The maximum distance.\n\n        Returns:\n            Tensor: The relative position bucket tensor.\n        \"\"\"\n        num_buckets //= 2\n        ret = (relative_position &gt;= 0).to(torch.long) * num_buckets\n        n = torch.abs(relative_position)\n\n        max_exact = num_buckets // 2\n        is_small = n &lt; max_exact\n\n        val_if_large = (\n            max_exact\n            + (\n                torch.log(n.float() / max_exact)\n                / log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n            ).long()\n        )\n        val_if_large = torch.min(\n            val_if_large, torch.full_like(val_if_large, num_buckets - 1),\n        )\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, num_queries: int, num_keys: int) -&gt; Tensor:\n        r\"\"\"Forward pass of the RelativePositionBias.\n\n        Args:\n            num_queries (int): The number of queries.\n            num_keys (int): The number of keys.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        i, j, device = num_queries, num_keys, self.relative_attention_bias.weight.device\n        q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n        k_pos = torch.arange(j, dtype=torch.long, device=device)\n        rel_pos = rearrange(k_pos, \"j -&gt; 1 j\") - rearrange(q_pos, \"i -&gt; i 1\")\n\n        relative_position_bucket = self._relative_position_bucket(\n            rel_pos, num_buckets=self.num_buckets, max_distance=self.max_distance,\n        )\n\n        bias = self.relative_attention_bias(relative_position_bucket)\n        bias = rearrange(bias, \"m n h -&gt; 1 h m n\")\n        return bias\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.RelativePositionBias.__init__","title":"<code>__init__(num_buckets, max_distance, num_heads)</code>","text":"<p>Initialize the RelativePositionBias with a number of buckets, maximum distance, and number of heads.</p> <p>Parameters:</p> Name Type Description Default <code>num_buckets</code> <code>int</code> <p>The number of buckets for the relative position bias.</p> required <code>max_distance</code> <code>int</code> <p>The maximum distance for the relative position bias.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads for the relative position bias.</p> required Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def __init__(self, num_buckets: int, max_distance: int, num_heads: int):\n    r\"\"\"Initialize the RelativePositionBias with a number of buckets, maximum distance, and number of heads.\n\n    Args:\n        num_buckets (int): The number of buckets for the relative position bias.\n        max_distance (int): The maximum distance for the relative position bias.\n        num_heads (int): The number of heads for the relative position bias.\n    \"\"\"\n    super().__init__()\n    self.num_buckets = num_buckets\n    self.max_distance = max_distance\n    self.num_heads = num_heads\n    self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.RelativePositionBias.forward","title":"<code>forward(num_queries, num_keys)</code>","text":"<p>Forward pass of the RelativePositionBias.</p> <p>Parameters:</p> Name Type Description Default <code>num_queries</code> <code>int</code> <p>The number of queries.</p> required <code>num_keys</code> <code>int</code> <p>The number of keys.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def forward(self, num_queries: int, num_keys: int) -&gt; Tensor:\n    r\"\"\"Forward pass of the RelativePositionBias.\n\n    Args:\n        num_queries (int): The number of queries.\n        num_keys (int): The number of keys.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    i, j, device = num_queries, num_keys, self.relative_attention_bias.weight.device\n    q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n    k_pos = torch.arange(j, dtype=torch.long, device=device)\n    rel_pos = rearrange(k_pos, \"j -&gt; 1 j\") - rearrange(q_pos, \"i -&gt; i 1\")\n\n    relative_position_bucket = self._relative_position_bucket(\n        rel_pos, num_buckets=self.num_buckets, max_distance=self.max_distance,\n    )\n\n    bias = self.relative_attention_bias(relative_position_bucket)\n    bias = rearrange(bias, \"m n h -&gt; 1 h m n\")\n    return bias\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/attention/#models.tts.styledtts2.diffusion.attention.FeedForward","title":"<code>FeedForward(features, multiplier)</code>","text":"<p>Creates a feed-forward neural network with GELU activation in the middle layer.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>The number of input and output features.</p> required <code>multiplier</code> <code>int</code> <p>The factor to multiply the number of features to get the number of features in the middle layer.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: A feed-forward neural network module.</p> Source code in <code>models/tts/styledtts2/diffusion/attention.py</code> <pre><code>def FeedForward(features: int, multiplier: int) -&gt; nn.Module:\n    r\"\"\"Creates a feed-forward neural network with GELU activation in the middle layer.\n\n    Args:\n        features (int): The number of input and output features.\n        multiplier (int): The factor to multiply the number of features to get the number of features in the middle layer.\n\n    Returns:\n        nn.Module: A feed-forward neural network module.\n    \"\"\"\n    mid_features = features * multiplier\n    return nn.Sequential(\n        nn.Linear(in_features=features, out_features=mid_features),\n        nn.GELU(),\n        nn.Linear(in_features=mid_features, out_features=features),\n    )\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/","title":"Diffusion","text":""},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.Diffusion","title":"<code>Diffusion</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base class for diffusion models.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>class Diffusion(nn.Module):\n    r\"\"\"Base class for diffusion models.\"\"\"\n\n    alias: str = \"\"\n\n    def denoise_fn(\n        self,\n        x_noisy: Tensor,\n        sigmas: Optional[Tensor] = None,\n        sigma: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Tensor:\n        r\"\"\"Denoises the input tensor.\n\n        Args:\n            x_noisy (Tensor): The noisy input tensor.\n            sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n            sigma (Optional[float], optional): The noise level. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Raises:\n            NotImplementedError: This method should be overridden by subclasses.\n\n        Returns:\n            Tensor: The denoised tensor.\n        \"\"\"\n        raise NotImplementedError(\"Diffusion class missing denoise_fn\")\n\n    def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n        r\"\"\"Forward pass of the diffusion model.\n\n        Args:\n            x (Tensor): The input tensor.\n            noise (Tensor, optional): The noise tensor. Defaults to torch.tensor([]).\n            **kwargs: Additional keyword arguments.\n\n        Raises:\n            NotImplementedError: This method should be overridden by subclasses.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        raise NotImplementedError(\"Diffusion class missing forward function\")\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.Diffusion.denoise_fn","title":"<code>denoise_fn(x_noisy, sigmas=None, sigma=None, **kwargs)</code>","text":"<p>Denoises the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_noisy</code> <code>Tensor</code> <p>The noisy input tensor.</p> required <code>sigmas</code> <code>Optional[Tensor]</code> <p>The noise levels. Defaults to None.</p> <code>None</code> <code>sigma</code> <code>Optional[float]</code> <p>The noise level. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be overridden by subclasses.</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The denoised tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def denoise_fn(\n    self,\n    x_noisy: Tensor,\n    sigmas: Optional[Tensor] = None,\n    sigma: Optional[float] = None,\n    **kwargs,\n) -&gt; Tensor:\n    r\"\"\"Denoises the input tensor.\n\n    Args:\n        x_noisy (Tensor): The noisy input tensor.\n        sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n        sigma (Optional[float], optional): The noise level. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Raises:\n        NotImplementedError: This method should be overridden by subclasses.\n\n    Returns:\n        Tensor: The denoised tensor.\n    \"\"\"\n    raise NotImplementedError(\"Diffusion class missing denoise_fn\")\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.Diffusion.forward","title":"<code>forward(x, noise=None, **kwargs)</code>","text":"<p>Forward pass of the diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>noise</code> <code>Tensor</code> <p>The noise tensor. Defaults to torch.tensor([]).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be overridden by subclasses.</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n    r\"\"\"Forward pass of the diffusion model.\n\n    Args:\n        x (Tensor): The input tensor.\n        noise (Tensor, optional): The noise tensor. Defaults to torch.tensor([]).\n        **kwargs: Additional keyword arguments.\n\n    Raises:\n        NotImplementedError: This method should be overridden by subclasses.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    raise NotImplementedError(\"Diffusion class missing forward function\")\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion","title":"<code>KDiffusion</code>","text":"<p>             Bases: <code>Diffusion</code></p> <p>Elucidated Diffusion (Karras et al. 2022): https://arxiv.org/abs/2206.00364</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>class KDiffusion(Diffusion):\n    r\"\"\"Elucidated Diffusion (Karras et al. 2022): https://arxiv.org/abs/2206.00364\"\"\"\n\n    alias = \"k\"\n\n    def __init__(\n        self,\n        net: nn.Module,\n        *,\n        sigma_distribution: Distribution,\n        sigma_data: float,  # data distribution standard deviation\n        dynamic_threshold: float = 0.0,\n    ):\n        r\"\"\"Initialize the KDiffusion with a network, a sigma distribution, sigma data, and a dynamic threshold.\n\n        Args:\n            net (nn.Module): The network module.\n            sigma_distribution (Distribution): The sigma distribution.\n            sigma_data (float): The data distribution standard deviation.\n            dynamic_threshold (float, optional): The dynamic threshold. Defaults to 0.0.\n        \"\"\"\n        super().__init__()\n        self.net = net\n        self.sigma_data = sigma_data\n        self.sigma_distribution = sigma_distribution\n        self.dynamic_threshold = dynamic_threshold\n\n    def get_scale_weights(self, sigmas: Tensor) -&gt; Tuple[Tensor, ...]:\n        r\"\"\"Get scale weights based on the input sigmas.\n\n        Args:\n            sigmas (Tensor): The input sigmas.\n\n        Returns:\n            Tuple[Tensor, ...]: The scale weights.\n        \"\"\"\n        sigma_data = self.sigma_data\n\n        c_noise = torch.log(sigmas) * 0.25\n        sigmas = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n        c_skip = (sigma_data ** 2) / (sigmas ** 2 + sigma_data ** 2)\n        c_out = sigmas * sigma_data * (sigma_data ** 2 + sigmas ** 2) ** -0.5\n        c_in = (sigmas ** 2 + sigma_data ** 2) ** -0.5\n\n        return c_skip, c_out, c_in, c_noise\n\n    def denoise_fn(\n        self,\n        x_noisy: Tensor,\n        sigmas: Optional[Tensor] = None,\n        sigma: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Tensor:\n        r\"\"\"Denoise the input tensor.\n\n        Args:\n            x_noisy (Tensor): The noisy input tensor.\n            sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n            sigma (Optional[float], optional): The noise level. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The denoised tensor.\n        \"\"\"\n        batch_size, device = x_noisy.shape[0], x_noisy.device\n\n        sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n\n        # Predict network output and add skip connection\n        c_skip, c_out, c_in, c_noise = self.get_scale_weights(sigmas)\n\n        x_pred = self.net(c_in * x_noisy, c_noise, **kwargs)\n        x_denoised = c_skip * x_noisy + c_out * x_pred\n\n        return x_denoised\n\n    def loss_weight(self, sigmas: Tensor) -&gt; Tensor:\n        r\"\"\"Computes weight depending on data distribution.\n\n        Args:\n            sigmas (Tensor): The input sigmas.\n\n        Returns:\n            Tensor: The loss weight.\n        \"\"\"\n        return (sigmas ** 2 + self.sigma_data ** 2) * (sigmas * self.sigma_data) ** -2\n\n    def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n        r\"\"\"Forward pass of the KDiffusion model.\n\n        Args:\n            x (Tensor): The input tensor.\n            noise (Optional[Tensor], optional): The noise tensor. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        batch_size, device = x.shape[0], x.device\n\n        # Sample amount of noise to add for each batch element\n        sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n        sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n        # Add noise to input\n        noise = default(noise, torch.randn_like(x)).to(device=device)\n        x_noisy = x + sigmas_padded * noise\n\n        # Compute denoised values\n        x_denoised = self.denoise_fn(x_noisy, sigmas=sigmas, **kwargs)\n\n        # Compute weighted loss\n        losses = F.mse_loss(x_denoised, x, reduction=\"none\")\n        losses = reduce(losses, \"b ... -&gt; b\", \"mean\")\n        losses = losses * self.loss_weight(sigmas)\n        loss = losses.mean()\n        return loss\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion.__init__","title":"<code>__init__(net, *, sigma_distribution, sigma_data, dynamic_threshold=0.0)</code>","text":"<p>Initialize the KDiffusion with a network, a sigma distribution, sigma data, and a dynamic threshold.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The network module.</p> required <code>sigma_distribution</code> <code>Distribution</code> <p>The sigma distribution.</p> required <code>sigma_data</code> <code>float</code> <p>The data distribution standard deviation.</p> required <code>dynamic_threshold</code> <code>float</code> <p>The dynamic threshold. Defaults to 0.0.</p> <code>0.0</code> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def __init__(\n    self,\n    net: nn.Module,\n    *,\n    sigma_distribution: Distribution,\n    sigma_data: float,  # data distribution standard deviation\n    dynamic_threshold: float = 0.0,\n):\n    r\"\"\"Initialize the KDiffusion with a network, a sigma distribution, sigma data, and a dynamic threshold.\n\n    Args:\n        net (nn.Module): The network module.\n        sigma_distribution (Distribution): The sigma distribution.\n        sigma_data (float): The data distribution standard deviation.\n        dynamic_threshold (float, optional): The dynamic threshold. Defaults to 0.0.\n    \"\"\"\n    super().__init__()\n    self.net = net\n    self.sigma_data = sigma_data\n    self.sigma_distribution = sigma_distribution\n    self.dynamic_threshold = dynamic_threshold\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion.denoise_fn","title":"<code>denoise_fn(x_noisy, sigmas=None, sigma=None, **kwargs)</code>","text":"<p>Denoise the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_noisy</code> <code>Tensor</code> <p>The noisy input tensor.</p> required <code>sigmas</code> <code>Optional[Tensor]</code> <p>The noise levels. Defaults to None.</p> <code>None</code> <code>sigma</code> <code>Optional[float]</code> <p>The noise level. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The denoised tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def denoise_fn(\n    self,\n    x_noisy: Tensor,\n    sigmas: Optional[Tensor] = None,\n    sigma: Optional[float] = None,\n    **kwargs,\n) -&gt; Tensor:\n    r\"\"\"Denoise the input tensor.\n\n    Args:\n        x_noisy (Tensor): The noisy input tensor.\n        sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n        sigma (Optional[float], optional): The noise level. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The denoised tensor.\n    \"\"\"\n    batch_size, device = x_noisy.shape[0], x_noisy.device\n\n    sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n\n    # Predict network output and add skip connection\n    c_skip, c_out, c_in, c_noise = self.get_scale_weights(sigmas)\n\n    x_pred = self.net(c_in * x_noisy, c_noise, **kwargs)\n    x_denoised = c_skip * x_noisy + c_out * x_pred\n\n    return x_denoised\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion.forward","title":"<code>forward(x, noise=None, **kwargs)</code>","text":"<p>Forward pass of the KDiffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>noise</code> <code>Optional[Tensor]</code> <p>The noise tensor. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n    r\"\"\"Forward pass of the KDiffusion model.\n\n    Args:\n        x (Tensor): The input tensor.\n        noise (Optional[Tensor], optional): The noise tensor. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    batch_size, device = x.shape[0], x.device\n\n    # Sample amount of noise to add for each batch element\n    sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n    sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n    # Add noise to input\n    noise = default(noise, torch.randn_like(x)).to(device=device)\n    x_noisy = x + sigmas_padded * noise\n\n    # Compute denoised values\n    x_denoised = self.denoise_fn(x_noisy, sigmas=sigmas, **kwargs)\n\n    # Compute weighted loss\n    losses = F.mse_loss(x_denoised, x, reduction=\"none\")\n    losses = reduce(losses, \"b ... -&gt; b\", \"mean\")\n    losses = losses * self.loss_weight(sigmas)\n    loss = losses.mean()\n    return loss\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion.get_scale_weights","title":"<code>get_scale_weights(sigmas)</code>","text":"<p>Get scale weights based on the input sigmas.</p> <p>Parameters:</p> Name Type Description Default <code>sigmas</code> <code>Tensor</code> <p>The input sigmas.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, ...]</code> <p>Tuple[Tensor, ...]: The scale weights.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def get_scale_weights(self, sigmas: Tensor) -&gt; Tuple[Tensor, ...]:\n    r\"\"\"Get scale weights based on the input sigmas.\n\n    Args:\n        sigmas (Tensor): The input sigmas.\n\n    Returns:\n        Tuple[Tensor, ...]: The scale weights.\n    \"\"\"\n    sigma_data = self.sigma_data\n\n    c_noise = torch.log(sigmas) * 0.25\n    sigmas = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n    c_skip = (sigma_data ** 2) / (sigmas ** 2 + sigma_data ** 2)\n    c_out = sigmas * sigma_data * (sigma_data ** 2 + sigmas ** 2) ** -0.5\n    c_in = (sigmas ** 2 + sigma_data ** 2) ** -0.5\n\n    return c_skip, c_out, c_in, c_noise\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.KDiffusion.loss_weight","title":"<code>loss_weight(sigmas)</code>","text":"<p>Computes weight depending on data distribution.</p> <p>Parameters:</p> Name Type Description Default <code>sigmas</code> <code>Tensor</code> <p>The input sigmas.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The loss weight.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def loss_weight(self, sigmas: Tensor) -&gt; Tensor:\n    r\"\"\"Computes weight depending on data distribution.\n\n    Args:\n        sigmas (Tensor): The input sigmas.\n\n    Returns:\n        Tensor: The loss weight.\n    \"\"\"\n    return (sigmas ** 2 + self.sigma_data ** 2) * (sigmas * self.sigma_data) ** -2\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VDiffusion","title":"<code>VDiffusion</code>","text":"<p>             Bases: <code>Diffusion</code></p> <p>VDiffusion class that extends the base Diffusion class.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>class VDiffusion(Diffusion):\n    r\"\"\"VDiffusion class that extends the base Diffusion class.\"\"\"\n\n    alias = \"v\"\n\n    def __init__(self, net: nn.Module, *, sigma_distribution: Distribution):\n        r\"\"\"Initialize the VDiffusion with a network and a sigma distribution.\n\n        Args:\n            net (nn.Module): The network module.\n            sigma_distribution (Distribution): The sigma distribution.\n        \"\"\"\n        super().__init__()\n        self.net = net\n        self.sigma_distribution = sigma_distribution\n\n    def get_alpha_beta(self, sigmas: Tensor) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Get alpha and beta values based on the input sigmas.\n\n        Args:\n            sigmas (Tensor): The input sigmas.\n\n        Returns:\n            Tuple[Tensor, Tensor]: The alpha and beta values.\n        \"\"\"\n        angle = sigmas * pi / 2\n        alpha = torch.cos(angle)\n        beta = torch.sin(angle)\n        return alpha, beta\n\n    def denoise_fn(\n        self,\n        x_noisy: Tensor,\n        sigmas: Optional[Tensor] = None,\n        sigma: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Tensor:\n        r\"\"\"Denoise the input tensor.\n\n        Args:\n            x_noisy (Tensor): The noisy input tensor.\n            sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n            sigma (Optional[float], optional): The noise level. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The denoised tensor.\n        \"\"\"\n        batch_size, device = x_noisy.shape[0], x_noisy.device\n        sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n        return self.net(x_noisy, sigmas, **kwargs)\n\n    def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n        r\"\"\"Forward pass of the VDiffusion model.\n\n        Args:\n            x (Tensor): The input tensor.\n            noise (Tensor, optional): The noise tensor. Defaults to torch.tensor([]).\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        batch_size, device = x.shape[0], x.device\n\n        # Sample amount of noise to add for each batch element\n        sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n        sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n        # Get noise\n        noise = default(noise, torch.randn_like(x)).to(device=device)\n\n        # Combine input and noise weighted by half-circle\n        alpha, beta = self.get_alpha_beta(sigmas_padded)\n        x_noisy = x * alpha + noise * beta\n        x_target = noise * alpha - x * beta\n\n        # Denoise and return loss\n        x_denoised = self.denoise_fn(x_noisy, sigmas, **kwargs)\n        return F.mse_loss(x_denoised, x_target)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VDiffusion.__init__","title":"<code>__init__(net, *, sigma_distribution)</code>","text":"<p>Initialize the VDiffusion with a network and a sigma distribution.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The network module.</p> required <code>sigma_distribution</code> <code>Distribution</code> <p>The sigma distribution.</p> required Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def __init__(self, net: nn.Module, *, sigma_distribution: Distribution):\n    r\"\"\"Initialize the VDiffusion with a network and a sigma distribution.\n\n    Args:\n        net (nn.Module): The network module.\n        sigma_distribution (Distribution): The sigma distribution.\n    \"\"\"\n    super().__init__()\n    self.net = net\n    self.sigma_distribution = sigma_distribution\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VDiffusion.denoise_fn","title":"<code>denoise_fn(x_noisy, sigmas=None, sigma=None, **kwargs)</code>","text":"<p>Denoise the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_noisy</code> <code>Tensor</code> <p>The noisy input tensor.</p> required <code>sigmas</code> <code>Optional[Tensor]</code> <p>The noise levels. Defaults to None.</p> <code>None</code> <code>sigma</code> <code>Optional[float]</code> <p>The noise level. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The denoised tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def denoise_fn(\n    self,\n    x_noisy: Tensor,\n    sigmas: Optional[Tensor] = None,\n    sigma: Optional[float] = None,\n    **kwargs,\n) -&gt; Tensor:\n    r\"\"\"Denoise the input tensor.\n\n    Args:\n        x_noisy (Tensor): The noisy input tensor.\n        sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n        sigma (Optional[float], optional): The noise level. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The denoised tensor.\n    \"\"\"\n    batch_size, device = x_noisy.shape[0], x_noisy.device\n    sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n    return self.net(x_noisy, sigmas, **kwargs)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VDiffusion.forward","title":"<code>forward(x, noise=None, **kwargs)</code>","text":"<p>Forward pass of the VDiffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>noise</code> <code>Tensor</code> <p>The noise tensor. Defaults to torch.tensor([]).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n    r\"\"\"Forward pass of the VDiffusion model.\n\n    Args:\n        x (Tensor): The input tensor.\n        noise (Tensor, optional): The noise tensor. Defaults to torch.tensor([]).\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    batch_size, device = x.shape[0], x.device\n\n    # Sample amount of noise to add for each batch element\n    sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n    sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n    # Get noise\n    noise = default(noise, torch.randn_like(x)).to(device=device)\n\n    # Combine input and noise weighted by half-circle\n    alpha, beta = self.get_alpha_beta(sigmas_padded)\n    x_noisy = x * alpha + noise * beta\n    x_target = noise * alpha - x * beta\n\n    # Denoise and return loss\n    x_denoised = self.denoise_fn(x_noisy, sigmas, **kwargs)\n    return F.mse_loss(x_denoised, x_target)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VDiffusion.get_alpha_beta","title":"<code>get_alpha_beta(sigmas)</code>","text":"<p>Get alpha and beta values based on the input sigmas.</p> <p>Parameters:</p> Name Type Description Default <code>sigmas</code> <code>Tensor</code> <p>The input sigmas.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor]: The alpha and beta values.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def get_alpha_beta(self, sigmas: Tensor) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Get alpha and beta values based on the input sigmas.\n\n    Args:\n        sigmas (Tensor): The input sigmas.\n\n    Returns:\n        Tuple[Tensor, Tensor]: The alpha and beta values.\n    \"\"\"\n    angle = sigmas * pi / 2\n    alpha = torch.cos(angle)\n    beta = torch.sin(angle)\n    return alpha, beta\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion","title":"<code>VKDiffusion</code>","text":"<p>             Bases: <code>Diffusion</code></p> <p>VKDiffusion class that extends the base Diffusion class.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>class VKDiffusion(Diffusion):\n    r\"\"\"VKDiffusion class that extends the base Diffusion class.\"\"\"\n\n    alias = \"vk\"\n\n    def __init__(self, net: nn.Module, *, sigma_distribution: Distribution):\n        r\"\"\"Initialize the VKDiffusion with a network and a sigma distribution.\n\n        Args:\n            net (nn.Module): The network module.\n            sigma_distribution (Distribution): The sigma distribution.\n        \"\"\"\n        super().__init__()\n        self.net = net\n        self.sigma_distribution = sigma_distribution\n\n    def get_scale_weights(self, sigmas: Tensor) -&gt; Tuple[Tensor, ...]:\n        r\"\"\"Get scale weights based on the input sigmas.\n\n        Args:\n            sigmas (Tensor): The input sigmas.\n\n        Returns:\n            Tuple[Tensor, ...]: The scale weights.\n        \"\"\"\n        sigma_data = 1.0\n        sigmas = rearrange(sigmas, \"b -&gt; b 1 1\")\n        c_skip = (sigma_data ** 2) / (sigmas ** 2 + sigma_data ** 2)\n        c_out = -sigmas * sigma_data * (sigma_data ** 2 + sigmas ** 2) ** -0.5\n        c_in = (sigmas ** 2 + sigma_data ** 2) ** -0.5\n        return c_skip, c_out, c_in\n\n    def sigma_to_t(self, sigmas: Tensor) -&gt; Tensor:\n        r\"\"\"Convert sigmas to t.\n\n        Args:\n            sigmas (Tensor): The input sigmas.\n\n        Returns:\n            Tensor: The converted t.\n        \"\"\"\n        return sigmas.atan() / pi * 2\n\n    def t_to_sigma(self, t: Tensor) -&gt; Tensor:\n        r\"\"\"Convert t to sigmas.\n\n        Args:\n            t (Tensor): The input t.\n\n        Returns:\n            Tensor: The converted sigmas.\n        \"\"\"\n        return (t * pi / 2).tan()\n\n    def denoise_fn(\n        self,\n        x_noisy: Tensor,\n        sigmas: Optional[Tensor] = None,\n        sigma: Optional[float] = None,\n        **kwargs,\n    ) -&gt; Tensor:\n        r\"\"\"Denoise the input tensor.\n\n        Args:\n            x_noisy (Tensor): The noisy input tensor.\n            sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n            sigma (Optional[float], optional): The noise level. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The denoised tensor.\n        \"\"\"\n        batch_size, device = x_noisy.shape[0], x_noisy.device\n        sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n\n        # Predict network output and add skip connection\n        c_skip, c_out, c_in = self.get_scale_weights(sigmas)\n        x_pred = self.net(c_in * x_noisy, self.sigma_to_t(sigmas), **kwargs)\n        x_denoised = c_skip * x_noisy + c_out * x_pred\n        return x_denoised\n\n    def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n        r\"\"\"Forward pass of the VKDiffusion model.\n\n        Args:\n            x (Tensor): The input tensor.\n            noise (Optional[Tensor], optional): The noise tensor. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        batch_size, device = x.shape[0], x.device\n\n        # Sample amount of noise to add for each batch element\n        sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n        sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n        # Add noise to input\n        noise = default(noise, torch.randn_like(x)).to(device=device)\n        x_noisy = x + sigmas_padded * noise\n\n        # Compute model output\n        c_skip, c_out, c_in = self.get_scale_weights(sigmas)\n        x_pred = self.net(c_in * x_noisy, self.sigma_to_t(sigmas), **kwargs)\n\n        # Compute v-objective target\n        v_target = (x - c_skip * x_noisy) / (c_out + 1e-7)\n\n        # Compute loss\n        loss = F.mse_loss(x_pred, v_target)\n        return loss\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.__init__","title":"<code>__init__(net, *, sigma_distribution)</code>","text":"<p>Initialize the VKDiffusion with a network and a sigma distribution.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The network module.</p> required <code>sigma_distribution</code> <code>Distribution</code> <p>The sigma distribution.</p> required Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def __init__(self, net: nn.Module, *, sigma_distribution: Distribution):\n    r\"\"\"Initialize the VKDiffusion with a network and a sigma distribution.\n\n    Args:\n        net (nn.Module): The network module.\n        sigma_distribution (Distribution): The sigma distribution.\n    \"\"\"\n    super().__init__()\n    self.net = net\n    self.sigma_distribution = sigma_distribution\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.denoise_fn","title":"<code>denoise_fn(x_noisy, sigmas=None, sigma=None, **kwargs)</code>","text":"<p>Denoise the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_noisy</code> <code>Tensor</code> <p>The noisy input tensor.</p> required <code>sigmas</code> <code>Optional[Tensor]</code> <p>The noise levels. Defaults to None.</p> <code>None</code> <code>sigma</code> <code>Optional[float]</code> <p>The noise level. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The denoised tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def denoise_fn(\n    self,\n    x_noisy: Tensor,\n    sigmas: Optional[Tensor] = None,\n    sigma: Optional[float] = None,\n    **kwargs,\n) -&gt; Tensor:\n    r\"\"\"Denoise the input tensor.\n\n    Args:\n        x_noisy (Tensor): The noisy input tensor.\n        sigmas (Optional[Tensor], optional): The noise levels. Defaults to None.\n        sigma (Optional[float], optional): The noise level. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The denoised tensor.\n    \"\"\"\n    batch_size, device = x_noisy.shape[0], x_noisy.device\n    sigmas = to_batch(batch_size, x=sigma, xs=sigmas).to(device=device)\n\n    # Predict network output and add skip connection\n    c_skip, c_out, c_in = self.get_scale_weights(sigmas)\n    x_pred = self.net(c_in * x_noisy, self.sigma_to_t(sigmas), **kwargs)\n    x_denoised = c_skip * x_noisy + c_out * x_pred\n    return x_denoised\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.forward","title":"<code>forward(x, noise=None, **kwargs)</code>","text":"<p>Forward pass of the VKDiffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>noise</code> <code>Optional[Tensor]</code> <p>The noise tensor. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def forward(self, x: Tensor, noise: Optional[Tensor] = None, **kwargs) -&gt; Tensor:\n    r\"\"\"Forward pass of the VKDiffusion model.\n\n    Args:\n        x (Tensor): The input tensor.\n        noise (Optional[Tensor], optional): The noise tensor. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    batch_size, device = x.shape[0], x.device\n\n    # Sample amount of noise to add for each batch element\n    sigmas = self.sigma_distribution(num_samples=batch_size).to(device=device)\n    sigmas_padded = rearrange(sigmas, \"b -&gt; b 1 1\")\n\n    # Add noise to input\n    noise = default(noise, torch.randn_like(x)).to(device=device)\n    x_noisy = x + sigmas_padded * noise\n\n    # Compute model output\n    c_skip, c_out, c_in = self.get_scale_weights(sigmas)\n    x_pred = self.net(c_in * x_noisy, self.sigma_to_t(sigmas), **kwargs)\n\n    # Compute v-objective target\n    v_target = (x - c_skip * x_noisy) / (c_out + 1e-7)\n\n    # Compute loss\n    loss = F.mse_loss(x_pred, v_target)\n    return loss\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.get_scale_weights","title":"<code>get_scale_weights(sigmas)</code>","text":"<p>Get scale weights based on the input sigmas.</p> <p>Parameters:</p> Name Type Description Default <code>sigmas</code> <code>Tensor</code> <p>The input sigmas.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, ...]</code> <p>Tuple[Tensor, ...]: The scale weights.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def get_scale_weights(self, sigmas: Tensor) -&gt; Tuple[Tensor, ...]:\n    r\"\"\"Get scale weights based on the input sigmas.\n\n    Args:\n        sigmas (Tensor): The input sigmas.\n\n    Returns:\n        Tuple[Tensor, ...]: The scale weights.\n    \"\"\"\n    sigma_data = 1.0\n    sigmas = rearrange(sigmas, \"b -&gt; b 1 1\")\n    c_skip = (sigma_data ** 2) / (sigmas ** 2 + sigma_data ** 2)\n    c_out = -sigmas * sigma_data * (sigma_data ** 2 + sigmas ** 2) ** -0.5\n    c_in = (sigmas ** 2 + sigma_data ** 2) ** -0.5\n    return c_skip, c_out, c_in\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.sigma_to_t","title":"<code>sigma_to_t(sigmas)</code>","text":"<p>Convert sigmas to t.</p> <p>Parameters:</p> Name Type Description Default <code>sigmas</code> <code>Tensor</code> <p>The input sigmas.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The converted t.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def sigma_to_t(self, sigmas: Tensor) -&gt; Tensor:\n    r\"\"\"Convert sigmas to t.\n\n    Args:\n        sigmas (Tensor): The input sigmas.\n\n    Returns:\n        Tensor: The converted t.\n    \"\"\"\n    return sigmas.atan() / pi * 2\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.VKDiffusion.t_to_sigma","title":"<code>t_to_sigma(t)</code>","text":"<p>Convert t to sigmas.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input t.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The converted sigmas.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def t_to_sigma(self, t: Tensor) -&gt; Tensor:\n    r\"\"\"Convert t to sigmas.\n\n    Args:\n        t (Tensor): The input t.\n\n    Returns:\n        Tensor: The converted sigmas.\n    \"\"\"\n    return (t * pi / 2).tan()\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.clip","title":"<code>clip(x, dynamic_threshold=0.0)</code>","text":"<p>Clips the input tensor between -1.0 and 1.0, or between -scale and scale if dynamic_threshold is not 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dynamic_threshold</code> <code>float</code> <p>The dynamic threshold for clipping. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The clipped tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def clip(x: Tensor, dynamic_threshold: float = 0.0):\n    r\"\"\"Clips the input tensor between -1.0 and 1.0, or between -scale and scale if dynamic_threshold is not 0.0.\n\n    Args:\n        x (Tensor): The input tensor.\n        dynamic_threshold (float, optional): The dynamic threshold for clipping. Defaults to 0.0.\n\n    Returns:\n        Tensor: The clipped tensor.\n    \"\"\"\n    if dynamic_threshold == 0.0:\n        return x.clamp(-1.0, 1.0)\n    else:\n        # Dynamic thresholding\n        # Find dynamic threshold quantile for each batch\n        x_flat = rearrange(x, \"b ... -&gt; b (...)\")\n        scale = torch.quantile(x_flat.abs(), dynamic_threshold, dim=-1)\n\n        # Clamp to a min of 1.0\n        scale.clamp_(min=1.0)\n\n        # Clamp all values and scale\n        scale = pad_dims(scale, ndim=x.ndim - scale.ndim)\n        return x.clamp(-scale, scale) / scale\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.pad_dims","title":"<code>pad_dims(x, ndim)</code>","text":"<p>Pads additional dimensions to the right of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to add.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The padded tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def pad_dims(x: Tensor, ndim: int) -&gt; Tensor:\n    r\"\"\"Pads additional dimensions to the right of the tensor.\n\n    Args:\n        x (Tensor): The input tensor.\n        ndim (int): The number of dimensions to add.\n\n    Returns:\n        Tensor: The padded tensor.\n    \"\"\"\n    return x.view(*x.shape, *((1,) * ndim))\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/diffusion/#models.tts.styledtts2.diffusion.diffusion.to_batch","title":"<code>to_batch(batch_size, x=None, xs=None)</code>","text":"<p>Converts a scalar or a tensor to a batch of tensors.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>x</code> <code>Optional[float]</code> <p>The scalar to convert. Defaults to None.</p> <code>None</code> <code>xs</code> <code>Optional[Tensor]</code> <p>The tensor to convert. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The batch of tensors.</p> Source code in <code>models/tts/styledtts2/diffusion/diffusion.py</code> <pre><code>def to_batch(\n    batch_size: int,\n    x: Optional[float] = None,\n    xs: Optional[Tensor] = None,\n) -&gt; Tensor:\n    r\"\"\"Converts a scalar or a tensor to a batch of tensors.\n\n    Args:\n        batch_size (int): The batch size.\n        x (Optional[float], optional): The scalar to convert. Defaults to None.\n        xs (Optional[Tensor], optional): The tensor to convert. Defaults to None.\n\n    Returns:\n        Tensor: The batch of tensors.\n    \"\"\"\n    assert exists(x) ^ exists(xs), \"Either x or xs must be provided\"\n    # If x provided use the same for all batch items\n    if exists(x):\n        xs = torch.full(size=(batch_size,), fill_value=x)\n    assert exists(xs)\n    return xs\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/","title":"Distributions","text":""},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.Distribution","title":"<code>Distribution</code>","text":"<p>Base class for all distributions.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>class Distribution:\n    r\"\"\"Base class for all distributions.\"\"\"\n\n    def __call__(self, num_samples: int) -&gt; Tensor:\n        r\"\"\"Generate a number of samples from the distribution.\n\n        Args:\n            num_samples (int): The number of samples to generate.\n\n        Raises:\n            NotImplementedError: This method should be overridden by subclasses.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.Distribution.__call__","title":"<code>__call__(num_samples)</code>","text":"<p>Generate a number of samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be overridden by subclasses.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __call__(self, num_samples: int) -&gt; Tensor:\n    r\"\"\"Generate a number of samples from the distribution.\n\n    Args:\n        num_samples (int): The number of samples to generate.\n\n    Raises:\n        NotImplementedError: This method should be overridden by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.LogNormalDistribution","title":"<code>LogNormalDistribution</code>","text":"<p>             Bases: <code>Distribution</code></p> <p>Log-normal distribution.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>class LogNormalDistribution(Distribution):\n    r\"\"\"Log-normal distribution.\"\"\"\n\n    def __init__(self, mean: float, std: float):\n        r\"\"\"Initialize the distribution with a mean and standard deviation.\n\n        Args:\n            mean (float): The mean of the log-normal distribution.\n            std (float): The standard deviation of the log-normal distribution.\n        \"\"\"\n        self.mean = mean\n        self.std = std\n\n    def __call__(\n        self, num_samples: int,\n    ) -&gt; Tensor:\n        r\"\"\"Generate a number of samples from the log-normal distribution.\n\n        Args:\n            num_samples (int): The number of samples to generate.\n\n        Returns:\n            Tensor: A tensor of samples from the log-normal distribution.\n        \"\"\"\n        normal = self.mean + self.std * torch.randn((num_samples,))\n        return normal.exp()\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.LogNormalDistribution.__call__","title":"<code>__call__(num_samples)</code>","text":"<p>Generate a number of samples from the log-normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of samples from the log-normal distribution.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __call__(\n    self, num_samples: int,\n) -&gt; Tensor:\n    r\"\"\"Generate a number of samples from the log-normal distribution.\n\n    Args:\n        num_samples (int): The number of samples to generate.\n\n    Returns:\n        Tensor: A tensor of samples from the log-normal distribution.\n    \"\"\"\n    normal = self.mean + self.std * torch.randn((num_samples,))\n    return normal.exp()\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.LogNormalDistribution.__init__","title":"<code>__init__(mean, std)</code>","text":"<p>Initialize the distribution with a mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>The mean of the log-normal distribution.</p> required <code>std</code> <code>float</code> <p>The standard deviation of the log-normal distribution.</p> required Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __init__(self, mean: float, std: float):\n    r\"\"\"Initialize the distribution with a mean and standard deviation.\n\n    Args:\n        mean (float): The mean of the log-normal distribution.\n        std (float): The standard deviation of the log-normal distribution.\n    \"\"\"\n    self.mean = mean\n    self.std = std\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.UniformDistribution","title":"<code>UniformDistribution</code>","text":"<p>             Bases: <code>Distribution</code></p> <p>Uniform distribution.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>class UniformDistribution(Distribution):\n    r\"\"\"Uniform distribution.\"\"\"\n\n    def __call__(self, num_samples: int):\n        r\"\"\"Generate a number of samples from the uniform distribution.\n\n        Args:\n            num_samples (int): The number of samples to generate.\n\n        Returns:\n            Tensor: A tensor of samples from the uniform distribution.\n        \"\"\"\n        return torch.rand(num_samples)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.UniformDistribution.__call__","title":"<code>__call__(num_samples)</code>","text":"<p>Generate a number of samples from the uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>A tensor of samples from the uniform distribution.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __call__(self, num_samples: int):\n    r\"\"\"Generate a number of samples from the uniform distribution.\n\n    Args:\n        num_samples (int): The number of samples to generate.\n\n    Returns:\n        Tensor: A tensor of samples from the uniform distribution.\n    \"\"\"\n    return torch.rand(num_samples)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.VKDistribution","title":"<code>VKDistribution</code>","text":"<p>             Bases: <code>Distribution</code></p> <p>VK distribution. The class is implementing a variant of a distribution that is based on the Von Mises distribution, which is a continuous probability distribution on the circle (it's often used as a circular version of the normal distribution).</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>class VKDistribution(Distribution):\n    r\"\"\"VK distribution. The class is implementing a variant of a distribution that is based on the Von Mises distribution, which is a continuous probability distribution on the circle (it's often used as a circular version of the normal distribution).\"\"\"\n\n    def __init__(\n        self,\n        min_value: float = 0.0,\n        max_value: float = float(\"inf\"),\n        sigma_data: float = 1.0,\n    ):\n        r\"\"\"Initialize the distribution with a minimum value, maximum value, and sigma data.\n\n        Args:\n            min_value (float): The minimum value for the inverse CDF. Defaults to 0.0.\n            max_value (float): The maximum value for the inverse CDF. Defaults to infinity.\n            sigma_data (float): The sigma data of the VK distribution. Defaults to 1.0.\n        \"\"\"\n        self.min_value = min_value\n        self.max_value = max_value\n        self.sigma_data = sigma_data\n\n    def __call__(\n        self, num_samples: int,\n    ) -&gt; Tensor:\n        r\"\"\"Generate a number of samples from the VK distribution.\n\n        Args:\n            num_samples (int): The number of samples to generate.\n\n        Returns:\n            Tensor: A tensor of samples from the VK distribution.\n        \"\"\"\n        sigma_data = self.sigma_data\n        min_cdf = atan(self.min_value / sigma_data) * 2 / pi\n        max_cdf = atan(self.max_value / sigma_data) * 2 / pi\n        u = (max_cdf - min_cdf) * torch.randn((num_samples,)) + min_cdf\n        return torch.tan(u * pi / 2) * sigma_data\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.VKDistribution.__call__","title":"<code>__call__(num_samples)</code>","text":"<p>Generate a number of samples from the VK distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of samples from the VK distribution.</p> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __call__(\n    self, num_samples: int,\n) -&gt; Tensor:\n    r\"\"\"Generate a number of samples from the VK distribution.\n\n    Args:\n        num_samples (int): The number of samples to generate.\n\n    Returns:\n        Tensor: A tensor of samples from the VK distribution.\n    \"\"\"\n    sigma_data = self.sigma_data\n    min_cdf = atan(self.min_value / sigma_data) * 2 / pi\n    max_cdf = atan(self.max_value / sigma_data) * 2 / pi\n    u = (max_cdf - min_cdf) * torch.randn((num_samples,)) + min_cdf\n    return torch.tan(u * pi / 2) * sigma_data\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/distributions/#models.tts.styledtts2.diffusion.distributions.VKDistribution.__init__","title":"<code>__init__(min_value=0.0, max_value=float('inf'), sigma_data=1.0)</code>","text":"<p>Initialize the distribution with a minimum value, maximum value, and sigma data.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>The minimum value for the inverse CDF. Defaults to 0.0.</p> <code>0.0</code> <code>max_value</code> <code>float</code> <p>The maximum value for the inverse CDF. Defaults to infinity.</p> <code>float('inf')</code> <code>sigma_data</code> <code>float</code> <p>The sigma data of the VK distribution. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>models/tts/styledtts2/diffusion/distributions.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0.0,\n    max_value: float = float(\"inf\"),\n    sigma_data: float = 1.0,\n):\n    r\"\"\"Initialize the distribution with a minimum value, maximum value, and sigma data.\n\n    Args:\n        min_value (float): The minimum value for the inverse CDF. Defaults to 0.0.\n        max_value (float): The maximum value for the inverse CDF. Defaults to infinity.\n        sigma_data (float): The sigma data of the VK distribution. Defaults to 1.0.\n    \"\"\"\n    self.min_value = min_value\n    self.max_value = max_value\n    self.sigma_data = sigma_data\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/","title":"Embeddings","text":""},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.FixedEmbedding","title":"<code>FixedEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Fixed Embedding class that creates a fixed embedding of a given maximum length and features.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>class FixedEmbedding(nn.Module):\n    r\"\"\"Fixed Embedding class that creates a fixed embedding of a given maximum length and features.\"\"\"\n\n    def __init__(self, max_length: int, features: int):\n        r\"\"\"Initialize the FixedEmbedding with a maximum length and features.\n\n        Args:\n            max_length (int): The maximum length of the embedding.\n            features (int): The number of features of the embedding.\n        \"\"\"\n        super().__init__()\n        self.max_length = max_length\n        self.embedding = nn.Embedding(max_length, features)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass of the FixedEmbedding.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        batch_size, length, device = *x.shape[0:2], x.device\n\n        assert length &lt;= self.max_length, \"Input sequence length must be &lt;= max_length\"\n\n        position = torch.arange(length, device=device)\n\n        fixed_embedding = self.embedding(position)\n        fixed_embedding = repeat(fixed_embedding, \"n d -&gt; b n d\", b=batch_size)\n\n        return fixed_embedding\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.FixedEmbedding.__init__","title":"<code>__init__(max_length, features)</code>","text":"<p>Initialize the FixedEmbedding with a maximum length and features.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>The maximum length of the embedding.</p> required <code>features</code> <code>int</code> <p>The number of features of the embedding.</p> required Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def __init__(self, max_length: int, features: int):\n    r\"\"\"Initialize the FixedEmbedding with a maximum length and features.\n\n    Args:\n        max_length (int): The maximum length of the embedding.\n        features (int): The number of features of the embedding.\n    \"\"\"\n    super().__init__()\n    self.max_length = max_length\n    self.embedding = nn.Embedding(max_length, features)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.FixedEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the FixedEmbedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass of the FixedEmbedding.\n\n    Args:\n        x (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    batch_size, length, device = *x.shape[0:2], x.device\n\n    assert length &lt;= self.max_length, \"Input sequence length must be &lt;= max_length\"\n\n    position = torch.arange(length, device=device)\n\n    fixed_embedding = self.embedding(position)\n    fixed_embedding = repeat(fixed_embedding, \"n d -&gt; b n d\", b=batch_size)\n\n    return fixed_embedding\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.LearnedPositionalEmbedding","title":"<code>LearnedPositionalEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Learned Positional Embedding class that creates a learned positional embedding of a given dimension. Used for continuous time.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>class LearnedPositionalEmbedding(nn.Module):\n    r\"\"\"Learned Positional Embedding class that creates a learned positional embedding of a given dimension. Used for continuous time.\"\"\"\n\n    def __init__(self, dim: int):\n        r\"\"\"Initialize the LearnedPositionalEmbedding with a dimension.\n\n        Args:\n            dim (int): The dimension of the embedding.\n        \"\"\"\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass of the LearnedPositionalEmbedding.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        x = rearrange(x, \"b -&gt; b 1\")\n\n        freqs = x * rearrange(self.weights, \"d -&gt; 1 d\") * 2 * pi\n\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n        fouriered = torch.cat((x, fouriered), dim=-1)\n\n        return fouriered\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.LearnedPositionalEmbedding.__init__","title":"<code>__init__(dim)</code>","text":"<p>Initialize the LearnedPositionalEmbedding with a dimension.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension of the embedding.</p> required Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def __init__(self, dim: int):\n    r\"\"\"Initialize the LearnedPositionalEmbedding with a dimension.\n\n    Args:\n        dim (int): The dimension of the embedding.\n    \"\"\"\n    super().__init__()\n    assert (dim % 2) == 0\n    half_dim = dim // 2\n    self.weights = nn.Parameter(torch.randn(half_dim))\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.LearnedPositionalEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the LearnedPositionalEmbedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass of the LearnedPositionalEmbedding.\n\n    Args:\n        x (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    x = rearrange(x, \"b -&gt; b 1\")\n\n    freqs = x * rearrange(self.weights, \"d -&gt; 1 d\") * 2 * pi\n\n    fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n    fouriered = torch.cat((x, fouriered), dim=-1)\n\n    return fouriered\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.SinusoidalEmbedding","title":"<code>SinusoidalEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>Sinusoidal Embedding class that creates a sinusoidal embedding of a given dimension.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>class SinusoidalEmbedding(nn.Module):\n    r\"\"\"Sinusoidal Embedding class that creates a sinusoidal embedding of a given dimension.\"\"\"\n\n    def __init__(self, dim: int):\n        r\"\"\"Initialize the SinusoidalEmbedding with a dimension.\n\n        Args:\n            dim (int): The dimension of the embedding.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        r\"\"\"Forward pass of the SinusoidalEmbedding.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n        device, half_dim = x.device, self.dim // 2\n\n        emb = torch.tensor(log(10000) / (half_dim - 1), device=device)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = rearrange(x, \"i -&gt; i 1\") * rearrange(emb, \"j -&gt; 1 j\")\n\n        return torch.cat((emb.sin(), emb.cos()), dim=-1)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.SinusoidalEmbedding.__init__","title":"<code>__init__(dim)</code>","text":"<p>Initialize the SinusoidalEmbedding with a dimension.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension of the embedding.</p> required Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def __init__(self, dim: int):\n    r\"\"\"Initialize the SinusoidalEmbedding with a dimension.\n\n    Args:\n        dim (int): The dimension of the embedding.\n    \"\"\"\n    super().__init__()\n    self.dim = dim\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.SinusoidalEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SinusoidalEmbedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output tensor.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    r\"\"\"Forward pass of the SinusoidalEmbedding.\n\n    Args:\n        x (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The output tensor.\n    \"\"\"\n    device, half_dim = x.device, self.dim // 2\n\n    emb = torch.tensor(log(10000) / (half_dim - 1), device=device)\n    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n    emb = rearrange(x, \"i -&gt; i 1\") * rearrange(emb, \"j -&gt; 1 j\")\n\n    return torch.cat((emb.sin(), emb.cos()), dim=-1)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/embeddings/#models.tts.styledtts2.diffusion.embeddings.TimePositionalEmbedding","title":"<code>TimePositionalEmbedding(dim, out_features)</code>","text":"<p>Creates a time positional embedding of a given dimension and output features.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension of the embedding.</p> required <code>out_features</code> <code>int</code> <p>The number of output features.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: The time positional embedding module.</p> Source code in <code>models/tts/styledtts2/diffusion/embeddings.py</code> <pre><code>def TimePositionalEmbedding(dim: int, out_features: int) -&gt; nn.Module:\n    r\"\"\"Creates a time positional embedding of a given dimension and output features.\n\n    Args:\n        dim (int): The dimension of the embedding.\n        out_features (int): The number of output features.\n\n    Returns:\n        nn.Module: The time positional embedding module.\n    \"\"\"\n    return nn.Sequential(\n        LearnedPositionalEmbedding(dim),\n        nn.Linear(in_features=dim + 1, out_features=out_features),\n    )\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/","title":"Utils","text":""},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.closest_power_2","title":"<code>closest_power_2(x)</code>","text":"<p>Find the closest power of 2 to a given number.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The number to find the closest power of 2 to.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The closest power of 2 to the given number.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def closest_power_2(x: float) -&gt; int:\n    r\"\"\"Find the closest power of 2 to a given number.\n\n    Args:\n        x: The number to find the closest power of 2 to.\n\n    Returns:\n        The closest power of 2 to the given number.\n    \"\"\"\n    exponent = log2(x)\n    distance_fn = lambda z: abs(x - 2 ** z)\n    exponent_closest = min((floor(exponent), ceil(exponent)), key=distance_fn)\n    return 2 ** int(exponent_closest)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.default","title":"<code>default(val, d)</code>","text":"<p>Return the value if it exists, otherwise return the default value.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Optional[T]</code> <p>The value to check.</p> required <code>d</code> <code>Union[Callable[..., T], T]</code> <p>The default value to return if the value does not exist.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The value if it exists, otherwise the default value.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def default(val: Optional[T], d: Union[Callable[..., T], T]) -&gt; T:\n    r\"\"\"Return the value if it exists, otherwise return the default value.\n\n    Args:\n        val: The value to check.\n        d: The default value to return if the value does not exist.\n\n    Returns:\n        The value if it exists, otherwise the default value.\n    \"\"\"\n    if exists(val):\n        return val\n    if callable(d):\n        return d()\n    else:\n        return d\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.exists","title":"<code>exists(val)</code>","text":"<p>Check if a value is not None.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Optional[T]</code> <p>The value to check.</p> required <p>Returns:</p> Type Description <code>TypeGuard[T]</code> <p>True if the value is not None, False otherwise.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def exists(val: Optional[T]) -&gt; TypeGuard[T]:\n    r\"\"\"Check if a value is not None.\n\n    Args:\n        val: The value to check.\n\n    Returns:\n        True if the value is not None, False otherwise.\n    \"\"\"\n    return val is not None\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.group_dict_by_prefix","title":"<code>group_dict_by_prefix(prefix, d)</code>","text":"<p>Group a dictionary by keys that start with a given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix to group by.</p> required <code>d</code> <code>Dict</code> <p>The dictionary to group.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>A tuple of two dictionaries: one with keys that start with the prefix, and one with keys that do not.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def group_dict_by_prefix(prefix: str, d: Dict) -&gt; Tuple[Dict, Dict]:\n    r\"\"\"Group a dictionary by keys that start with a given prefix.\n\n    Args:\n        prefix: The prefix to group by.\n        d: The dictionary to group.\n\n    Returns:\n        A tuple of two dictionaries: one with keys that start with the prefix, and one with keys that do not.\n    \"\"\"\n    return_dicts: Tuple[Dict, Dict] = ({}, {})\n    for key in d:\n        no_prefix = int(not key.startswith(prefix))\n        return_dicts[no_prefix][key] = d[key]\n    return return_dicts\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.groupby","title":"<code>groupby(prefix, d, keep_prefix=False)</code>","text":"<p>Group a dictionary by keys that start with a given prefix and optionally remove the prefix from the keys.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix to group by.</p> required <code>d</code> <code>Dict</code> <p>The dictionary to group.</p> required <code>keep_prefix</code> <code>bool</code> <p>Whether to keep the prefix in the keys.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>A tuple of two dictionaries: one with keys that start with the prefix, and one with keys that do not.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def groupby(prefix: str, d: Dict, keep_prefix: bool = False) -&gt; Tuple[Dict, Dict]:\n    r\"\"\"Group a dictionary by keys that start with a given prefix and optionally remove the prefix from the keys.\n\n    Args:\n        prefix: The prefix to group by.\n        d: The dictionary to group.\n        keep_prefix: Whether to keep the prefix in the keys.\n\n    Returns:\n        A tuple of two dictionaries: one with keys that start with the prefix, and one with keys that do not.\n    \"\"\"\n    kwargs_with_prefix, kwargs = group_dict_by_prefix(prefix, d)\n    if keep_prefix:\n        return kwargs_with_prefix, kwargs\n    kwargs_no_prefix = {k[len(prefix) :]: v for k, v in kwargs_with_prefix.items()}\n    return kwargs_no_prefix, kwargs\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.iff","title":"<code>iff(condition, value)</code>","text":"<p>Return the value if the condition is True, None otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>bool</code> <p>The condition to check.</p> required <code>value</code> <code>T</code> <p>The value to return if the condition is True.</p> required <p>Returns:</p> Type Description <code>Optional[T]</code> <p>The value if the condition is True, None otherwise.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def iff(condition: bool, value: T) -&gt; Optional[T]:\n    r\"\"\"Return the value if the condition is True, None otherwise.\n\n    Args:\n        condition: The condition to check.\n        value: The value to return if the condition is True.\n\n    Returns:\n        The value if the condition is True, None otherwise.\n    \"\"\"\n    return value if condition else None\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.is_sequence","title":"<code>is_sequence(obj)</code>","text":"<p>Check if an object is a list or a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>T</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>TypeGuard[Union[list, tuple]]</code> <p>True if the object is a list or a tuple, False otherwise.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def is_sequence(obj: T) -&gt; TypeGuard[Union[list, tuple]]:\n    r\"\"\"Check if an object is a list or a tuple.\n\n    Args:\n        obj: The object to check.\n\n    Returns:\n        True if the object is a list or a tuple, False otherwise.\n    \"\"\"\n    return isinstance(obj, (list, tuple))\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.prefix_dict","title":"<code>prefix_dict(prefix, d)</code>","text":"<p>Add a prefix to all keys in a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix to add.</p> required <code>d</code> <code>Dict</code> <p>The dictionary to modify.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>The modified dictionary with the prefix added to all keys.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def prefix_dict(prefix: str, d: Dict) -&gt; Dict:\n    r\"\"\"Add a prefix to all keys in a dictionary.\n\n    Args:\n        prefix: The prefix to add.\n        d: The dictionary to modify.\n\n    Returns:\n        The modified dictionary with the prefix added to all keys.\n    \"\"\"\n    return {prefix + str(k): v for k, v in d.items()}\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.prod","title":"<code>prod(vals)</code>","text":"<p>Calculate the product of a sequence of integers.</p> <p>Parameters:</p> Name Type Description Default <code>vals</code> <code>Sequence[int]</code> <p>The sequence of integers.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The product of the sequence of integers.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def prod(vals: Sequence[int]) -&gt; int:\n    r\"\"\"Calculate the product of a sequence of integers.\n\n    Args:\n        vals: The sequence of integers.\n\n    Returns:\n        The product of the sequence of integers.\n    \"\"\"\n    return reduce(lambda x, y: x * y, vals)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.rand_bool","title":"<code>rand_bool(shape, proba)</code>","text":"<p>Generate a tensor of random booleans.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the tensor.</p> required <code>proba</code> <code>float</code> <p>The probability of a True value.</p> required <code>device</code> <p>The device to create the tensor on.</p> required <p>Returns:</p> Type Description <p>A tensor of random booleans.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def rand_bool(shape: Tuple[int, ...], proba: float):\n    r\"\"\"Generate a tensor of random booleans.\n\n    Args:\n        shape: The shape of the tensor.\n        proba: The probability of a True value.\n        device: The device to create the tensor on.\n\n    Returns:\n        A tensor of random booleans.\n    \"\"\"\n    if proba == 1:\n        return torch.ones(shape, dtype=torch.bool)\n    elif proba == 0:\n        return torch.zeros(shape, dtype=torch.bool)\n    else:\n        return torch.bernoulli(torch.full(shape, proba)).to(dtype=torch.bool)\n</code></pre>"},{"location":"models/tts/styledtts2/diffusion/utils/#models.tts.styledtts2.diffusion.utils.to_list","title":"<code>to_list(val)</code>","text":"<p>Convert a value or a sequence of values to a list.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[T, Sequence[T]]</code> <p>The value or sequence of values to convert.</p> required <p>Returns:</p> Type Description <code>List[T]</code> <p>The value or sequence of values as a list.</p> Source code in <code>models/tts/styledtts2/diffusion/utils.py</code> <pre><code>def to_list(val: Union[T, Sequence[T]]) -&gt; List[T]:\n    r\"\"\"Convert a value or a sequence of values to a list.\n\n    Args:\n        val: The value or sequence of values to convert.\n\n    Returns:\n        The value or sequence of values as a list.\n    \"\"\"\n    if isinstance(val, tuple):\n        return list(val)\n    if isinstance(val, list):\n        return val\n    if isinstance(val, Sequence):\n        return list(val)\n    return [val]\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator/","title":"Discriminator","text":""},{"location":"models/vocoder/univnet/discriminator/#models.vocoder.univnet.discriminator.Discriminator","title":"<code>Discriminator</code>","text":"<p>             Bases: <code>Module</code></p> <p>Discriminator for the UnuvNet vocoder.</p> <p>This class implements a discriminator that consists of a <code>MultiResolutionDiscriminator</code> and a <code>MultiPeriodDiscriminator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>VocoderModelConfig</code> <p>Model configuration object.</p> required <p>Attributes:</p> Name Type Description <code>MRD</code> <code>MultiResolutionDiscriminator</code> <p>Multi-resolution discriminator instance.</p> <code>MPD</code> <code>MultiPeriodDiscriminator</code> <p>Multi-period discriminator instance.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the forward pass of the discriminator.</p> Source code in <code>models/vocoder/univnet/discriminator.py</code> <pre><code>class Discriminator(Module):\n    r\"\"\"Discriminator for the UnuvNet vocoder.\n\n    This class implements a discriminator that consists of a `MultiResolutionDiscriminator` and a `MultiPeriodDiscriminator`.\n\n    Args:\n        model_config (VocoderModelConfig): Model configuration object.\n\n    Attributes:\n        MRD (MultiResolutionDiscriminator): Multi-resolution discriminator instance.\n        MPD (MultiPeriodDiscriminator): Multi-period discriminator instance.\n\n    Methods:\n        forward(x): Computes the forward pass of the discriminator.\n\n    \"\"\"\n\n    def __init__(self, model_config: VocoderModelConfig):\n        super().__init__()\n        self.MRD = MultiResolutionDiscriminator(model_config=model_config)\n        self.MPD = MultiPeriodDiscriminator(model_config=model_config)\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Computes the forward pass of the discriminator.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape [B, C, T].\n\n        Returns:\n            tuple(torch.Tensor, torch.Tensor): Tuple containing the output tensors of the `MultiResolutionDiscriminator` and `MultiPeriodDiscriminator` instances.\n        \"\"\"\n        return self.MRD(x), self.MPD(x)\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator/#models.vocoder.univnet.discriminator.Discriminator.forward","title":"<code>forward(x)</code>","text":"<p>Computes the forward pass of the discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [B, C, T].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>(Tensor, Tensor)</code> <p>Tuple containing the output tensors of the <code>MultiResolutionDiscriminator</code> and <code>MultiPeriodDiscriminator</code> instances.</p> Source code in <code>models/vocoder/univnet/discriminator.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Computes the forward pass of the discriminator.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [B, C, T].\n\n    Returns:\n        tuple(torch.Tensor, torch.Tensor): Tuple containing the output tensors of the `MultiResolutionDiscriminator` and `MultiPeriodDiscriminator` instances.\n    \"\"\"\n    return self.MRD(x), self.MPD(x)\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator_p/","title":"DiscriminatorP","text":""},{"location":"models/vocoder/univnet/discriminator_p/#models.vocoder.univnet.discriminator_p.DiscriminatorP","title":"<code>DiscriminatorP</code>","text":"<p>             Bases: <code>Module</code></p> <p>DiscriminatorP is a class that implements a discriminator network for the UnivNet vocoder.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>The period of the Mel spectrogram.</p> required <code>model_config</code> <code>VocoderModelConfig</code> <p>The configuration object for the UnivNet vocoder model.</p> required Source code in <code>models/vocoder/univnet/discriminator_p.py</code> <pre><code>class DiscriminatorP(Module):\n    r\"\"\"DiscriminatorP is a class that implements a discriminator network for the UnivNet vocoder.\n\n    Args:\n        period (int): The period of the Mel spectrogram.\n        model_config (VocoderModelConfig): The configuration object for the UnivNet vocoder model.\n    \"\"\"\n\n    def __init__(\n        self,\n        period: int,\n        model_config: VocoderModelConfig,\n    ):\n        super().__init__()\n\n        self.LRELU_SLOPE = model_config.mpd.lReLU_slope\n        self.period = period\n\n        kernel_size = model_config.mpd.kernel_size\n        stride = model_config.mpd.stride\n\n        norm_f: Any = (\n            spectral_norm if model_config.mpd.use_spectral_norm else weight_norm\n        )\n\n        self.convs = nn.ModuleList(\n            [\n                norm_f(\n                    nn.Conv2d(\n                        1,\n                        64,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(kernel_size // 2, 0),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        64,\n                        128,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(kernel_size // 2, 0),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        128,\n                        256,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(kernel_size // 2, 0),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        256,\n                        512,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(kernel_size // 2, 0),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        512,\n                        1024,\n                        (kernel_size, 1),\n                        1,\n                        padding=(kernel_size // 2, 0),\n                    ),\n                ),\n            ],\n        )\n        self.conv_post = norm_f(\n            nn.Conv2d(\n                1024,\n                1,\n                (3, 1),\n                1,\n                padding=(1, 0),\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[list, torch.Tensor]:\n        r\"\"\"Forward pass of the discriminator network.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, channels, time_steps).\n\n        Returns:\n            Tuple[list, torch.Tensor]: A tuple containing a list of feature maps and the output tensor of shape (batch_size, period).\n        \"\"\"\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for layers in self.convs:\n            x = layers(x.to(dtype=self.conv_post.weight.dtype))\n            x = F.leaky_relu(x, self.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return fmap, x\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator_p/#models.vocoder.univnet.discriminator_p.DiscriminatorP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the discriminator network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, channels, time_steps).</p> required <p>Returns:</p> Type Description <code>Tuple[list, Tensor]</code> <p>Tuple[list, torch.Tensor]: A tuple containing a list of feature maps and the output tensor of shape (batch_size, period).</p> Source code in <code>models/vocoder/univnet/discriminator_p.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[list, torch.Tensor]:\n    r\"\"\"Forward pass of the discriminator network.\n\n    Args:\n        x (torch.Tensor): The input tensor of shape (batch_size, channels, time_steps).\n\n    Returns:\n        Tuple[list, torch.Tensor]: A tuple containing a list of feature maps and the output tensor of shape (batch_size, period).\n    \"\"\"\n    fmap = []\n\n    # 1d to 2d\n    b, c, t = x.shape\n    if t % self.period != 0:  # pad first\n        n_pad = self.period - (t % self.period)\n        x = F.pad(x, (0, n_pad), \"reflect\")\n        t = t + n_pad\n    x = x.view(b, c, t // self.period, self.period)\n\n    for layers in self.convs:\n        x = layers(x.to(dtype=self.conv_post.weight.dtype))\n        x = F.leaky_relu(x, self.LRELU_SLOPE)\n        fmap.append(x)\n    x = self.conv_post(x)\n    fmap.append(x)\n    x = torch.flatten(x, 1, -1)\n\n    return fmap, x\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator_r/","title":"DiscriminatorR","text":""},{"location":"models/vocoder/univnet/discriminator_r/#models.vocoder.univnet.discriminator_r.DiscriminatorR","title":"<code>DiscriminatorR</code>","text":"<p>             Bases: <code>Module</code></p> <p>A class representing the Residual Discriminator network for a UnivNet vocoder.</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>Tuple</code> <p>A tuple containing the number of FFT points, hop length, and window length.</p> required <code>model_config</code> <code>VocoderModelConfig</code> <p>A configuration object for the UnivNet model.</p> required Source code in <code>models/vocoder/univnet/discriminator_r.py</code> <pre><code>class DiscriminatorR(Module):\n    r\"\"\"A class representing the Residual Discriminator network for a UnivNet vocoder.\n\n    Args:\n        resolution (Tuple): A tuple containing the number of FFT points, hop length, and window length.\n        model_config (VocoderModelConfig): A configuration object for the UnivNet model.\n    \"\"\"\n\n    def __init__(\n        self,\n        resolution: Tuple[int, int, int],\n        model_config: VocoderModelConfig,\n    ):\n        super().__init__()\n\n        self.resolution = resolution\n        self.LRELU_SLOPE = model_config.mrd.lReLU_slope\n\n        # Use spectral normalization or weight normalization based on the configuration\n        norm_f: Any = (\n            spectral_norm if model_config.mrd.use_spectral_norm else weight_norm\n        )\n\n        # Define the convolutional layers\n        self.convs = nn.ModuleList(\n            [\n                norm_f(\n                    nn.Conv2d(\n                        1,\n                        32,\n                        (3, 9),\n                        padding=(1, 4),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        32,\n                        32,\n                        (3, 9),\n                        stride=(1, 2),\n                        padding=(1, 4),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        32,\n                        32,\n                        (3, 9),\n                        stride=(1, 2),\n                        padding=(1, 4),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        32,\n                        32,\n                        (3, 9),\n                        stride=(1, 2),\n                        padding=(1, 4),\n                    ),\n                ),\n                norm_f(\n                    nn.Conv2d(\n                        32,\n                        32,\n                        (3, 3),\n                        padding=(1, 1),\n                    ),\n                ),\n            ],\n        )\n        self.conv_post = norm_f(\n            nn.Conv2d(\n                32,\n                1,\n                (3, 3),\n                padding=(1, 1),\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[list[torch.Tensor], torch.Tensor]:\n        r\"\"\"Forward pass of the DiscriminatorR class.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            tuple: A tuple containing the intermediate feature maps and the output tensor.\n        \"\"\"\n        fmap = []\n\n        # Compute the magnitude spectrogram of the input waveform\n        x = self.spectrogram(x)\n\n        # Add a channel dimension to the spectrogram tensor\n        x = x.unsqueeze(1)\n\n        # Apply the convolutional layers with leaky ReLU activation\n        for layer in self.convs:\n            x = layer(x.to(dtype=self.conv_post.weight.dtype))\n            x = F.leaky_relu(x, self.LRELU_SLOPE)\n            fmap.append(x)\n\n        # Apply the post-convolutional layer\n        x = self.conv_post(x)\n        fmap.append(x)\n\n        # Flatten the output tensor\n        x = torch.flatten(x, 1, -1)\n\n        return fmap, x\n\n    def spectrogram(self, x: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the magnitude spectrogram of the input waveform.\n\n        Args:\n            x (torch.Tensor): Input waveform tensor of shape [B, C, T].\n\n        Returns:\n            torch.Tensor: Magnitude spectrogram tensor of shape [B, F, TT], where F is the number of frequency bins and TT is the number of time frames.\n        \"\"\"\n        n_fft, hop_length, win_length = self.resolution\n\n        # Apply reflection padding to the input waveform\n        x = F.pad(\n            x,\n            (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n            mode=\"reflect\",\n        )\n\n        # Squeeze the input waveform to remove the channel dimension\n        x = x.squeeze(1)\n\n        # Compute the short-time Fourier transform of the input waveform\n        x = torch.stft(\n            x,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            center=False,\n            return_complex=True,\n            window=torch.ones(win_length, device=x.device),\n        )  # [B, F, TT, 2]\n\n        x = torch.view_as_real(x)\n\n        # Compute the magnitude spectrogram from the complex spectrogram\n        return torch.norm(x, p=2, dim=-1)  # [B, F, TT]\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator_r/#models.vocoder.univnet.discriminator_r.DiscriminatorR.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the DiscriminatorR class.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[Tensor], Tensor]</code> <p>A tuple containing the intermediate feature maps and the output tensor.</p> Source code in <code>models/vocoder/univnet/discriminator_r.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[list[torch.Tensor], torch.Tensor]:\n    r\"\"\"Forward pass of the DiscriminatorR class.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        tuple: A tuple containing the intermediate feature maps and the output tensor.\n    \"\"\"\n    fmap = []\n\n    # Compute the magnitude spectrogram of the input waveform\n    x = self.spectrogram(x)\n\n    # Add a channel dimension to the spectrogram tensor\n    x = x.unsqueeze(1)\n\n    # Apply the convolutional layers with leaky ReLU activation\n    for layer in self.convs:\n        x = layer(x.to(dtype=self.conv_post.weight.dtype))\n        x = F.leaky_relu(x, self.LRELU_SLOPE)\n        fmap.append(x)\n\n    # Apply the post-convolutional layer\n    x = self.conv_post(x)\n    fmap.append(x)\n\n    # Flatten the output tensor\n    x = torch.flatten(x, 1, -1)\n\n    return fmap, x\n</code></pre>"},{"location":"models/vocoder/univnet/discriminator_r/#models.vocoder.univnet.discriminator_r.DiscriminatorR.spectrogram","title":"<code>spectrogram(x)</code>","text":"<p>Computes the magnitude spectrogram of the input waveform.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input waveform tensor of shape [B, C, T].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Magnitude spectrogram tensor of shape [B, F, TT], where F is the number of frequency bins and TT is the number of time frames.</p> Source code in <code>models/vocoder/univnet/discriminator_r.py</code> <pre><code>def spectrogram(self, x: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the magnitude spectrogram of the input waveform.\n\n    Args:\n        x (torch.Tensor): Input waveform tensor of shape [B, C, T].\n\n    Returns:\n        torch.Tensor: Magnitude spectrogram tensor of shape [B, F, TT], where F is the number of frequency bins and TT is the number of time frames.\n    \"\"\"\n    n_fft, hop_length, win_length = self.resolution\n\n    # Apply reflection padding to the input waveform\n    x = F.pad(\n        x,\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n\n    # Squeeze the input waveform to remove the channel dimension\n    x = x.squeeze(1)\n\n    # Compute the short-time Fourier transform of the input waveform\n    x = torch.stft(\n        x,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        center=False,\n        return_complex=True,\n        window=torch.ones(win_length, device=x.device),\n    )  # [B, F, TT, 2]\n\n    x = torch.view_as_real(x)\n\n    # Compute the magnitude spectrogram from the complex spectrogram\n    return torch.norm(x, p=2, dim=-1)  # [B, F, TT]\n</code></pre>"},{"location":"models/vocoder/univnet/generator/","title":"Generator","text":""},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator","title":"<code>Generator</code>","text":"<p>             Bases: <code>Module</code></p> <p>UnivNet Generator</p> Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>class Generator(Module):\n    \"\"\"UnivNet Generator\"\"\"\n\n    def __init__(\n        self,\n        model_config: VocoderModelConfig,\n        preprocess_config: PreprocessingConfig,\n    ):\n        r\"\"\"UnivNet Generator.\n        Initializes the UnivNet module.\n\n        Args:\n            model_config (VocoderModelConfig): the model configuration.\n            preprocess_config (PreprocessingConfig): the preprocessing configuration.\n        \"\"\"\n        super().__init__()\n\n        self.mel_channel = preprocess_config.stft.n_mel_channels\n        self.noise_dim = model_config.gen.noise_dim\n        self.hop_length = preprocess_config.stft.hop_length\n        channel_size = model_config.gen.channel_size\n        kpnet_conv_size = model_config.gen.kpnet_conv_size\n\n        hop_length = 1\n        self.res_stack = nn.ModuleList()\n\n        for stride in model_config.gen.strides:\n            hop_length = stride * hop_length\n            self.res_stack.append(\n                LVCBlock(\n                    channel_size,\n                    preprocess_config.stft.n_mel_channels,\n                    stride=stride,\n                    dilations=model_config.gen.dilations,\n                    lReLU_slope=model_config.gen.lReLU_slope,\n                    cond_hop_length=hop_length,\n                    kpnet_conv_size=kpnet_conv_size,\n                ),\n            )\n\n        self.conv_pre = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                model_config.gen.noise_dim,\n                channel_size,\n                7,\n                padding=3,\n                padding_mode=\"reflect\",\n            ),\n        )\n\n        self.conv_post = nn.Sequential(\n            nn.LeakyReLU(model_config.gen.lReLU_slope),\n            nn.utils.parametrizations.weight_norm(\n                nn.Conv1d(\n                    channel_size,\n                    1,\n                    7,\n                    padding=3,\n                    padding_mode=\"reflect\",\n                ),\n            ),\n            nn.Tanh(),\n        )\n\n        # Output of STFT(zeros)\n        self.mel_mask_value = -11.5129\n\n    def forward(self, c: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the Generator module.\n\n        Args:\n            c (Tensor): the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)\n\n        Returns:\n            Tensor: the generated audio waveform (batch, 1, out_length)\n        \"\"\"\n        z = torch.randn(\n            c.shape[0],\n            self.noise_dim,\n            c.shape[2],\n            device=c.device,\n            dtype=self.conv_pre.weight.data.dtype,\n        )\n        z = self.conv_pre(z)  # (B, c_g, L)\n\n        for res_block in self.res_stack:\n            z = res_block(z, c)  # (B, c_g, L * s_0 * ... * s_i)\n\n        return self.conv_post(z)  # (B, 1, L * 256)\n\n    def eval(self, inference: bool = False):\n        r\"\"\"Sets the module to evaluation mode.\n\n        Args:\n            inference (bool): whether to remove weight normalization or not.\n        \"\"\"\n        super().eval()\n        # don't remove weight norm while validation in training loop\n        if inference:\n            self.remove_weight_norm()\n\n    def remove_weight_norm(self) -&gt; None:\n        r\"\"\"Removes weight normalization from the module.\"\"\"\n        print(\"Removing weight norm...\")\n\n        parametrize.remove_parametrizations(self.conv_pre, \"weight\")\n\n        for layer in self.conv_post:\n            if len(layer.state_dict()) != 0:\n                parametrize.remove_parametrizations(layer, \"weight\")\n\n        for res_block in self.res_stack:\n            res_block.remove_weight_norm()\n\n    def infer(self, c: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Infers the audio waveform from the mel-spectrogram conditioning sequence.\n\n        Args:\n            c (Tensor): the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)\n            mel_lens (Tensor): the lengths of the mel-spectrogram conditioning sequence.\n\n        Returns:\n            Tensor: the generated audio waveform (batch, 1, out_length)\n        \"\"\"\n        mel_mask = get_mask_from_lengths(mel_lens).unsqueeze(1).to(c.device)\n        c = c.masked_fill(mel_mask, self.mel_mask_value)\n        zero = torch.full(\n            (c.shape[0], self.mel_channel, 10), self.mel_mask_value, device=c.device,\n        )\n        mel = torch.cat((c, zero), dim=2)\n        audio = self(mel)\n        audio = audio[:, :, : -(self.hop_length * 10)]\n        audio_mask = get_mask_from_lengths(mel_lens * 256).unsqueeze(1)\n        return audio.masked_fill(audio_mask, 0.0)\n</code></pre>"},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator.__init__","title":"<code>__init__(model_config, preprocess_config)</code>","text":"<p>UnivNet Generator. Initializes the UnivNet module.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>VocoderModelConfig</code> <p>the model configuration.</p> required <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>the preprocessing configuration.</p> required Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>def __init__(\n    self,\n    model_config: VocoderModelConfig,\n    preprocess_config: PreprocessingConfig,\n):\n    r\"\"\"UnivNet Generator.\n    Initializes the UnivNet module.\n\n    Args:\n        model_config (VocoderModelConfig): the model configuration.\n        preprocess_config (PreprocessingConfig): the preprocessing configuration.\n    \"\"\"\n    super().__init__()\n\n    self.mel_channel = preprocess_config.stft.n_mel_channels\n    self.noise_dim = model_config.gen.noise_dim\n    self.hop_length = preprocess_config.stft.hop_length\n    channel_size = model_config.gen.channel_size\n    kpnet_conv_size = model_config.gen.kpnet_conv_size\n\n    hop_length = 1\n    self.res_stack = nn.ModuleList()\n\n    for stride in model_config.gen.strides:\n        hop_length = stride * hop_length\n        self.res_stack.append(\n            LVCBlock(\n                channel_size,\n                preprocess_config.stft.n_mel_channels,\n                stride=stride,\n                dilations=model_config.gen.dilations,\n                lReLU_slope=model_config.gen.lReLU_slope,\n                cond_hop_length=hop_length,\n                kpnet_conv_size=kpnet_conv_size,\n            ),\n        )\n\n    self.conv_pre = nn.utils.parametrizations.weight_norm(\n        nn.Conv1d(\n            model_config.gen.noise_dim,\n            channel_size,\n            7,\n            padding=3,\n            padding_mode=\"reflect\",\n        ),\n    )\n\n    self.conv_post = nn.Sequential(\n        nn.LeakyReLU(model_config.gen.lReLU_slope),\n        nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                channel_size,\n                1,\n                7,\n                padding=3,\n                padding_mode=\"reflect\",\n            ),\n        ),\n        nn.Tanh(),\n    )\n\n    # Output of STFT(zeros)\n    self.mel_mask_value = -11.5129\n</code></pre>"},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator.eval","title":"<code>eval(inference=False)</code>","text":"<p>Sets the module to evaluation mode.</p> <p>Parameters:</p> Name Type Description Default <code>inference</code> <code>bool</code> <p>whether to remove weight normalization or not.</p> <code>False</code> Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>def eval(self, inference: bool = False):\n    r\"\"\"Sets the module to evaluation mode.\n\n    Args:\n        inference (bool): whether to remove weight normalization or not.\n    \"\"\"\n    super().eval()\n    # don't remove weight norm while validation in training loop\n    if inference:\n        self.remove_weight_norm()\n</code></pre>"},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator.forward","title":"<code>forward(c)</code>","text":"<p>Forward pass of the Generator module.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>the generated audio waveform (batch, 1, out_length)</p> Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>def forward(self, c: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the Generator module.\n\n    Args:\n        c (Tensor): the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)\n\n    Returns:\n        Tensor: the generated audio waveform (batch, 1, out_length)\n    \"\"\"\n    z = torch.randn(\n        c.shape[0],\n        self.noise_dim,\n        c.shape[2],\n        device=c.device,\n        dtype=self.conv_pre.weight.data.dtype,\n    )\n    z = self.conv_pre(z)  # (B, c_g, L)\n\n    for res_block in self.res_stack:\n        z = res_block(z, c)  # (B, c_g, L * s_0 * ... * s_i)\n\n    return self.conv_post(z)  # (B, 1, L * 256)\n</code></pre>"},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator.infer","title":"<code>infer(c, mel_lens)</code>","text":"<p>Infers the audio waveform from the mel-spectrogram conditioning sequence.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)</p> required <code>mel_lens</code> <code>Tensor</code> <p>the lengths of the mel-spectrogram conditioning sequence.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>the generated audio waveform (batch, 1, out_length)</p> Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>def infer(self, c: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Infers the audio waveform from the mel-spectrogram conditioning sequence.\n\n    Args:\n        c (Tensor): the conditioning sequence of mel-spectrogram (batch, mel_channels, in_length)\n        mel_lens (Tensor): the lengths of the mel-spectrogram conditioning sequence.\n\n    Returns:\n        Tensor: the generated audio waveform (batch, 1, out_length)\n    \"\"\"\n    mel_mask = get_mask_from_lengths(mel_lens).unsqueeze(1).to(c.device)\n    c = c.masked_fill(mel_mask, self.mel_mask_value)\n    zero = torch.full(\n        (c.shape[0], self.mel_channel, 10), self.mel_mask_value, device=c.device,\n    )\n    mel = torch.cat((c, zero), dim=2)\n    audio = self(mel)\n    audio = audio[:, :, : -(self.hop_length * 10)]\n    audio_mask = get_mask_from_lengths(mel_lens * 256).unsqueeze(1)\n    return audio.masked_fill(audio_mask, 0.0)\n</code></pre>"},{"location":"models/vocoder/univnet/generator/#models.vocoder.univnet.generator.Generator.remove_weight_norm","title":"<code>remove_weight_norm()</code>","text":"<p>Removes weight normalization from the module.</p> Source code in <code>models/vocoder/univnet/generator.py</code> <pre><code>def remove_weight_norm(self) -&gt; None:\n    r\"\"\"Removes weight normalization from the module.\"\"\"\n    print(\"Removing weight norm...\")\n\n    parametrize.remove_parametrizations(self.conv_pre, \"weight\")\n\n    for layer in self.conv_post:\n        if len(layer.state_dict()) != 0:\n            parametrize.remove_parametrizations(layer, \"weight\")\n\n    for res_block in self.res_stack:\n        res_block.remove_weight_norm()\n</code></pre>"},{"location":"models/vocoder/univnet/kernel_predictor/","title":"Kernel Predictor","text":""},{"location":"models/vocoder/univnet/kernel_predictor/#models.vocoder.univnet.kernel_predictor.KernelPredictor","title":"<code>KernelPredictor</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>models/vocoder/univnet/kernel_predictor.py</code> <pre><code>class KernelPredictor(Module):\n    def __init__(\n        self,\n        cond_channels: int,\n        conv_in_channels: int,\n        conv_out_channels: int,\n        conv_layers: int,\n        conv_kernel_size: int = 3,\n        kpnet_hidden_channels: int = 64,\n        kpnet_conv_size: int = 3,\n        kpnet_dropout: float = 0.0,\n        lReLU_slope: float = 0.1,\n    ):\n        r\"\"\"Initializes a KernelPredictor object.\n        KernelPredictor is a class that predicts the kernel size for the convolutional layers in the UnivNet model.\n        The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram as the input.\n\n        Args:\n            cond_channels (int): The number of channels for the conditioning sequence.\n            conv_in_channels (int): The number of channels for the input sequence.\n            conv_out_channels (int): The number of channels for the output sequence.\n            conv_layers (int): The number of layers in the model.\n            conv_kernel_size (int, optional): The kernel size for the convolutional layers. Defaults to 3.\n            kpnet_hidden_channels (int, optional): The number of hidden channels in the kernel predictor network. Defaults to 64.\n            kpnet_conv_size (int, optional): The kernel size for the kernel predictor network. Defaults to 3.\n            kpnet_dropout (float, optional): The dropout rate for the kernel predictor network. Defaults to 0.0.\n            lReLU_slope (float, optional): The slope for the leaky ReLU activation function. Defaults to 0.1.\n        \"\"\"\n        super().__init__()\n\n        self.conv_in_channels = conv_in_channels\n        self.conv_out_channels = conv_out_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_layers = conv_layers\n\n        kpnet_kernel_channels = (\n            conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers\n        )  # l_w\n\n        kpnet_bias_channels = conv_out_channels * conv_layers  # l_b\n\n        padding = (kpnet_conv_size - 1) // 2\n\n        self.input_conv = nn.Sequential(\n            nn.utils.parametrizations.weight_norm(\n                nn.Conv1d(\n                    cond_channels,\n                    kpnet_hidden_channels,\n                    5,\n                    padding=2,\n                    bias=True,\n                ),\n            ),\n            nn.LeakyReLU(lReLU_slope),\n        )\n\n        self.residual_convs = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.Dropout(kpnet_dropout),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        ),\n                    ),\n                    nn.LeakyReLU(lReLU_slope),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            kpnet_hidden_channels,\n                            kpnet_hidden_channels,\n                            kpnet_conv_size,\n                            padding=padding,\n                            bias=True,\n                        ),\n                    ),\n                    nn.LeakyReLU(lReLU_slope),\n                )\n                for _ in range(3)\n            ],\n        )\n\n        self.kernel_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_kernel_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            ),\n        )\n        self.bias_conv = nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                kpnet_hidden_channels,\n                kpnet_bias_channels,\n                kpnet_conv_size,\n                padding=padding,\n                bias=True,\n            ),\n        )\n\n    def forward(self, c: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Computes the forward pass of the model.\n\n        Args:\n            c (Tensor): The conditioning sequence (batch, cond_channels, cond_length).\n\n        Returns:\n            Tuple[Tensor, Tensor]: A tuple containing the kernel and bias tensors.\n        \"\"\"\n        batch, _, cond_length = c.shape\n        c = self.input_conv(c.to(dtype=self.kernel_conv.weight.dtype))\n        for residual_conv in self.residual_convs:\n            c = c + residual_conv(c)\n        k = self.kernel_conv(c)\n        b = self.bias_conv(c)\n        kernels = k.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_in_channels,\n            self.conv_out_channels,\n            self.conv_kernel_size,\n            cond_length,\n        )\n        bias = b.contiguous().view(\n            batch,\n            self.conv_layers,\n            self.conv_out_channels,\n            cond_length,\n        )\n\n        return kernels, bias\n\n    def remove_weight_norm(self):\n        r\"\"\"Removes weight normalization from the input, kernel, bias, and residual convolutional layers.\"\"\"\n        parametrize.remove_parametrizations(self.input_conv[0], \"weight\")\n        parametrize.remove_parametrizations(self.kernel_conv, \"weight\")\n        parametrize.remove_parametrizations(self.bias_conv, \"weight\")\n\n        for block in self.residual_convs:\n            parametrize.remove_parametrizations(block[1], \"weight\") # type: ignore\n            parametrize.remove_parametrizations(block[3], \"weight\") # type: ignore\n</code></pre>"},{"location":"models/vocoder/univnet/kernel_predictor/#models.vocoder.univnet.kernel_predictor.KernelPredictor.__init__","title":"<code>__init__(cond_channels, conv_in_channels, conv_out_channels, conv_layers, conv_kernel_size=3, kpnet_hidden_channels=64, kpnet_conv_size=3, kpnet_dropout=0.0, lReLU_slope=0.1)</code>","text":"<p>Initializes a KernelPredictor object. KernelPredictor is a class that predicts the kernel size for the convolutional layers in the UnivNet model. The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram as the input.</p> <p>Parameters:</p> Name Type Description Default <code>cond_channels</code> <code>int</code> <p>The number of channels for the conditioning sequence.</p> required <code>conv_in_channels</code> <code>int</code> <p>The number of channels for the input sequence.</p> required <code>conv_out_channels</code> <code>int</code> <p>The number of channels for the output sequence.</p> required <code>conv_layers</code> <code>int</code> <p>The number of layers in the model.</p> required <code>conv_kernel_size</code> <code>int</code> <p>The kernel size for the convolutional layers. Defaults to 3.</p> <code>3</code> <code>kpnet_hidden_channels</code> <code>int</code> <p>The number of hidden channels in the kernel predictor network. Defaults to 64.</p> <code>64</code> <code>kpnet_conv_size</code> <code>int</code> <p>The kernel size for the kernel predictor network. Defaults to 3.</p> <code>3</code> <code>kpnet_dropout</code> <code>float</code> <p>The dropout rate for the kernel predictor network. Defaults to 0.0.</p> <code>0.0</code> <code>lReLU_slope</code> <code>float</code> <p>The slope for the leaky ReLU activation function. Defaults to 0.1.</p> <code>0.1</code> Source code in <code>models/vocoder/univnet/kernel_predictor.py</code> <pre><code>def __init__(\n    self,\n    cond_channels: int,\n    conv_in_channels: int,\n    conv_out_channels: int,\n    conv_layers: int,\n    conv_kernel_size: int = 3,\n    kpnet_hidden_channels: int = 64,\n    kpnet_conv_size: int = 3,\n    kpnet_dropout: float = 0.0,\n    lReLU_slope: float = 0.1,\n):\n    r\"\"\"Initializes a KernelPredictor object.\n    KernelPredictor is a class that predicts the kernel size for the convolutional layers in the UnivNet model.\n    The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram as the input.\n\n    Args:\n        cond_channels (int): The number of channels for the conditioning sequence.\n        conv_in_channels (int): The number of channels for the input sequence.\n        conv_out_channels (int): The number of channels for the output sequence.\n        conv_layers (int): The number of layers in the model.\n        conv_kernel_size (int, optional): The kernel size for the convolutional layers. Defaults to 3.\n        kpnet_hidden_channels (int, optional): The number of hidden channels in the kernel predictor network. Defaults to 64.\n        kpnet_conv_size (int, optional): The kernel size for the kernel predictor network. Defaults to 3.\n        kpnet_dropout (float, optional): The dropout rate for the kernel predictor network. Defaults to 0.0.\n        lReLU_slope (float, optional): The slope for the leaky ReLU activation function. Defaults to 0.1.\n    \"\"\"\n    super().__init__()\n\n    self.conv_in_channels = conv_in_channels\n    self.conv_out_channels = conv_out_channels\n    self.conv_kernel_size = conv_kernel_size\n    self.conv_layers = conv_layers\n\n    kpnet_kernel_channels = (\n        conv_in_channels * conv_out_channels * conv_kernel_size * conv_layers\n    )  # l_w\n\n    kpnet_bias_channels = conv_out_channels * conv_layers  # l_b\n\n    padding = (kpnet_conv_size - 1) // 2\n\n    self.input_conv = nn.Sequential(\n        nn.utils.parametrizations.weight_norm(\n            nn.Conv1d(\n                cond_channels,\n                kpnet_hidden_channels,\n                5,\n                padding=2,\n                bias=True,\n            ),\n        ),\n        nn.LeakyReLU(lReLU_slope),\n    )\n\n    self.residual_convs = nn.ModuleList(\n        [\n            nn.Sequential(\n                nn.Dropout(kpnet_dropout),\n                nn.utils.parametrizations.weight_norm(\n                    nn.Conv1d(\n                        kpnet_hidden_channels,\n                        kpnet_hidden_channels,\n                        kpnet_conv_size,\n                        padding=padding,\n                        bias=True,\n                    ),\n                ),\n                nn.LeakyReLU(lReLU_slope),\n                nn.utils.parametrizations.weight_norm(\n                    nn.Conv1d(\n                        kpnet_hidden_channels,\n                        kpnet_hidden_channels,\n                        kpnet_conv_size,\n                        padding=padding,\n                        bias=True,\n                    ),\n                ),\n                nn.LeakyReLU(lReLU_slope),\n            )\n            for _ in range(3)\n        ],\n    )\n\n    self.kernel_conv = nn.utils.parametrizations.weight_norm(\n        nn.Conv1d(\n            kpnet_hidden_channels,\n            kpnet_kernel_channels,\n            kpnet_conv_size,\n            padding=padding,\n            bias=True,\n        ),\n    )\n    self.bias_conv = nn.utils.parametrizations.weight_norm(\n        nn.Conv1d(\n            kpnet_hidden_channels,\n            kpnet_bias_channels,\n            kpnet_conv_size,\n            padding=padding,\n            bias=True,\n        ),\n    )\n</code></pre>"},{"location":"models/vocoder/univnet/kernel_predictor/#models.vocoder.univnet.kernel_predictor.KernelPredictor.forward","title":"<code>forward(c)</code>","text":"<p>Computes the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The conditioning sequence (batch, cond_channels, cond_length).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor]: A tuple containing the kernel and bias tensors.</p> Source code in <code>models/vocoder/univnet/kernel_predictor.py</code> <pre><code>def forward(self, c: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Computes the forward pass of the model.\n\n    Args:\n        c (Tensor): The conditioning sequence (batch, cond_channels, cond_length).\n\n    Returns:\n        Tuple[Tensor, Tensor]: A tuple containing the kernel and bias tensors.\n    \"\"\"\n    batch, _, cond_length = c.shape\n    c = self.input_conv(c.to(dtype=self.kernel_conv.weight.dtype))\n    for residual_conv in self.residual_convs:\n        c = c + residual_conv(c)\n    k = self.kernel_conv(c)\n    b = self.bias_conv(c)\n    kernels = k.contiguous().view(\n        batch,\n        self.conv_layers,\n        self.conv_in_channels,\n        self.conv_out_channels,\n        self.conv_kernel_size,\n        cond_length,\n    )\n    bias = b.contiguous().view(\n        batch,\n        self.conv_layers,\n        self.conv_out_channels,\n        cond_length,\n    )\n\n    return kernels, bias\n</code></pre>"},{"location":"models/vocoder/univnet/kernel_predictor/#models.vocoder.univnet.kernel_predictor.KernelPredictor.remove_weight_norm","title":"<code>remove_weight_norm()</code>","text":"<p>Removes weight normalization from the input, kernel, bias, and residual convolutional layers.</p> Source code in <code>models/vocoder/univnet/kernel_predictor.py</code> <pre><code>def remove_weight_norm(self):\n    r\"\"\"Removes weight normalization from the input, kernel, bias, and residual convolutional layers.\"\"\"\n    parametrize.remove_parametrizations(self.input_conv[0], \"weight\")\n    parametrize.remove_parametrizations(self.kernel_conv, \"weight\")\n    parametrize.remove_parametrizations(self.bias_conv, \"weight\")\n\n    for block in self.residual_convs:\n        parametrize.remove_parametrizations(block[1], \"weight\") # type: ignore\n        parametrize.remove_parametrizations(block[3], \"weight\") # type: ignore\n</code></pre>"},{"location":"models/vocoder/univnet/lvc_block/","title":"LVC Block","text":""},{"location":"models/vocoder/univnet/lvc_block/#models.vocoder.univnet.lvc_block.LVCBlock","title":"<code>LVCBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>The location-variable convolutions block.</p> <p>To efficiently capture the local information of the condition, location-variable convolution (LVC) obtained better sound quality and speed while maintaining the model size. The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram as the input. The kernel predictor is connected to a residual stack. One kernel predictor simultaneously predicts the kernels of all LVC layers in one residual stack.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>cond_channels</code> <code>int</code> <p>The number of conditioning channels.</p> required <code>stride</code> <code>int</code> <p>The stride of the convolutional layers.</p> required <code>dilations</code> <code>List[int]</code> <p>A list of dilation values for the convolutional layers.</p> <code>[1, 3, 9, 27]</code> <code>lReLU_slope</code> <code>float</code> <p>The slope of the LeakyReLU activation function.</p> <code>0.2</code> <code>conv_kernel_size</code> <code>int</code> <p>The kernel size of the convolutional layers.</p> <code>3</code> <code>cond_hop_length</code> <code>int</code> <p>The hop length of the conditioning sequence.</p> <code>256</code> <code>kpnet_hidden_channels</code> <code>int</code> <p>The number of hidden channels in the kernel predictor network.</p> <code>64</code> <code>kpnet_conv_size</code> <code>int</code> <p>The kernel size of the convolutional layers in the kernel predictor network.</p> <code>3</code> <code>kpnet_dropout</code> <code>float</code> <p>The dropout rate for the kernel predictor network.</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>cond_hop_length</code> <code>int</code> <p>The hop length of the conditioning sequence.</p> <code>conv_layers</code> <code>int</code> <p>The number of convolutional layers.</p> <code>conv_kernel_size</code> <code>int</code> <p>The kernel size of the convolutional layers.</p> <code>kernel_predictor</code> <code>KernelPredictor</code> <p>The kernel predictor network.</p> <code>convt_pre</code> <code>Sequential</code> <p>The convolutional transpose layer.</p> <code>conv_blocks</code> <code>ModuleList</code> <p>The list of convolutional blocks.</p> Source code in <code>models/vocoder/univnet/lvc_block.py</code> <pre><code>class LVCBlock(Module):\n    r\"\"\"The location-variable convolutions block.\n\n    To efficiently capture the local information of the condition, location-variable convolution (LVC)\n    obtained better sound quality and speed while maintaining the model size.\n    The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram\n    as the input. The kernel predictor is connected to a residual stack. One kernel predictor simultaneously\n    predicts the kernels of all LVC layers in one residual stack.\n\n    Args:\n        in_channels (int): The number of input channels.\n        cond_channels (int): The number of conditioning channels.\n        stride (int): The stride of the convolutional layers.\n        dilations (List[int]): A list of dilation values for the convolutional layers.\n        lReLU_slope (float): The slope of the LeakyReLU activation function.\n        conv_kernel_size (int): The kernel size of the convolutional layers.\n        cond_hop_length (int): The hop length of the conditioning sequence.\n        kpnet_hidden_channels (int): The number of hidden channels in the kernel predictor network.\n        kpnet_conv_size (int): The kernel size of the convolutional layers in the kernel predictor network.\n        kpnet_dropout (float): The dropout rate for the kernel predictor network.\n\n    Attributes:\n        cond_hop_length (int): The hop length of the conditioning sequence.\n        conv_layers (int): The number of convolutional layers.\n        conv_kernel_size (int): The kernel size of the convolutional layers.\n        kernel_predictor (KernelPredictor): The kernel predictor network.\n        convt_pre (nn.Sequential): The convolutional transpose layer.\n        conv_blocks (nn.ModuleList): The list of convolutional blocks.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        cond_channels: int,\n        stride: int,\n        dilations: List[int] = [1, 3, 9, 27],\n        lReLU_slope: float = 0.2,\n        conv_kernel_size: int = 3,\n        cond_hop_length: int = 256,\n        kpnet_hidden_channels: int = 64,\n        kpnet_conv_size: int = 3,\n        kpnet_dropout: float = 0.0,\n    ):\n        super().__init__()\n\n        self.cond_hop_length = cond_hop_length\n        self.conv_layers = len(dilations)\n        self.conv_kernel_size = conv_kernel_size\n\n        self.kernel_predictor = KernelPredictor(\n            cond_channels=cond_channels,\n            conv_in_channels=in_channels,\n            conv_out_channels=2 * in_channels,\n            conv_layers=len(dilations),\n            conv_kernel_size=conv_kernel_size,\n            kpnet_hidden_channels=kpnet_hidden_channels,\n            kpnet_conv_size=kpnet_conv_size,\n            kpnet_dropout=kpnet_dropout,\n            lReLU_slope=lReLU_slope,\n        )\n\n        self.convt_pre = nn.Sequential(\n            nn.LeakyReLU(lReLU_slope),\n            nn.utils.parametrizations.weight_norm(\n                nn.ConvTranspose1d(\n                    in_channels,\n                    in_channels,\n                    2 * stride,\n                    stride=stride,\n                    padding=stride // 2 + stride % 2,\n                    output_padding=stride % 2,\n                ),\n            ),\n        )\n\n        self.conv_blocks = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.LeakyReLU(lReLU_slope),\n                    nn.utils.parametrizations.weight_norm(\n                        nn.Conv1d(\n                            in_channels,\n                            in_channels,\n                            conv_kernel_size,\n                            padding=dilation * (conv_kernel_size - 1) // 2,\n                            dilation=dilation,\n                        ),\n                    ),\n                    nn.LeakyReLU(lReLU_slope),\n                )\n                for dilation in dilations\n            ],\n        )\n\n    def forward(self, x: torch.Tensor, c: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward propagation of the location-variable convolutions.\n\n        Args:\n            x (Tensor): The input sequence (batch, in_channels, in_length).\n            c (Tensor): The conditioning sequence (batch, cond_channels, cond_length).\n\n        Returns:\n            Tensor: The output sequence (batch, in_channels, in_length).\n        \"\"\"\n        _, in_channels, _ = x.shape  # (B, c_g, L')\n\n        x = self.convt_pre(x)  # (B, c_g, stride * L')\n        kernels, bias = self.kernel_predictor(c)\n\n        for i, conv in enumerate(self.conv_blocks):\n            output = conv(x)  # (B, c_g, stride * L')\n\n            k = kernels[:, i, :, :, :, :]  # (B, 2 * c_g, c_g, kernel_size, cond_length)\n            b = bias[:, i, :, :]  # (B, 2 * c_g, cond_length)\n\n            output = self.location_variable_convolution(\n                output, k, b, hop_size=self.cond_hop_length,\n            )  # (B, 2 * c_g, stride * L'): LVC\n            x = x + torch.sigmoid(output[:, :in_channels, :]) * torch.tanh(\n                output[:, in_channels:, :],\n            )  # (B, c_g, stride * L'): GAU\n\n        return x\n\n    def location_variable_convolution(\n        self,\n        x: torch.Tensor,\n        kernel: torch.Tensor,\n        bias: torch.Tensor,\n        dilation: int = 1,\n        hop_size: int = 256,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Perform location-variable convolution operation on the input sequence (x) using the local convolution kernel.\n        Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n\n        Args:\n            x (Tensor): The input sequence (batch, in_channels, in_length).\n            kernel (Tensor): The local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length).\n            bias (Tensor): The bias for the local convolution (batch, out_channels, kernel_length).\n            dilation (int): The dilation of convolution.\n            hop_size (int): The hop_size of the conditioning sequence.\n\n        Returns:\n            (Tensor): The output sequence after performing local convolution. (batch, out_channels, in_length).\n        \"\"\"\n        batch, _, in_length = x.shape\n        batch, _, out_channels, kernel_size, kernel_length = kernel.shape\n        assert in_length == (\n            kernel_length * hop_size\n        ), \"length of (x, kernel) is not matched\"\n\n        padding = dilation * int((kernel_size - 1) / 2)\n        x = F.pad(\n            x, (padding, padding), \"constant\", 0,\n        )  # (batch, in_channels, in_length + 2*padding)\n        x = x.unfold(\n            2, hop_size + 2 * padding, hop_size,\n        )  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n        if hop_size &lt; dilation:\n            x = F.pad(x, (0, dilation), \"constant\", 0)\n        x = x.unfold(\n            3, dilation, dilation,\n        )  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n        x = x[:, :, :, :, :hop_size]\n        x = x.transpose(\n            3, 4,\n        )  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n        x = x.unfold(\n            4, kernel_size, 1,\n        )  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n        o = torch.einsum(\"bildsk,biokl-&gt;bolsd\", x, kernel)\n        o = o.contiguous(memory_format=torch.channels_last_3d)\n\n        bias = (\n            bias.unsqueeze(-1)\n            .unsqueeze(-1)\n            .contiguous(memory_format=torch.channels_last_3d)\n        )\n\n        o = o + bias\n        return o.contiguous().view(batch, out_channels, -1)\n\n    def remove_weight_norm(self) -&gt; None:\n        r\"\"\"Remove weight normalization from the convolutional layers in the LVCBlock.\n\n        This method removes weight normalization from the kernel predictor and all convolutional layers in the LVCBlock.\n        \"\"\"\n        self.kernel_predictor.remove_weight_norm()\n        parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n        for block in self.conv_blocks:\n            parametrize.remove_parametrizations(block[1], \"weight\") # type: ignore\n</code></pre>"},{"location":"models/vocoder/univnet/lvc_block/#models.vocoder.univnet.lvc_block.LVCBlock.forward","title":"<code>forward(x, c)</code>","text":"<p>Forward propagation of the location-variable convolutions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input sequence (batch, in_channels, in_length).</p> required <code>c</code> <code>Tensor</code> <p>The conditioning sequence (batch, cond_channels, cond_length).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output sequence (batch, in_channels, in_length).</p> Source code in <code>models/vocoder/univnet/lvc_block.py</code> <pre><code>def forward(self, x: torch.Tensor, c: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward propagation of the location-variable convolutions.\n\n    Args:\n        x (Tensor): The input sequence (batch, in_channels, in_length).\n        c (Tensor): The conditioning sequence (batch, cond_channels, cond_length).\n\n    Returns:\n        Tensor: The output sequence (batch, in_channels, in_length).\n    \"\"\"\n    _, in_channels, _ = x.shape  # (B, c_g, L')\n\n    x = self.convt_pre(x)  # (B, c_g, stride * L')\n    kernels, bias = self.kernel_predictor(c)\n\n    for i, conv in enumerate(self.conv_blocks):\n        output = conv(x)  # (B, c_g, stride * L')\n\n        k = kernels[:, i, :, :, :, :]  # (B, 2 * c_g, c_g, kernel_size, cond_length)\n        b = bias[:, i, :, :]  # (B, 2 * c_g, cond_length)\n\n        output = self.location_variable_convolution(\n            output, k, b, hop_size=self.cond_hop_length,\n        )  # (B, 2 * c_g, stride * L'): LVC\n        x = x + torch.sigmoid(output[:, :in_channels, :]) * torch.tanh(\n            output[:, in_channels:, :],\n        )  # (B, c_g, stride * L'): GAU\n\n    return x\n</code></pre>"},{"location":"models/vocoder/univnet/lvc_block/#models.vocoder.univnet.lvc_block.LVCBlock.location_variable_convolution","title":"<code>location_variable_convolution(x, kernel, bias, dilation=1, hop_size=256)</code>","text":"<p>Perform location-variable convolution operation on the input sequence (x) using the local convolution kernel. Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input sequence (batch, in_channels, in_length).</p> required <code>kernel</code> <code>Tensor</code> <p>The local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length).</p> required <code>bias</code> <code>Tensor</code> <p>The bias for the local convolution (batch, out_channels, kernel_length).</p> required <code>dilation</code> <code>int</code> <p>The dilation of convolution.</p> <code>1</code> <code>hop_size</code> <code>int</code> <p>The hop_size of the conditioning sequence.</p> <code>256</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output sequence after performing local convolution. (batch, out_channels, in_length).</p> Source code in <code>models/vocoder/univnet/lvc_block.py</code> <pre><code>def location_variable_convolution(\n    self,\n    x: torch.Tensor,\n    kernel: torch.Tensor,\n    bias: torch.Tensor,\n    dilation: int = 1,\n    hop_size: int = 256,\n) -&gt; torch.Tensor:\n    r\"\"\"Perform location-variable convolution operation on the input sequence (x) using the local convolution kernel.\n    Time: 414 \u03bcs \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each), test on NVIDIA V100.\n\n    Args:\n        x (Tensor): The input sequence (batch, in_channels, in_length).\n        kernel (Tensor): The local convolution kernel (batch, in_channel, out_channels, kernel_size, kernel_length).\n        bias (Tensor): The bias for the local convolution (batch, out_channels, kernel_length).\n        dilation (int): The dilation of convolution.\n        hop_size (int): The hop_size of the conditioning sequence.\n\n    Returns:\n        (Tensor): The output sequence after performing local convolution. (batch, out_channels, in_length).\n    \"\"\"\n    batch, _, in_length = x.shape\n    batch, _, out_channels, kernel_size, kernel_length = kernel.shape\n    assert in_length == (\n        kernel_length * hop_size\n    ), \"length of (x, kernel) is not matched\"\n\n    padding = dilation * int((kernel_size - 1) / 2)\n    x = F.pad(\n        x, (padding, padding), \"constant\", 0,\n    )  # (batch, in_channels, in_length + 2*padding)\n    x = x.unfold(\n        2, hop_size + 2 * padding, hop_size,\n    )  # (batch, in_channels, kernel_length, hop_size + 2*padding)\n\n    if hop_size &lt; dilation:\n        x = F.pad(x, (0, dilation), \"constant\", 0)\n    x = x.unfold(\n        3, dilation, dilation,\n    )  # (batch, in_channels, kernel_length, (hop_size + 2*padding)/dilation, dilation)\n    x = x[:, :, :, :, :hop_size]\n    x = x.transpose(\n        3, 4,\n    )  # (batch, in_channels, kernel_length, dilation, (hop_size + 2*padding)/dilation)\n    x = x.unfold(\n        4, kernel_size, 1,\n    )  # (batch, in_channels, kernel_length, dilation, _, kernel_size)\n\n    o = torch.einsum(\"bildsk,biokl-&gt;bolsd\", x, kernel)\n    o = o.contiguous(memory_format=torch.channels_last_3d)\n\n    bias = (\n        bias.unsqueeze(-1)\n        .unsqueeze(-1)\n        .contiguous(memory_format=torch.channels_last_3d)\n    )\n\n    o = o + bias\n    return o.contiguous().view(batch, out_channels, -1)\n</code></pre>"},{"location":"models/vocoder/univnet/lvc_block/#models.vocoder.univnet.lvc_block.LVCBlock.remove_weight_norm","title":"<code>remove_weight_norm()</code>","text":"<p>Remove weight normalization from the convolutional layers in the LVCBlock.</p> <p>This method removes weight normalization from the kernel predictor and all convolutional layers in the LVCBlock.</p> Source code in <code>models/vocoder/univnet/lvc_block.py</code> <pre><code>def remove_weight_norm(self) -&gt; None:\n    r\"\"\"Remove weight normalization from the convolutional layers in the LVCBlock.\n\n    This method removes weight normalization from the kernel predictor and all convolutional layers in the LVCBlock.\n    \"\"\"\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\") # type: ignore\n</code></pre>"},{"location":"models/vocoder/univnet/multi_period_discriminator/","title":"Multi Period Discriminator","text":""},{"location":"models/vocoder/univnet/multi_period_discriminator/#models.vocoder.univnet.multi_period_discriminator.MultiPeriodDiscriminator","title":"<code>MultiPeriodDiscriminator</code>","text":"<p>             Bases: <code>Module</code></p> <p>MultiPeriodDiscriminator is a class that implements a multi-period discriminator network for the UnivNet vocoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>VocoderModelConfig</code> <p>The configuration object for the UnivNet vocoder model.</p> required Source code in <code>models/vocoder/univnet/multi_period_discriminator.py</code> <pre><code>class MultiPeriodDiscriminator(Module):\n    r\"\"\"MultiPeriodDiscriminator is a class that implements a multi-period discriminator network for the UnivNet vocoder.\n\n    Args:\n        model_config (VocoderModelConfig): The configuration object for the UnivNet vocoder model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: VocoderModelConfig,\n    ):\n        super().__init__()\n\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorP(period, model_config=model_config)\n                for period in model_config.mpd.periods\n            ],\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n        r\"\"\"Forward pass of the multi-period discriminator network.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, channels, time_steps).\n\n        Returns:\n            list: A list of output tensors from each discriminator network.\n        \"\"\"\n        return [disc(x) for disc in self.discriminators]\n</code></pre>"},{"location":"models/vocoder/univnet/multi_period_discriminator/#models.vocoder.univnet.multi_period_discriminator.MultiPeriodDiscriminator.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the multi-period discriminator network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, channels, time_steps).</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[Tensor]</code> <p>A list of output tensors from each discriminator network.</p> Source code in <code>models/vocoder/univnet/multi_period_discriminator.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n    r\"\"\"Forward pass of the multi-period discriminator network.\n\n    Args:\n        x (torch.Tensor): The input tensor of shape (batch_size, channels, time_steps).\n\n    Returns:\n        list: A list of output tensors from each discriminator network.\n    \"\"\"\n    return [disc(x) for disc in self.discriminators]\n</code></pre>"},{"location":"models/vocoder/univnet/multi_resolution_discriminator/","title":"Multi Resolution Discriminator","text":""},{"location":"models/vocoder/univnet/multi_resolution_discriminator/#models.vocoder.univnet.multi_resolution_discriminator.MultiResolutionDiscriminator","title":"<code>MultiResolutionDiscriminator</code>","text":"<p>             Bases: <code>Module</code></p> <p>Multi-resolution discriminator for the UnivNet vocoder.</p> <p>This class implements a multi-resolution discriminator that consists of multiple DiscriminatorR instances, each operating at a different resolution.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>VocoderModelConfig</code> <p>Model configuration object.</p> required <p>Attributes:</p> Name Type Description <code>resolutions</code> <code>list</code> <p>List of resolutions for each DiscriminatorR instance.</p> <code>discriminators</code> <code>ModuleList</code> <p>List of DiscriminatorR instances.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the forward pass of the multi-resolution discriminator.</p> Source code in <code>models/vocoder/univnet/multi_resolution_discriminator.py</code> <pre><code>class MultiResolutionDiscriminator(Module):\n    r\"\"\"Multi-resolution discriminator for the UnivNet vocoder.\n\n    This class implements a multi-resolution discriminator that consists of multiple DiscriminatorR instances, each operating at a different resolution.\n\n    Args:\n        model_config (VocoderModelConfig): Model configuration object.\n\n    Attributes:\n        resolutions (list): List of resolutions for each DiscriminatorR instance.\n        discriminators (nn.ModuleList): List of DiscriminatorR instances.\n\n    Methods:\n        forward(x): Computes the forward pass of the multi-resolution discriminator.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: VocoderModelConfig,\n    ):\n        super().__init__()\n\n        self.resolutions = model_config.mrd.resolutions\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorR(resolution, model_config=model_config)\n                for resolution in self.resolutions\n            ],\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n        r\"\"\"Computes the forward pass of the multi-resolution discriminator.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape [B, C, T].\n\n        Returns:\n            list: List of tuples containing the intermediate feature maps and the output scores for each `DiscriminatorR` instance.\n        \"\"\"\n        return [disc(x) for disc in self.discriminators] # [(feat, score), (feat, score), (feat, score)]\n</code></pre>"},{"location":"models/vocoder/univnet/multi_resolution_discriminator/#models.vocoder.univnet.multi_resolution_discriminator.MultiResolutionDiscriminator.forward","title":"<code>forward(x)</code>","text":"<p>Computes the forward pass of the multi-resolution discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [B, C, T].</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[tuple[Tensor, Tensor]]</code> <p>List of tuples containing the intermediate feature maps and the output scores for each <code>DiscriminatorR</code> instance.</p> Source code in <code>models/vocoder/univnet/multi_resolution_discriminator.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n    r\"\"\"Computes the forward pass of the multi-resolution discriminator.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [B, C, T].\n\n    Returns:\n        list: List of tuples containing the intermediate feature maps and the output scores for each `DiscriminatorR` instance.\n    \"\"\"\n    return [disc(x) for disc in self.discriminators] # [(feat, score), (feat, score), (feat, score)]\n</code></pre>"},{"location":"models/vocoder/univnet/readme/","title":"References","text":""},{"location":"models/vocoder/univnet/readme/#references","title":"References","text":"<p>UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</p> <p>Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch. </p>"},{"location":"models/vocoder/univnet/readme/#univnet","title":"Univnet","text":"<p>The core module for the training.</p>"},{"location":"models/vocoder/univnet/readme/#generator","title":"Generator","text":"<p>UnivNet Generator. Initializes the Generator module.</p>"},{"location":"models/vocoder/univnet/readme/#traced-generator","title":"Traced Generator","text":"<p>A traced version of the UnivNet Generator class that can be used for faster inference.</p>"},{"location":"models/vocoder/univnet/readme/#kernel-predictor","title":"Kernel Predictor","text":"<p>KernelPredictor is a class that predicts the kernel size for the convolutional layers in the UnivNet model. The kernels of the LVC layers are predicted using a kernel predictor that takes the log-mel-spectrogram as the input.</p>"},{"location":"models/vocoder/univnet/readme/#lvc-block","title":"LVC Block","text":"<p>The location-variable convolutions block. To efficiently capture the local information of the condition, location-variable convolution (LVC) obtained better sound quality and speed while maintaining the model size.</p>"},{"location":"models/vocoder/univnet/readme/#discriminator","title":"Discriminator","text":"<p>Discriminator for the UnuvNet vocoder.</p> <p>This class implements a discriminator that consists of a <code>MultiResolutionDiscriminator</code> and a <code>MultiPeriodDiscriminator</code>.</p>"},{"location":"models/vocoder/univnet/readme/#multiperioddiscriminator","title":"MultiPeriodDiscriminator","text":"<p><code>MultiPeriodDiscriminator</code> is a class that implements a multi-period discriminator network for the UnivNet vocoder.</p>"},{"location":"models/vocoder/univnet/readme/#discriminatorp","title":"DiscriminatorP","text":"<p><code>DiscriminatorP</code> is a class that implements a discriminator network for the UnivNet vocoder.</p>"},{"location":"models/vocoder/univnet/readme/#discriminatorr","title":"DiscriminatorR","text":"<p>A class representing the Residual Discriminator network for a UnivNet vocoder.</p>"},{"location":"models/vocoder/univnet/readme/#multiresolutiondiscriminator","title":"MultiResolutionDiscriminator","text":"<p>Multi-resolution discriminator for the UnivNet vocoder.</p>"},{"location":"models/vocoder/univnet/traced_generator/","title":"Traced Generator","text":""},{"location":"models/vocoder/univnet/traced_generator/#models.vocoder.univnet.traced_generator.TracedGenerator","title":"<code>TracedGenerator</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>models/vocoder/univnet/traced_generator.py</code> <pre><code>class TracedGenerator(Module):\n    def __init__(\n        self,\n        generator: Generator,\n        # example_inputs: Tuple[Any], example_inputs (Tuple[Any]): Example inputs to use for tracing.\n    ):\n        r\"\"\"A traced version of the UnivNet class that can be used for faster inference.\n\n        Args:\n            generator (UnivNet): The UnivNet instance to trace.\n        \"\"\"\n        super().__init__()\n\n        self.mel_mask_value: float = generator.mel_mask_value\n        self.hop_length: int = generator.hop_length\n\n        # TODO: https://github.com/Lightning-AI/lightning/issues/14036\n        # Disable trace since model is non-deterministic\n        # self.generator = torch.jit.trace(generator, example_inputs, check_trace=False)\n        # self.generator = generator.to_torchscript(\n        #     method=\"trace\", example_inputs=example_inputs, check_trace=False\n        # )\n        self.generator = generator\n\n    def forward(self, c: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Forward pass of the traced UnivNet.\n\n        Args:\n            c (torch.Tensor): The input mel-spectrogram tensor.\n            mel_lens (torch.Tensor): The lengths of the input mel-spectrograms.\n\n        Returns:\n            torch.Tensor: The generated audio tensor.\n        \"\"\"\n        mel_mask = get_mask_from_lengths(mel_lens).unsqueeze(1).to(c.device)\n        c = c.masked_fill(mel_mask, self.mel_mask_value)\n        zero = torch.full(\n            (c.shape[0], c.shape[1], 10), self.mel_mask_value, device=c.device,\n        )\n        mel = torch.cat((c, zero), dim=2)\n        audio = self.generator(mel)\n        audio = audio[:, :, : -(self.hop_length * 10)]\n        audio_mask = get_mask_from_lengths(mel_lens * 256).unsqueeze(1)\n        return audio.masked_fill(audio_mask, 0.0)\n</code></pre>"},{"location":"models/vocoder/univnet/traced_generator/#models.vocoder.univnet.traced_generator.TracedGenerator.__init__","title":"<code>__init__(generator)</code>","text":"<p>A traced version of the UnivNet class that can be used for faster inference.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>UnivNet</code> <p>The UnivNet instance to trace.</p> required Source code in <code>models/vocoder/univnet/traced_generator.py</code> <pre><code>def __init__(\n    self,\n    generator: Generator,\n    # example_inputs: Tuple[Any], example_inputs (Tuple[Any]): Example inputs to use for tracing.\n):\n    r\"\"\"A traced version of the UnivNet class that can be used for faster inference.\n\n    Args:\n        generator (UnivNet): The UnivNet instance to trace.\n    \"\"\"\n    super().__init__()\n\n    self.mel_mask_value: float = generator.mel_mask_value\n    self.hop_length: int = generator.hop_length\n\n    # TODO: https://github.com/Lightning-AI/lightning/issues/14036\n    # Disable trace since model is non-deterministic\n    # self.generator = torch.jit.trace(generator, example_inputs, check_trace=False)\n    # self.generator = generator.to_torchscript(\n    #     method=\"trace\", example_inputs=example_inputs, check_trace=False\n    # )\n    self.generator = generator\n</code></pre>"},{"location":"models/vocoder/univnet/traced_generator/#models.vocoder.univnet.traced_generator.TracedGenerator.forward","title":"<code>forward(c, mel_lens)</code>","text":"<p>Forward pass of the traced UnivNet.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The input mel-spectrogram tensor.</p> required <code>mel_lens</code> <code>Tensor</code> <p>The lengths of the input mel-spectrograms.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The generated audio tensor.</p> Source code in <code>models/vocoder/univnet/traced_generator.py</code> <pre><code>def forward(self, c: torch.Tensor, mel_lens: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Forward pass of the traced UnivNet.\n\n    Args:\n        c (torch.Tensor): The input mel-spectrogram tensor.\n        mel_lens (torch.Tensor): The lengths of the input mel-spectrograms.\n\n    Returns:\n        torch.Tensor: The generated audio tensor.\n    \"\"\"\n    mel_mask = get_mask_from_lengths(mel_lens).unsqueeze(1).to(c.device)\n    c = c.masked_fill(mel_mask, self.mel_mask_value)\n    zero = torch.full(\n        (c.shape[0], c.shape[1], 10), self.mel_mask_value, device=c.device,\n    )\n    mel = torch.cat((c, zero), dim=2)\n    audio = self.generator(mel)\n    audio = audio[:, :, : -(self.hop_length * 10)]\n    audio_mask = get_mask_from_lengths(mel_lens * 256).unsqueeze(1)\n    return audio.masked_fill(audio_mask, 0.0)\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/","title":"Univnet","text":""},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet","title":"<code>UnivNet</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Univnet module.</p> <p>This module contains the <code>Generator</code> and <code>Discriminator</code> models, and handles training and optimization.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>class UnivNet(LightningModule):\n    r\"\"\"Univnet module.\n\n    This module contains the `Generator` and `Discriminator` models, and handles training and optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        fine_tuning: bool = False,\n        lang: str = \"en\",\n        acc_grad_steps: int = 10,\n        batch_size: int = 6,\n        root: str = \"datasets_cache/LIBRITTS\",\n        checkpoint_path_v1: Optional[str] = \"checkpoints/vocoder_pretrained.pt\",\n    ):\n        r\"\"\"Initializes the `VocoderModule`.\n\n        Args:\n            fine_tuning (bool, optional): Whether to use fine-tuning mode or not. Defaults to False.\n            lang (str): Language of the dataset.\n            acc_grad_steps (int): Accumulated gradient steps.\n            batch_size (int): The batch size.\n            root (str, optional): The root directory for the dataset. Defaults to \"datasets_cache/LIBRITTS\".\n            checkpoint_path_v1 (str, optional): The path to the checkpoint for the model. If provided, the model weights will be loaded from this checkpoint. Defaults to None.\n        \"\"\"\n        super().__init__()\n\n        # Switch to manual optimization\n        self.automatic_optimization = False\n        self.acc_grad_steps = acc_grad_steps\n        self.batch_size = batch_size\n\n        self.lang = lang\n        self.root = root\n\n        model_config = VocoderModelConfig()\n        preprocess_config = PreprocessingConfig(\"english_only\")\n\n        self.univnet = Generator(\n            model_config=model_config,\n            preprocess_config=preprocess_config,\n        )\n        self.discriminator = Discriminator(model_config=model_config)\n\n        # Initialize SWA\n        self.swa_averaged_univnet = swa_utils.AveragedModel(self.univnet)\n        self.swa_averaged_discriminator = swa_utils.AveragedModel(self.discriminator)\n\n        self.loss = UnivnetLoss()\n\n        self.train_config: VoicoderTrainingConfig = (\n            VocoderFinetuningConfig() if fine_tuning else VocoderPretrainingConfig()\n        )\n\n        # NOTE: this code is used only for the v0.1.0 checkpoint.\n        # In the future, this code will be removed!\n        self.checkpoint_path_v1 = checkpoint_path_v1\n        if checkpoint_path_v1 is not None:\n            generator, discriminator, _, _ = self.get_weights_v1(checkpoint_path_v1)\n            self.univnet.load_state_dict(generator, strict=False)\n            self.discriminator.load_state_dict(discriminator, strict=False)\n\n    def get_weights_v1(self, checkpoint_path: str) -&gt; Tuple[dict, dict, dict, dict]:\n        r\"\"\"NOTE: this method is used only for the v0.1.0 checkpoint.\n        Prepares the weights for the model.\n\n        This is required for the model to be loaded from the checkpoint.\n\n        Args:\n            checkpoint_path (str): The path to the checkpoint.\n\n        Returns:\n            Tuple[dict, dict, dict, dict]: The weights for the generator and discriminator.\n        \"\"\"\n        ckpt_acoustic = torch.load(checkpoint_path)\n\n        return (\n            ckpt_acoustic[\"generator\"],\n            ckpt_acoustic[\"discriminator\"],\n            ckpt_acoustic[\"optim_g\"],\n            ckpt_acoustic[\"optim_d\"],\n        )\n\n    def forward(self, y_pred: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Performs a forward pass through the UnivNet model.\n\n        Args:\n            y_pred (torch.Tensor): The predicted mel spectrogram.\n\n        Returns:\n            torch.Tensor: The output of the UnivNet model.\n        \"\"\"\n        mel_lens = torch.tensor(\n            [y_pred.shape[2]],\n            dtype=torch.int32,\n            device=y_pred.device,\n        )\n\n        wav_prediction = self.univnet.infer(y_pred, mel_lens)\n\n        return wav_prediction[0, 0]\n\n    def training_step(self, batch: List, batch_idx: int):\n        r\"\"\"Performs a training step for the model.\n\n        Args:\n            batch (List): The batch of data for training. The batch should contain the mel spectrogram, its length, the audio, and the speaker ID.\n            batch_idx (int): Index of the batch.\n\n        Returns:\n            dict: A dictionary containing the total loss for the generator and logs for tensorboard.\n        \"\"\"\n        (\n            _,\n            _,\n            _,\n            _,\n            _,\n            mels,\n            _,\n            _,\n            _,\n            _,\n            _,\n            wavs,\n            _,\n        ) = batch\n\n        # Access your optimizers\n        optimizers = self.optimizers()\n        schedulers = self.lr_schedulers()\n        opt_univnet: Optimizer = optimizers[0]  # type: ignore\n        sch_univnet: ExponentialLR = schedulers[0]  # type: ignore\n\n        opt_discriminator: Optimizer = optimizers[1]  # type: ignore\n        sch_discriminator: ExponentialLR = schedulers[1]  # type: ignore\n\n        audio = wavs\n        fake_audio = self.univnet(mels)\n\n        res_fake, period_fake = self.discriminator(fake_audio.detach())\n        res_real, period_real = self.discriminator(audio)\n\n        (\n            total_loss_gen,\n            total_loss_disc,\n            stft_loss,\n            score_loss,\n            esr_loss,\n            snr_loss,\n        ) = self.loss.forward(\n            audio,\n            fake_audio,\n            res_fake,\n            period_fake,\n            res_real,\n            period_real,\n        )\n\n        self.log(\n            \"total_loss_gen\",\n            total_loss_gen,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\n            \"total_loss_disc\",\n            total_loss_disc,\n            sync_dist=True,\n            batch_size=self.batch_size,\n        )\n        self.log(\"stft_loss\", stft_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\"esr_loss\", esr_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\"snr_loss\", snr_loss, sync_dist=True, batch_size=self.batch_size)\n        self.log(\"score_loss\", score_loss, sync_dist=True, batch_size=self.batch_size)\n\n        # Perform manual optimization\n        self.manual_backward(total_loss_gen / self.acc_grad_steps, retain_graph=True)\n        self.manual_backward(total_loss_disc / self.acc_grad_steps, retain_graph=True)\n\n        # accumulate gradients of N batches\n        if (batch_idx + 1) % self.acc_grad_steps == 0:\n            # clip gradients\n            self.clip_gradients(\n                opt_univnet,\n                gradient_clip_val=0.5,\n                gradient_clip_algorithm=\"norm\",\n            )\n            self.clip_gradients(\n                opt_discriminator,\n                gradient_clip_val=0.5,\n                gradient_clip_algorithm=\"norm\",\n            )\n\n            # optimizer step\n            opt_univnet.step()\n            opt_discriminator.step()\n\n            # Scheduler step\n            sch_univnet.step()\n            sch_discriminator.step()\n\n            # zero the gradients\n            opt_univnet.zero_grad()\n            opt_discriminator.zero_grad()\n\n    def configure_optimizers(self):\n        r\"\"\"Configures the optimizers and learning rate schedulers for the `UnivNet` and `Discriminator` models.\n\n        This method creates an `AdamW` optimizer and an `ExponentialLR` scheduler for each model.\n        The learning rate, betas, and decay rate for the optimizers and schedulers are taken from the training configuration.\n\n        Returns\n            tuple: A tuple containing two dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.\n\n        Examples\n            ```python\n            vocoder_module = VocoderModule()\n            optimizers = vocoder_module.configure_optimizers()\n\n            print(optimizers)\n            (\n                {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3d90&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3e50&gt;},\n                {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3f10&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3fd0&gt;}\n            )\n            ```\n        \"\"\"\n        optim_univnet = AdamW(\n            self.univnet.parameters(),\n            self.train_config.learning_rate,\n            betas=(self.train_config.adam_b1, self.train_config.adam_b2),\n        )\n        scheduler_univnet = ExponentialLR(\n            optim_univnet,\n            gamma=self.train_config.lr_decay,\n            last_epoch=-1,\n        )\n\n        optim_discriminator = AdamW(\n            self.discriminator.parameters(),\n            self.train_config.learning_rate,\n            betas=(self.train_config.adam_b1, self.train_config.adam_b2),\n        )\n        scheduler_discriminator = ExponentialLR(\n            optim_discriminator,\n            gamma=self.train_config.lr_decay,\n            last_epoch=-1,\n        )\n\n        # NOTE: this code is used only for the v0.1.0 checkpoint.\n        # In the future, this code will be removed!\n        if self.checkpoint_path_v1 is not None:\n            _, _, optim_g, optim_d = self.get_weights_v1(self.checkpoint_path_v1)\n            optim_univnet.load_state_dict(optim_g)\n            optim_discriminator.load_state_dict(optim_d)\n\n        return (\n            {\"optimizer\": optim_univnet, \"lr_scheduler\": scheduler_univnet},\n            {\"optimizer\": optim_discriminator, \"lr_scheduler\": scheduler_discriminator},\n        )\n\n    def on_train_epoch_end(self):\n        r\"\"\"Updates the averaged model after each optimizer step with SWA.\"\"\"\n        self.swa_averaged_univnet.update_parameters(self.univnet)\n        self.swa_averaged_discriminator.update_parameters(self.discriminator)\n\n    def on_train_end(self):\n        # Update SWA model after training\n        swa_utils.update_bn(self.train_dataloader(), self.swa_averaged_univnet)\n        swa_utils.update_bn(self.train_dataloader(), self.swa_averaged_discriminator)\n\n    def train_dataloader(\n        self,\n        num_workers: int = 5,\n        root: str = \"datasets_cache/LIBRITTS\",\n        cache: bool = True,\n        cache_dir: str = \"datasets_cache\",\n        mem_cache: bool = False,\n        url: str = \"train-clean-360\",\n    ) -&gt; DataLoader:\n        r\"\"\"Returns the training dataloader, that is using the LibriTTS dataset.\n\n        Args:\n            num_workers (int): The number of workers.\n            root (str): The root directory of the dataset.\n            cache (bool): Whether to cache the preprocessed data.\n            cache_dir (str): The directory for the cache.\n            mem_cache (bool): Whether to use memory cache.\n            url (str): The URL of the dataset.\n\n        Returns:\n            DataLoader: The training and validation dataloaders.\n        \"\"\"\n        return train_dataloader(\n            batch_size=self.batch_size,\n            num_workers=num_workers,\n            root=root,\n            cache=cache,\n            cache_dir=cache_dir,\n            mem_cache=mem_cache,\n            url=url,\n            lang=self.lang,\n        )\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.__init__","title":"<code>__init__(fine_tuning=False, lang='en', acc_grad_steps=10, batch_size=6, root='datasets_cache/LIBRITTS', checkpoint_path_v1='checkpoints/vocoder_pretrained.pt')</code>","text":"<p>Initializes the <code>VocoderModule</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tuning</code> <code>bool</code> <p>Whether to use fine-tuning mode or not. Defaults to False.</p> <code>False</code> <code>lang</code> <code>str</code> <p>Language of the dataset.</p> <code>'en'</code> <code>acc_grad_steps</code> <code>int</code> <p>Accumulated gradient steps.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>6</code> <code>root</code> <code>str</code> <p>The root directory for the dataset. Defaults to \"datasets_cache/LIBRITTS\".</p> <code>'datasets_cache/LIBRITTS'</code> <code>checkpoint_path_v1</code> <code>str</code> <p>The path to the checkpoint for the model. If provided, the model weights will be loaded from this checkpoint. Defaults to None.</p> <code>'checkpoints/vocoder_pretrained.pt'</code> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def __init__(\n    self,\n    fine_tuning: bool = False,\n    lang: str = \"en\",\n    acc_grad_steps: int = 10,\n    batch_size: int = 6,\n    root: str = \"datasets_cache/LIBRITTS\",\n    checkpoint_path_v1: Optional[str] = \"checkpoints/vocoder_pretrained.pt\",\n):\n    r\"\"\"Initializes the `VocoderModule`.\n\n    Args:\n        fine_tuning (bool, optional): Whether to use fine-tuning mode or not. Defaults to False.\n        lang (str): Language of the dataset.\n        acc_grad_steps (int): Accumulated gradient steps.\n        batch_size (int): The batch size.\n        root (str, optional): The root directory for the dataset. Defaults to \"datasets_cache/LIBRITTS\".\n        checkpoint_path_v1 (str, optional): The path to the checkpoint for the model. If provided, the model weights will be loaded from this checkpoint. Defaults to None.\n    \"\"\"\n    super().__init__()\n\n    # Switch to manual optimization\n    self.automatic_optimization = False\n    self.acc_grad_steps = acc_grad_steps\n    self.batch_size = batch_size\n\n    self.lang = lang\n    self.root = root\n\n    model_config = VocoderModelConfig()\n    preprocess_config = PreprocessingConfig(\"english_only\")\n\n    self.univnet = Generator(\n        model_config=model_config,\n        preprocess_config=preprocess_config,\n    )\n    self.discriminator = Discriminator(model_config=model_config)\n\n    # Initialize SWA\n    self.swa_averaged_univnet = swa_utils.AveragedModel(self.univnet)\n    self.swa_averaged_discriminator = swa_utils.AveragedModel(self.discriminator)\n\n    self.loss = UnivnetLoss()\n\n    self.train_config: VoicoderTrainingConfig = (\n        VocoderFinetuningConfig() if fine_tuning else VocoderPretrainingConfig()\n    )\n\n    # NOTE: this code is used only for the v0.1.0 checkpoint.\n    # In the future, this code will be removed!\n    self.checkpoint_path_v1 = checkpoint_path_v1\n    if checkpoint_path_v1 is not None:\n        generator, discriminator, _, _ = self.get_weights_v1(checkpoint_path_v1)\n        self.univnet.load_state_dict(generator, strict=False)\n        self.discriminator.load_state_dict(discriminator, strict=False)\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizers and learning rate schedulers for the <code>UnivNet</code> and <code>Discriminator</code> models.</p> <p>This method creates an <code>AdamW</code> optimizer and an <code>ExponentialLR</code> scheduler for each model. The learning rate, betas, and decay rate for the optimizers and schedulers are taken from the training configuration.</p> <p>Returns     tuple: A tuple containing two dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.</p> <p>Examples     <pre><code>vocoder_module = VocoderModule()\noptimizers = vocoder_module.configure_optimizers()\n\nprint(optimizers)\n(\n    {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3d90&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3e50&gt;},\n    {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3f10&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3fd0&gt;}\n)\n</code></pre></p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def configure_optimizers(self):\n    r\"\"\"Configures the optimizers and learning rate schedulers for the `UnivNet` and `Discriminator` models.\n\n    This method creates an `AdamW` optimizer and an `ExponentialLR` scheduler for each model.\n    The learning rate, betas, and decay rate for the optimizers and schedulers are taken from the training configuration.\n\n    Returns\n        tuple: A tuple containing two dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.\n\n    Examples\n        ```python\n        vocoder_module = VocoderModule()\n        optimizers = vocoder_module.configure_optimizers()\n\n        print(optimizers)\n        (\n            {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3d90&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3e50&gt;},\n            {\"optimizer\": &lt;torch.optim.adamw.AdamW object at 0x7f8c0c0b3f10&gt;, \"lr_scheduler\": &lt;torch.optim.lr_scheduler.ExponentialLR object at 0x7f8c0c0b3fd0&gt;}\n        )\n        ```\n    \"\"\"\n    optim_univnet = AdamW(\n        self.univnet.parameters(),\n        self.train_config.learning_rate,\n        betas=(self.train_config.adam_b1, self.train_config.adam_b2),\n    )\n    scheduler_univnet = ExponentialLR(\n        optim_univnet,\n        gamma=self.train_config.lr_decay,\n        last_epoch=-1,\n    )\n\n    optim_discriminator = AdamW(\n        self.discriminator.parameters(),\n        self.train_config.learning_rate,\n        betas=(self.train_config.adam_b1, self.train_config.adam_b2),\n    )\n    scheduler_discriminator = ExponentialLR(\n        optim_discriminator,\n        gamma=self.train_config.lr_decay,\n        last_epoch=-1,\n    )\n\n    # NOTE: this code is used only for the v0.1.0 checkpoint.\n    # In the future, this code will be removed!\n    if self.checkpoint_path_v1 is not None:\n        _, _, optim_g, optim_d = self.get_weights_v1(self.checkpoint_path_v1)\n        optim_univnet.load_state_dict(optim_g)\n        optim_discriminator.load_state_dict(optim_d)\n\n    return (\n        {\"optimizer\": optim_univnet, \"lr_scheduler\": scheduler_univnet},\n        {\"optimizer\": optim_discriminator, \"lr_scheduler\": scheduler_discriminator},\n    )\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.forward","title":"<code>forward(y_pred)</code>","text":"<p>Performs a forward pass through the UnivNet model.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>The predicted mel spectrogram.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the UnivNet model.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def forward(self, y_pred: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Performs a forward pass through the UnivNet model.\n\n    Args:\n        y_pred (torch.Tensor): The predicted mel spectrogram.\n\n    Returns:\n        torch.Tensor: The output of the UnivNet model.\n    \"\"\"\n    mel_lens = torch.tensor(\n        [y_pred.shape[2]],\n        dtype=torch.int32,\n        device=y_pred.device,\n    )\n\n    wav_prediction = self.univnet.infer(y_pred, mel_lens)\n\n    return wav_prediction[0, 0]\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.get_weights_v1","title":"<code>get_weights_v1(checkpoint_path)</code>","text":"<p>NOTE: this method is used only for the v0.1.0 checkpoint. Prepares the weights for the model.</p> <p>This is required for the model to be loaded from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>The path to the checkpoint.</p> required <p>Returns:</p> Type Description <code>Tuple[dict, dict, dict, dict]</code> <p>Tuple[dict, dict, dict, dict]: The weights for the generator and discriminator.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def get_weights_v1(self, checkpoint_path: str) -&gt; Tuple[dict, dict, dict, dict]:\n    r\"\"\"NOTE: this method is used only for the v0.1.0 checkpoint.\n    Prepares the weights for the model.\n\n    This is required for the model to be loaded from the checkpoint.\n\n    Args:\n        checkpoint_path (str): The path to the checkpoint.\n\n    Returns:\n        Tuple[dict, dict, dict, dict]: The weights for the generator and discriminator.\n    \"\"\"\n    ckpt_acoustic = torch.load(checkpoint_path)\n\n    return (\n        ckpt_acoustic[\"generator\"],\n        ckpt_acoustic[\"discriminator\"],\n        ckpt_acoustic[\"optim_g\"],\n        ckpt_acoustic[\"optim_d\"],\n    )\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.on_train_epoch_end","title":"<code>on_train_epoch_end()</code>","text":"<p>Updates the averaged model after each optimizer step with SWA.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def on_train_epoch_end(self):\n    r\"\"\"Updates the averaged model after each optimizer step with SWA.\"\"\"\n    self.swa_averaged_univnet.update_parameters(self.univnet)\n    self.swa_averaged_discriminator.update_parameters(self.discriminator)\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.train_dataloader","title":"<code>train_dataloader(num_workers=5, root='datasets_cache/LIBRITTS', cache=True, cache_dir='datasets_cache', mem_cache=False, url='train-clean-360')</code>","text":"<p>Returns the training dataloader, that is using the LibriTTS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>The number of workers.</p> <code>5</code> <code>root</code> <code>str</code> <p>The root directory of the dataset.</p> <code>'datasets_cache/LIBRITTS'</code> <code>cache</code> <code>bool</code> <p>Whether to cache the preprocessed data.</p> <code>True</code> <code>cache_dir</code> <code>str</code> <p>The directory for the cache.</p> <code>'datasets_cache'</code> <code>mem_cache</code> <code>bool</code> <p>Whether to use memory cache.</p> <code>False</code> <code>url</code> <code>str</code> <p>The URL of the dataset.</p> <code>'train-clean-360'</code> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training and validation dataloaders.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def train_dataloader(\n    self,\n    num_workers: int = 5,\n    root: str = \"datasets_cache/LIBRITTS\",\n    cache: bool = True,\n    cache_dir: str = \"datasets_cache\",\n    mem_cache: bool = False,\n    url: str = \"train-clean-360\",\n) -&gt; DataLoader:\n    r\"\"\"Returns the training dataloader, that is using the LibriTTS dataset.\n\n    Args:\n        num_workers (int): The number of workers.\n        root (str): The root directory of the dataset.\n        cache (bool): Whether to cache the preprocessed data.\n        cache_dir (str): The directory for the cache.\n        mem_cache (bool): Whether to use memory cache.\n        url (str): The URL of the dataset.\n\n    Returns:\n        DataLoader: The training and validation dataloaders.\n    \"\"\"\n    return train_dataloader(\n        batch_size=self.batch_size,\n        num_workers=num_workers,\n        root=root,\n        cache=cache,\n        cache_dir=cache_dir,\n        mem_cache=mem_cache,\n        url=url,\n        lang=self.lang,\n    )\n</code></pre>"},{"location":"models/vocoder/univnet/univnet/#models.vocoder.univnet.univnet.UnivNet.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a training step for the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List</code> <p>The batch of data for training. The batch should contain the mel spectrogram, its length, the audio, and the speaker ID.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the total loss for the generator and logs for tensorboard.</p> Source code in <code>models/vocoder/univnet/univnet.py</code> <pre><code>def training_step(self, batch: List, batch_idx: int):\n    r\"\"\"Performs a training step for the model.\n\n    Args:\n        batch (List): The batch of data for training. The batch should contain the mel spectrogram, its length, the audio, and the speaker ID.\n        batch_idx (int): Index of the batch.\n\n    Returns:\n        dict: A dictionary containing the total loss for the generator and logs for tensorboard.\n    \"\"\"\n    (\n        _,\n        _,\n        _,\n        _,\n        _,\n        mels,\n        _,\n        _,\n        _,\n        _,\n        _,\n        wavs,\n        _,\n    ) = batch\n\n    # Access your optimizers\n    optimizers = self.optimizers()\n    schedulers = self.lr_schedulers()\n    opt_univnet: Optimizer = optimizers[0]  # type: ignore\n    sch_univnet: ExponentialLR = schedulers[0]  # type: ignore\n\n    opt_discriminator: Optimizer = optimizers[1]  # type: ignore\n    sch_discriminator: ExponentialLR = schedulers[1]  # type: ignore\n\n    audio = wavs\n    fake_audio = self.univnet(mels)\n\n    res_fake, period_fake = self.discriminator(fake_audio.detach())\n    res_real, period_real = self.discriminator(audio)\n\n    (\n        total_loss_gen,\n        total_loss_disc,\n        stft_loss,\n        score_loss,\n        esr_loss,\n        snr_loss,\n    ) = self.loss.forward(\n        audio,\n        fake_audio,\n        res_fake,\n        period_fake,\n        res_real,\n        period_real,\n    )\n\n    self.log(\n        \"total_loss_gen\",\n        total_loss_gen,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\n        \"total_loss_disc\",\n        total_loss_disc,\n        sync_dist=True,\n        batch_size=self.batch_size,\n    )\n    self.log(\"stft_loss\", stft_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\"esr_loss\", esr_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\"snr_loss\", snr_loss, sync_dist=True, batch_size=self.batch_size)\n    self.log(\"score_loss\", score_loss, sync_dist=True, batch_size=self.batch_size)\n\n    # Perform manual optimization\n    self.manual_backward(total_loss_gen / self.acc_grad_steps, retain_graph=True)\n    self.manual_backward(total_loss_disc / self.acc_grad_steps, retain_graph=True)\n\n    # accumulate gradients of N batches\n    if (batch_idx + 1) % self.acc_grad_steps == 0:\n        # clip gradients\n        self.clip_gradients(\n            opt_univnet,\n            gradient_clip_val=0.5,\n            gradient_clip_algorithm=\"norm\",\n        )\n        self.clip_gradients(\n            opt_discriminator,\n            gradient_clip_val=0.5,\n            gradient_clip_algorithm=\"norm\",\n        )\n\n        # optimizer step\n        opt_univnet.step()\n        opt_discriminator.step()\n\n        # Scheduler step\n        sch_univnet.step()\n        sch_discriminator.step()\n\n        # zero the gradients\n        opt_univnet.zero_grad()\n        opt_discriminator.zero_grad()\n</code></pre>"},{"location":"training/readme/","title":"References","text":""},{"location":"training/readme/#references","title":"References","text":"<p>Training code documentation. Here you can find the documentation for the training code and the training process. Also training tools and utilities are documented here.</p>"},{"location":"training/readme/#dataset","title":"Dataset","text":"<p>Preprocessed datasets for the training of Acoustic and Vocoder</p>"},{"location":"training/readme/#preprocess","title":"Preprocess","text":"<p>The preprocessing code for the datasets.</p>"},{"location":"training/readme/#loss","title":"Loss","text":"<p>Here you can find docs for the loss functions.</p>"},{"location":"training/readme/#training-tools","title":"Training tools","text":"<p>Tools for data preparation, training and evaluation.</p>"},{"location":"training/tools/","title":"Tools","text":""},{"location":"training/tools/#training.tools.pad_1D","title":"<code>pad_1D(inputs, pad_value=0.0)</code>","text":"<p>Pad a list of 1D tensor list to the same length.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Tensor]</code> <p>List of 1D numpy arrays to pad.</p> required <code>pad_value</code> <code>float</code> <p>Value to use for padding. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Padded 2D numpy array of shape (len(inputs), max_len), where max_len is the length of the longest input array.</p> Source code in <code>training/tools.py</code> <pre><code>def pad_1D(inputs: List[Tensor], pad_value: float = 0.0) -&gt; Tensor:\n    r\"\"\"Pad a list of 1D tensor list to the same length.\n\n    Args:\n        inputs (List[torch.Tensor]): List of 1D numpy arrays to pad.\n        pad_value (float): Value to use for padding. Default is 0.0.\n\n    Returns:\n        torch.Tensor: Padded 2D numpy array of shape (len(inputs), max_len), where max_len is the length of the longest input array.\n    \"\"\"\n    max_len = max(x.size(0) for x in inputs)\n    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(0)), value=pad_value) for x in inputs]\n    return torch.stack(padded_inputs)\n</code></pre>"},{"location":"training/tools/#training.tools.pad_2D","title":"<code>pad_2D(inputs, maxlen=None, pad_value=0.0)</code>","text":"<p>Pad a list of 2D tensor arrays to the same length.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Tensor]</code> <p>List of 2D numpy arrays to pad.</p> required <code>maxlen</code> <code>Union[int, None]</code> <p>Maximum length to pad the arrays to. If None, pad to the length of the longest array. Default is None.</p> <code>None</code> <code>pad_value</code> <code>float</code> <p>Value to use for padding. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Padded 3D numpy array of shape (len(inputs), max_len, input_dim), where max_len is the maximum length of the input arrays, and input_dim is the dimension of the input arrays.</p> Source code in <code>training/tools.py</code> <pre><code>def pad_2D(\n    inputs: List[Tensor], maxlen: Union[int, None] = None, pad_value: float = 0.0,\n) -&gt; Tensor:\n    r\"\"\"Pad a list of 2D tensor arrays to the same length.\n\n    Args:\n        inputs (List[torch.Tensor]): List of 2D numpy arrays to pad.\n        maxlen (Union[int, None]): Maximum length to pad the arrays to. If None, pad to the length of the longest array. Default is None.\n        pad_value (float): Value to use for padding. Default is 0.0.\n\n    Returns:\n        torch.Tensor: Padded 3D numpy array of shape (len(inputs), max_len, input_dim), where max_len is the maximum length of the input arrays, and input_dim is the dimension of the input arrays.\n    \"\"\"\n    max_len = max(x.size(1) for x in inputs) if maxlen is None else maxlen\n\n    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(1), 0, 0), value=pad_value) for x in inputs]\n    return torch.stack(padded_inputs)\n</code></pre>"},{"location":"training/tools/#training.tools.pad_3D","title":"<code>pad_3D(inputs, B, T, L)</code>","text":"<p>Pad a 3D torch tensor to a specified shape.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>3D numpy array to pad.</p> required <code>B</code> <code>int</code> <p>Batch size to pad the array to.</p> required <code>T</code> <code>int</code> <p>Time steps to pad the array to.</p> required <code>L</code> <code>int</code> <p>Length to pad the array to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Padded 3D numpy array of shape (B, T, L), where B is the batch size, T is the time steps, and L is the length.</p> Source code in <code>training/tools.py</code> <pre><code>def pad_3D(inputs: Union[Tensor, List[Tensor]], B: int, T: int, L: int) -&gt; Tensor:\n    r\"\"\"Pad a 3D torch tensor to a specified shape.\n\n    Args:\n        inputs (torch.Tensor): 3D numpy array to pad.\n        B (int): Batch size to pad the array to.\n        T (int): Time steps to pad the array to.\n        L (int): Length to pad the array to.\n\n    Returns:\n        torch.Tensor: Padded 3D numpy array of shape (B, T, L), where B is the batch size, T is the time steps, and L is the length.\n    \"\"\"\n    if isinstance(inputs, list):\n        inputs_padded = torch.zeros(B, T, L, dtype=inputs[0].dtype)\n        for i, input_ in enumerate(inputs):\n            inputs_padded[i, :input_.size(0), :input_.size(1)] = input_\n\n    elif isinstance(inputs, torch.Tensor):\n        inputs_padded = torch.zeros(B, T, L, dtype=inputs.dtype)\n        inputs_padded[:inputs.size(0), :inputs.size(1), :inputs.size(2)] = inputs\n\n    return inputs_padded\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/","title":"Libri TTS dataset acoustic","text":""},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic","title":"<code>LibriTTSDatasetAcoustic</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Loading preprocessed acoustic model data.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>class LibriTTSDatasetAcoustic(Dataset):\n    r\"\"\"Loading preprocessed acoustic model data.\"\"\"\n\n    def __init__(\n        self,\n        lang: str = \"en\",\n        root: str = \"datasets_cache/LIBRITTS\",\n        url: str = \"train-clean-360\",\n        download: bool = False,\n        cache: bool = False,\n        mem_cache: bool = False,\n        cache_dir: str = \"datasets_cache\",\n        selected_speaker_ids: Optional[List[int]] = None,\n    ):\n        r\"\"\"A PyTorch dataset for loading preprocessed acoustic data.\n\n        Args:\n            root (str): Path to the directory where the dataset is found or downloaded.\n            lang (str): The language of the dataset.\n            url (str): The dataset url, default \"train-clean-360\".\n            download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n            cache (bool, optional): Whether to cache the preprocessed data to RAM. Defaults to False.\n            mem_cache (bool, optional): Whether to cache the preprocessed data. Defaults to False.\n            cache_dir (str, optional): Path to the directory where the cache is stored. Defaults to \"datasets_cache\".\n            selected_speaker_ids (Optional[List[int]], optional): A list of selected speakers. Defaults to None.\n        \"\"\"\n        lang_map = get_lang_map(lang)\n        processing_lang_type = lang_map.processing_lang_type\n        preprocess_config = PreprocessingConfig(processing_lang_type)\n\n        self.dataset = LIBRITTS_R(\n            root=root,\n            download=download,\n            url=url,\n            selected_speaker_ids=selected_speaker_ids,\n            min_audio_length=preprocess_config.min_seconds,\n            max_audio_length=preprocess_config.max_seconds,\n        )\n        self.cache = cache\n\n        # Calculate the directory for the cache file\n        self.cache_subdir = lambda idx: str(((idx // 1000) + 1) * 1000)\n\n        self.cache_dir = os.path.join(cache_dir, f\"cache-{url}\")\n\n        self.mem_cache = mem_cache\n        self.memory_cache = {}\n\n        # Load the id_mapping dictionary from the JSON file\n        with open(\"speaker_id_mapping_libri.json\") as f:\n            self.id_mapping = json.load(f)\n\n        self.preprocess_libtts = PreprocessLibriTTS(\n            preprocess_config,\n            lang,\n        )\n\n    def __len__(self) -&gt; int:\n        r\"\"\"Returns the number of samples in the dataset.\n\n        Returns\n            int: Number of samples in the dataset.\n        \"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n        r\"\"\"Returns a sample from the dataset at the given index.\n\n        Args:\n            idx (int): Index of the sample to return.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the sample data.\n        \"\"\"\n        # Check if the data is in the memory cache\n        if self.mem_cache and idx in self.memory_cache:\n            return self.memory_cache[idx]\n\n        # Check if the data is in the cache\n        cache_subdir_path = os.path.join(self.cache_dir, self.cache_subdir(idx))\n        cache_file = os.path.join(cache_subdir_path, f\"{idx}.pt\")\n\n        # Check if the data is in the cache\n        if self.cache and os.path.exists(cache_file):\n            # If the data is in the cache, load it from the cache file and return it\n            data = torch.load(cache_file)\n            return data\n\n        # Retrive the dataset row\n        data = self.dataset[idx]\n\n        data = self.preprocess_libtts.acoustic(data)\n\n        # TODO: bad way to do filtering, fix this!\n        if data is None:\n            # print(\"Skipping due to preprocessing error\")\n            rand_idx = np.random.randint(0, self.__len__())\n            return self.__getitem__(rand_idx)\n\n        data.wav = data.wav.unsqueeze(0)\n\n        result = {\n            \"id\": data.utterance_id,\n            \"wav\": data.wav,\n            \"mel\": data.mel,\n            \"pitch\": data.pitch,\n            \"text\": data.phones,\n            \"attn_prior\": data.attn_prior,\n            \"energy\": data.energy,\n            \"raw_text\": data.raw_text,\n            \"normalized_text\": data.normalized_text,\n            \"speaker\": self.id_mapping.get(str(data.speaker_id)),\n            \"pitch_is_normalized\": data.pitch_is_normalized,\n            # TODO: fix lang!\n            \"lang\": lang2id[\"en\"],\n        }\n\n        # Add the data to the memory cache\n        if self.mem_cache:\n            self.memory_cache[idx] = result\n\n        if self.cache:\n            # Create the cache subdirectory if it doesn't exist\n            os.makedirs(cache_subdir_path, exist_ok=True)\n\n            # Save the preprocessed data to the cache\n            torch.save(result, cache_file)\n\n        return result\n\n    def __iter__(self):\n        r\"\"\"Method makes the class iterable. It iterates over the `_walker` attribute\n        and for each item, it gets the corresponding item from the dataset using the\n        `__getitem__` method.\n\n        Yields:\n        The item from the dataset corresponding to the current item in `_walker`.\n        \"\"\"\n        for item in range(self.__len__()):\n            yield self.__getitem__(item)\n\n    def collate_fn(self, data: List) -&gt; List:\n        r\"\"\"Collates a batch of data samples.\n\n        Args:\n            data (List): A list of data samples.\n\n        Returns:\n            List: A list of reprocessed data batches.\n        \"\"\"\n        data_size = len(data)\n\n        idxs = list(range(data_size))\n\n        # Initialize empty lists to store extracted values\n        empty_lists: List[List] = [[] for _ in range(12)]\n        (\n            ids,\n            speakers,\n            texts,\n            raw_texts,\n            mels,\n            pitches,\n            attn_priors,\n            langs,\n            src_lens,\n            mel_lens,\n            wavs,\n            energy,\n        ) = empty_lists\n\n        # Extract fields from data dictionary and populate the lists\n        for idx in idxs:\n            data_entry = data[idx]\n            ids.append(data_entry[\"id\"])\n            speakers.append(data_entry[\"speaker\"])\n            texts.append(data_entry[\"text\"])\n            raw_texts.append(data_entry[\"raw_text\"])\n            mels.append(data_entry[\"mel\"])\n            pitches.append(data_entry[\"pitch\"])\n            attn_priors.append(data_entry[\"attn_prior\"])\n            langs.append(data_entry[\"lang\"])\n            src_lens.append(data_entry[\"text\"].shape[0])\n            mel_lens.append(data_entry[\"mel\"].shape[1])\n            wavs.append(data_entry[\"wav\"])\n            energy.append(data_entry[\"energy\"])\n\n        # Convert langs, src_lens, and mel_lens to numpy arrays\n        langs = np.array(langs)\n        src_lens = np.array(src_lens)\n        mel_lens = np.array(mel_lens)\n\n        # NOTE: Instead of the pitches for the whole dataset, used stat for the batch\n        # Take only min and max values for pitch\n        pitches_stat = list(self.normalize_pitch(pitches)[:2])\n\n        texts = pad_1D(texts)\n        mels = pad_2D(mels)\n        pitches = pad_1D(pitches)\n        attn_priors = pad_3D(attn_priors, len(idxs), max(src_lens), max(mel_lens))\n\n        speakers = np.repeat(\n            np.expand_dims(np.array(speakers), axis=1),\n            texts.shape[1],\n            axis=1,\n        )\n        langs = np.repeat(\n            np.expand_dims(np.array(langs), axis=1),\n            texts.shape[1],\n            axis=1,\n        )\n\n        wavs = pad_2D(wavs)\n        energy = pad_2D(energy)\n\n        return [\n            ids,\n            raw_texts,\n            torch.from_numpy(speakers),\n            texts.int(),\n            torch.from_numpy(src_lens),\n            mels,\n            pitches,\n            pitches_stat,\n            torch.from_numpy(mel_lens),\n            torch.from_numpy(langs),\n            attn_priors,\n            wavs,\n            energy,\n        ]\n\n    def normalize_pitch(\n        self,\n        pitches: List[torch.Tensor],\n    ) -&gt; Tuple[float, float, float, float]:\n        r\"\"\"Normalizes the pitch values.\n\n        Args:\n            pitches (List[torch.Tensor]): A list of pitch values.\n\n        Returns:\n            Tuple: A tuple containing the normalized pitch values.\n        \"\"\"\n        pitches_t = torch.concatenate(pitches)\n\n        min_value = torch.min(pitches_t).item()\n        max_value = torch.max(pitches_t).item()\n\n        mean = torch.mean(pitches_t).item()\n        std = torch.std(pitches_t).item()\n\n        return min_value, max_value, mean, std\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample from the dataset at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to return.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the sample data.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    r\"\"\"Returns a sample from the dataset at the given index.\n\n    Args:\n        idx (int): Index of the sample to return.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the sample data.\n    \"\"\"\n    # Check if the data is in the memory cache\n    if self.mem_cache and idx in self.memory_cache:\n        return self.memory_cache[idx]\n\n    # Check if the data is in the cache\n    cache_subdir_path = os.path.join(self.cache_dir, self.cache_subdir(idx))\n    cache_file = os.path.join(cache_subdir_path, f\"{idx}.pt\")\n\n    # Check if the data is in the cache\n    if self.cache and os.path.exists(cache_file):\n        # If the data is in the cache, load it from the cache file and return it\n        data = torch.load(cache_file)\n        return data\n\n    # Retrive the dataset row\n    data = self.dataset[idx]\n\n    data = self.preprocess_libtts.acoustic(data)\n\n    # TODO: bad way to do filtering, fix this!\n    if data is None:\n        # print(\"Skipping due to preprocessing error\")\n        rand_idx = np.random.randint(0, self.__len__())\n        return self.__getitem__(rand_idx)\n\n    data.wav = data.wav.unsqueeze(0)\n\n    result = {\n        \"id\": data.utterance_id,\n        \"wav\": data.wav,\n        \"mel\": data.mel,\n        \"pitch\": data.pitch,\n        \"text\": data.phones,\n        \"attn_prior\": data.attn_prior,\n        \"energy\": data.energy,\n        \"raw_text\": data.raw_text,\n        \"normalized_text\": data.normalized_text,\n        \"speaker\": self.id_mapping.get(str(data.speaker_id)),\n        \"pitch_is_normalized\": data.pitch_is_normalized,\n        # TODO: fix lang!\n        \"lang\": lang2id[\"en\"],\n    }\n\n    # Add the data to the memory cache\n    if self.mem_cache:\n        self.memory_cache[idx] = result\n\n    if self.cache:\n        # Create the cache subdirectory if it doesn't exist\n        os.makedirs(cache_subdir_path, exist_ok=True)\n\n        # Save the preprocessed data to the cache\n        torch.save(result, cache_file)\n\n    return result\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.__init__","title":"<code>__init__(lang='en', root='datasets_cache/LIBRITTS', url='train-clean-360', download=False, cache=False, mem_cache=False, cache_dir='datasets_cache', selected_speaker_ids=None)</code>","text":"<p>A PyTorch dataset for loading preprocessed acoustic data.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the directory where the dataset is found or downloaded.</p> <code>'datasets_cache/LIBRITTS'</code> <code>lang</code> <code>str</code> <p>The language of the dataset.</p> <code>'en'</code> <code>url</code> <code>str</code> <p>The dataset url, default \"train-clean-360\".</p> <code>'train-clean-360'</code> <code>download</code> <code>bool</code> <p>Whether to download the dataset if it is not found. Defaults to True.</p> <code>False</code> <code>cache</code> <code>bool</code> <p>Whether to cache the preprocessed data to RAM. Defaults to False.</p> <code>False</code> <code>mem_cache</code> <code>bool</code> <p>Whether to cache the preprocessed data. Defaults to False.</p> <code>False</code> <code>cache_dir</code> <code>str</code> <p>Path to the directory where the cache is stored. Defaults to \"datasets_cache\".</p> <code>'datasets_cache'</code> <code>selected_speaker_ids</code> <code>Optional[List[int]]</code> <p>A list of selected speakers. Defaults to None.</p> <code>None</code> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def __init__(\n    self,\n    lang: str = \"en\",\n    root: str = \"datasets_cache/LIBRITTS\",\n    url: str = \"train-clean-360\",\n    download: bool = False,\n    cache: bool = False,\n    mem_cache: bool = False,\n    cache_dir: str = \"datasets_cache\",\n    selected_speaker_ids: Optional[List[int]] = None,\n):\n    r\"\"\"A PyTorch dataset for loading preprocessed acoustic data.\n\n    Args:\n        root (str): Path to the directory where the dataset is found or downloaded.\n        lang (str): The language of the dataset.\n        url (str): The dataset url, default \"train-clean-360\".\n        download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n        cache (bool, optional): Whether to cache the preprocessed data to RAM. Defaults to False.\n        mem_cache (bool, optional): Whether to cache the preprocessed data. Defaults to False.\n        cache_dir (str, optional): Path to the directory where the cache is stored. Defaults to \"datasets_cache\".\n        selected_speaker_ids (Optional[List[int]], optional): A list of selected speakers. Defaults to None.\n    \"\"\"\n    lang_map = get_lang_map(lang)\n    processing_lang_type = lang_map.processing_lang_type\n    preprocess_config = PreprocessingConfig(processing_lang_type)\n\n    self.dataset = LIBRITTS_R(\n        root=root,\n        download=download,\n        url=url,\n        selected_speaker_ids=selected_speaker_ids,\n        min_audio_length=preprocess_config.min_seconds,\n        max_audio_length=preprocess_config.max_seconds,\n    )\n    self.cache = cache\n\n    # Calculate the directory for the cache file\n    self.cache_subdir = lambda idx: str(((idx // 1000) + 1) * 1000)\n\n    self.cache_dir = os.path.join(cache_dir, f\"cache-{url}\")\n\n    self.mem_cache = mem_cache\n    self.memory_cache = {}\n\n    # Load the id_mapping dictionary from the JSON file\n    with open(\"speaker_id_mapping_libri.json\") as f:\n        self.id_mapping = json.load(f)\n\n    self.preprocess_libtts = PreprocessLibriTTS(\n        preprocess_config,\n        lang,\n    )\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.__iter__","title":"<code>__iter__()</code>","text":"<p>Method makes the class iterable. It iterates over the <code>_walker</code> attribute and for each item, it gets the corresponding item from the dataset using the <code>__getitem__</code> method.</p> <p>Yields: The item from the dataset corresponding to the current item in <code>_walker</code>.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def __iter__(self):\n    r\"\"\"Method makes the class iterable. It iterates over the `_walker` attribute\n    and for each item, it gets the corresponding item from the dataset using the\n    `__getitem__` method.\n\n    Yields:\n    The item from the dataset corresponding to the current item in `_walker`.\n    \"\"\"\n    for item in range(self.__len__()):\n        yield self.__getitem__(item)\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples in the dataset.</p> <p>Returns     int: Number of samples in the dataset.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def __len__(self) -&gt; int:\n    r\"\"\"Returns the number of samples in the dataset.\n\n    Returns\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.collate_fn","title":"<code>collate_fn(data)</code>","text":"<p>Collates a batch of data samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List</code> <p>A list of data samples.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A list of reprocessed data batches.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def collate_fn(self, data: List) -&gt; List:\n    r\"\"\"Collates a batch of data samples.\n\n    Args:\n        data (List): A list of data samples.\n\n    Returns:\n        List: A list of reprocessed data batches.\n    \"\"\"\n    data_size = len(data)\n\n    idxs = list(range(data_size))\n\n    # Initialize empty lists to store extracted values\n    empty_lists: List[List] = [[] for _ in range(12)]\n    (\n        ids,\n        speakers,\n        texts,\n        raw_texts,\n        mels,\n        pitches,\n        attn_priors,\n        langs,\n        src_lens,\n        mel_lens,\n        wavs,\n        energy,\n    ) = empty_lists\n\n    # Extract fields from data dictionary and populate the lists\n    for idx in idxs:\n        data_entry = data[idx]\n        ids.append(data_entry[\"id\"])\n        speakers.append(data_entry[\"speaker\"])\n        texts.append(data_entry[\"text\"])\n        raw_texts.append(data_entry[\"raw_text\"])\n        mels.append(data_entry[\"mel\"])\n        pitches.append(data_entry[\"pitch\"])\n        attn_priors.append(data_entry[\"attn_prior\"])\n        langs.append(data_entry[\"lang\"])\n        src_lens.append(data_entry[\"text\"].shape[0])\n        mel_lens.append(data_entry[\"mel\"].shape[1])\n        wavs.append(data_entry[\"wav\"])\n        energy.append(data_entry[\"energy\"])\n\n    # Convert langs, src_lens, and mel_lens to numpy arrays\n    langs = np.array(langs)\n    src_lens = np.array(src_lens)\n    mel_lens = np.array(mel_lens)\n\n    # NOTE: Instead of the pitches for the whole dataset, used stat for the batch\n    # Take only min and max values for pitch\n    pitches_stat = list(self.normalize_pitch(pitches)[:2])\n\n    texts = pad_1D(texts)\n    mels = pad_2D(mels)\n    pitches = pad_1D(pitches)\n    attn_priors = pad_3D(attn_priors, len(idxs), max(src_lens), max(mel_lens))\n\n    speakers = np.repeat(\n        np.expand_dims(np.array(speakers), axis=1),\n        texts.shape[1],\n        axis=1,\n    )\n    langs = np.repeat(\n        np.expand_dims(np.array(langs), axis=1),\n        texts.shape[1],\n        axis=1,\n    )\n\n    wavs = pad_2D(wavs)\n    energy = pad_2D(energy)\n\n    return [\n        ids,\n        raw_texts,\n        torch.from_numpy(speakers),\n        texts.int(),\n        torch.from_numpy(src_lens),\n        mels,\n        pitches,\n        pitches_stat,\n        torch.from_numpy(mel_lens),\n        torch.from_numpy(langs),\n        attn_priors,\n        wavs,\n        energy,\n    ]\n</code></pre>"},{"location":"training/dataset/libritts_dataset_acoustic/#training.datasets.libritts_dataset_acoustic.LibriTTSDatasetAcoustic.normalize_pitch","title":"<code>normalize_pitch(pitches)</code>","text":"<p>Normalizes the pitch values.</p> <p>Parameters:</p> Name Type Description Default <code>pitches</code> <code>List[Tensor]</code> <p>A list of pitch values.</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[float, float, float, float]</code> <p>A tuple containing the normalized pitch values.</p> Source code in <code>training/datasets/libritts_dataset_acoustic.py</code> <pre><code>def normalize_pitch(\n    self,\n    pitches: List[torch.Tensor],\n) -&gt; Tuple[float, float, float, float]:\n    r\"\"\"Normalizes the pitch values.\n\n    Args:\n        pitches (List[torch.Tensor]): A list of pitch values.\n\n    Returns:\n        Tuple: A tuple containing the normalized pitch values.\n    \"\"\"\n    pitches_t = torch.concatenate(pitches)\n\n    min_value = torch.min(pitches_t).item()\n    max_value = torch.max(pitches_t).item()\n\n    mean = torch.mean(pitches_t).item()\n    std = torch.std(pitches_t).item()\n\n    return min_value, max_value, mean, std\n</code></pre>"},{"location":"training/dataset/libritts_dataset_vocoder/","title":"Libri TTS dataset vocoder","text":""},{"location":"training/dataset/libritts_dataset_vocoder/#training.datasets.libritts_dataset_vocoder.LibriTTSDatasetVocoder","title":"<code>LibriTTSDatasetVocoder</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Loading preprocessed univnet model data.</p> Source code in <code>training/datasets/libritts_dataset_vocoder.py</code> <pre><code>class LibriTTSDatasetVocoder(Dataset):\n    r\"\"\"Loading preprocessed univnet model data.\"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        batch_size: int,\n        download: bool = True,\n        lang: str = \"en\",\n    ):\n        r\"\"\"A PyTorch dataset for loading preprocessed univnet data.\n\n        Args:\n            root (str): Path to the directory where the dataset is found or downloaded.\n            batch_size (int): Batch size for the dataset.\n            download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n        \"\"\"\n        self.dataset = datasets.LIBRITTS(root=root, download=download)\n        self.batch_size = batch_size\n\n        lang_map = get_lang_map(lang)\n        self.preprocess_libtts = PreprocessLibriTTS(\n            PreprocessingConfigUnivNet(lang_map.processing_lang_type),\n        )\n\n    def __len__(self) -&gt; int:\n        r\"\"\"Returns the number of samples in the dataset.\n\n        Returns\n            int: Number of samples in the dataset.\n        \"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n        r\"\"\"Returns a sample from the dataset at the given index.\n\n        Args:\n            idx (int): Index of the sample to return.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the sample data.\n        \"\"\"\n        # Retrive the dataset row\n        data = self.dataset[idx]\n\n        data = self.preprocess_libtts.univnet(data)\n\n        if data is None:\n            # print(\"Skipping due to preprocessing error\")\n            rand_idx = np.random.randint(0, self.__len__())\n            return self.__getitem__(rand_idx)\n\n        mel, audio, speaker_id = data\n\n        return {\n            \"mel\": mel,\n            \"audio\": audio,\n            \"speaker_id\": speaker_id,\n        }\n\n    def collate_fn(self, data: List) -&gt; List:\n        r\"\"\"Collates a batch of data samples.\n\n        Args:\n            data (List): A list of data samples.\n\n        Returns:\n            List: A list of reprocessed data batches.\n        \"\"\"\n        data_size = len(data)\n\n        idxs = list(range(data_size))\n\n        # Initialize empty lists to store extracted values\n        empty_lists: List[List] = [[] for _ in range(4)]\n        (\n            mels,\n            mel_lens,\n            audios,\n            speaker_ids,\n        ) = empty_lists\n\n        # Extract fields from data dictionary and populate the lists\n        for idx in idxs:\n            data_entry = data[idx]\n\n            mels.append(data_entry[\"mel\"])\n            mel_lens.append(data_entry[\"mel\"].shape[1])\n            audios.append(data_entry[\"audio\"])\n            speaker_ids.append(data_entry[\"speaker_id\"])\n\n        mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n        mel_lens = torch.tensor(mel_lens, dtype=torch.int64)\n        audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n        speaker_ids = torch.tensor(speaker_ids, dtype=torch.int64)\n\n        return [\n            mels,\n            mel_lens,\n            audios,\n            speaker_ids,\n        ]\n</code></pre>"},{"location":"training/dataset/libritts_dataset_vocoder/#training.datasets.libritts_dataset_vocoder.LibriTTSDatasetVocoder.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample from the dataset at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to return.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the sample data.</p> Source code in <code>training/datasets/libritts_dataset_vocoder.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    r\"\"\"Returns a sample from the dataset at the given index.\n\n    Args:\n        idx (int): Index of the sample to return.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the sample data.\n    \"\"\"\n    # Retrive the dataset row\n    data = self.dataset[idx]\n\n    data = self.preprocess_libtts.univnet(data)\n\n    if data is None:\n        # print(\"Skipping due to preprocessing error\")\n        rand_idx = np.random.randint(0, self.__len__())\n        return self.__getitem__(rand_idx)\n\n    mel, audio, speaker_id = data\n\n    return {\n        \"mel\": mel,\n        \"audio\": audio,\n        \"speaker_id\": speaker_id,\n    }\n</code></pre>"},{"location":"training/dataset/libritts_dataset_vocoder/#training.datasets.libritts_dataset_vocoder.LibriTTSDatasetVocoder.__init__","title":"<code>__init__(root, batch_size, download=True, lang='en')</code>","text":"<p>A PyTorch dataset for loading preprocessed univnet data.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the directory where the dataset is found or downloaded.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the dataset.</p> required <code>download</code> <code>bool</code> <p>Whether to download the dataset if it is not found. Defaults to True.</p> <code>True</code> Source code in <code>training/datasets/libritts_dataset_vocoder.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    batch_size: int,\n    download: bool = True,\n    lang: str = \"en\",\n):\n    r\"\"\"A PyTorch dataset for loading preprocessed univnet data.\n\n    Args:\n        root (str): Path to the directory where the dataset is found or downloaded.\n        batch_size (int): Batch size for the dataset.\n        download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n    \"\"\"\n    self.dataset = datasets.LIBRITTS(root=root, download=download)\n    self.batch_size = batch_size\n\n    lang_map = get_lang_map(lang)\n    self.preprocess_libtts = PreprocessLibriTTS(\n        PreprocessingConfigUnivNet(lang_map.processing_lang_type),\n    )\n</code></pre>"},{"location":"training/dataset/libritts_dataset_vocoder/#training.datasets.libritts_dataset_vocoder.LibriTTSDatasetVocoder.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples in the dataset.</p> <p>Returns     int: Number of samples in the dataset.</p> Source code in <code>training/datasets/libritts_dataset_vocoder.py</code> <pre><code>def __len__(self) -&gt; int:\n    r\"\"\"Returns the number of samples in the dataset.\n\n    Returns\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"training/dataset/libritts_dataset_vocoder/#training.datasets.libritts_dataset_vocoder.LibriTTSDatasetVocoder.collate_fn","title":"<code>collate_fn(data)</code>","text":"<p>Collates a batch of data samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List</code> <p>A list of data samples.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A list of reprocessed data batches.</p> Source code in <code>training/datasets/libritts_dataset_vocoder.py</code> <pre><code>def collate_fn(self, data: List) -&gt; List:\n    r\"\"\"Collates a batch of data samples.\n\n    Args:\n        data (List): A list of data samples.\n\n    Returns:\n        List: A list of reprocessed data batches.\n    \"\"\"\n    data_size = len(data)\n\n    idxs = list(range(data_size))\n\n    # Initialize empty lists to store extracted values\n    empty_lists: List[List] = [[] for _ in range(4)]\n    (\n        mels,\n        mel_lens,\n        audios,\n        speaker_ids,\n    ) = empty_lists\n\n    # Extract fields from data dictionary and populate the lists\n    for idx in idxs:\n        data_entry = data[idx]\n\n        mels.append(data_entry[\"mel\"])\n        mel_lens.append(data_entry[\"mel\"].shape[1])\n        audios.append(data_entry[\"audio\"])\n        speaker_ids.append(data_entry[\"speaker_id\"])\n\n    mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n    mel_lens = torch.tensor(mel_lens, dtype=torch.int64)\n    audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n    speaker_ids = torch.tensor(speaker_ids, dtype=torch.int64)\n\n    return [\n        mels,\n        mel_lens,\n        audios,\n        speaker_ids,\n    ]\n</code></pre>"},{"location":"training/dataset/readme/","title":"References","text":""},{"location":"training/dataset/readme/#references","title":"References","text":"<p>Here is the code that was used to preprocess the dataset.</p>"},{"location":"training/dataset/readme/#libritts-dataset-for-acoustic","title":"LibriTTS dataset for acoustic","text":"<p>A PyTorch dataset for loading preprocessed data for acoustic module.</p>"},{"location":"training/dataset/readme/#libritts-dataset-for-vocoder","title":"LibriTTS dataset for vocoder","text":"<p>A PyTorch dataset for loading preprocessed data for vocoder module.</p>"},{"location":"training/loss/bin_loss/","title":"Binary Cross Entropy Loss","text":""},{"location":"training/loss/bin_loss/#training.loss.bin_loss.BinLoss","title":"<code>BinLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Binary cross-entropy loss for hard and soft attention.</p> <p>Attributes     None</p> <p>Methods     forward: Computes the binary cross-entropy loss for hard and soft attention.</p> Source code in <code>training/loss/bin_loss.py</code> <pre><code>class BinLoss(Module):\n    r\"\"\"Binary cross-entropy loss for hard and soft attention.\n\n    Attributes\n        None\n\n    Methods\n        forward: Computes the binary cross-entropy loss for hard and soft attention.\n\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(\n        self, hard_attention: torch.Tensor, soft_attention: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Computes the binary cross-entropy loss for hard and soft attention.\n\n        Args:\n            hard_attention (torch.Tensor): A binary tensor indicating the hard attention.\n            soft_attention (torch.Tensor): A tensor containing the soft attention probabilities.\n\n        Returns:\n            torch.Tensor: The binary cross-entropy loss.\n\n        \"\"\"\n        log_sum = torch.log(\n            torch.clamp(soft_attention[hard_attention == 1], min=1e-12),\n        ).sum()\n        return -log_sum / hard_attention.sum()\n</code></pre>"},{"location":"training/loss/bin_loss/#training.loss.bin_loss.BinLoss.forward","title":"<code>forward(hard_attention, soft_attention)</code>","text":"<p>Computes the binary cross-entropy loss for hard and soft attention.</p> <p>Parameters:</p> Name Type Description Default <code>hard_attention</code> <code>Tensor</code> <p>A binary tensor indicating the hard attention.</p> required <code>soft_attention</code> <code>Tensor</code> <p>A tensor containing the soft attention probabilities.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The binary cross-entropy loss.</p> Source code in <code>training/loss/bin_loss.py</code> <pre><code>def forward(\n    self, hard_attention: torch.Tensor, soft_attention: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"Computes the binary cross-entropy loss for hard and soft attention.\n\n    Args:\n        hard_attention (torch.Tensor): A binary tensor indicating the hard attention.\n        soft_attention (torch.Tensor): A tensor containing the soft attention probabilities.\n\n    Returns:\n        torch.Tensor: The binary cross-entropy loss.\n\n    \"\"\"\n    log_sum = torch.log(\n        torch.clamp(soft_attention[hard_attention == 1], min=1e-12),\n    ).sum()\n    return -log_sum / hard_attention.sum()\n</code></pre>"},{"location":"training/loss/fast_speech_2_loss_gen/","title":"FastSpeech 2 Loss","text":""},{"location":"training/loss/fast_speech_2_loss_gen/#training.loss.fast_speech_2_loss_gen.FastSpeech2LossGen","title":"<code>FastSpeech2LossGen</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>training/loss/fast_speech_2_loss_gen.py</code> <pre><code>class FastSpeech2LossGen(Module):\n    def __init__(\n        self,\n        bin_warmup: bool = True,\n        binarization_loss_enable_steps: int = 1260,\n        binarization_loss_warmup_steps: int = 700,\n    ):\n        r\"\"\"Initializes the FastSpeech2LossGen module.\n\n        Args:\n            bin_warmup (bool, optional): Whether to use binarization warmup. Defaults to True. NOTE: Switch this off if you preload the model with a checkpoint that has already passed the warmup phase.\n            binarization_loss_enable_steps (int, optional): Number of steps to enable the binarization loss. Defaults to 1260.\n            binarization_loss_warmup_steps (int, optional): Number of warmup steps for the binarization loss. Defaults to 700.\n        \"\"\"\n        super().__init__()\n\n        self.mse_loss = nn.MSELoss()\n        self.mae_loss = nn.L1Loss()\n        self.ssim_loss = SSIMLoss()\n        self.sum_loss = ForwardSumLoss()\n        self.bin_loss = BinLoss()\n\n        self.bin_warmup = bin_warmup\n        self.binarization_loss_enable_steps = binarization_loss_enable_steps\n        self.binarization_loss_warmup_steps = binarization_loss_warmup_steps\n\n    def forward(\n        self,\n        src_masks: torch.Tensor,\n        mel_masks: torch.Tensor,\n        mel_targets: torch.Tensor,\n        mel_predictions: torch.Tensor,\n        log_duration_predictions: torch.Tensor,\n        u_prosody_ref: torch.Tensor,\n        u_prosody_pred: torch.Tensor,\n        p_prosody_ref: torch.Tensor,\n        p_prosody_pred: torch.Tensor,\n        durations: torch.Tensor,\n        pitch_predictions: torch.Tensor,\n        p_targets: torch.Tensor,\n        attn_logprob: torch.Tensor,\n        attn_soft: torch.Tensor,\n        attn_hard: torch.Tensor,\n        step: int,\n        src_lens: torch.Tensor,\n        mel_lens: torch.Tensor,\n        energy_pred: torch.Tensor,\n        energy_target: torch.Tensor,\n    ) -&gt; Tuple[\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n        torch.Tensor,\n    ]:\n        r\"\"\"Computes the loss for the FastSpeech2 model.\n\n        Args:\n            src_masks (torch.Tensor): Mask for the source sequence.\n            mel_masks (torch.Tensor): Mask for the mel-spectrogram.\n            mel_targets (torch.Tensor): Target mel-spectrogram.\n            mel_predictions (torch.Tensor): Predicted mel-spectrogram.\n            log_duration_predictions (torch.Tensor): Predicted log-duration.\n            u_prosody_ref (torch.Tensor): Reference unvoiced prosody.\n            u_prosody_pred (torch.Tensor): Predicted unvoiced prosody.\n            p_prosody_ref (torch.Tensor): Reference voiced prosody.\n            p_prosody_pred (torch.Tensor): Predicted voiced prosody.\n            durations (torch.Tensor): Ground-truth durations.\n            pitch_predictions (torch.Tensor): Predicted pitch.\n            p_targets (torch.Tensor): Ground-truth pitch.\n            attn_logprob (torch.Tensor): Log-probability of attention.\n            attn_soft (torch.Tensor): Soft attention.\n            attn_hard (torch.Tensor): Hard attention.\n            step (int): Current training step.\n            src_lens (torch.Tensor): Lengths of the source sequences.\n            mel_lens (torch.Tensor): Lengths of the mel-spectrograms.\n            energy_pred (torch.Tensor): Predicted energy.\n            energy_target (torch.Tensor): Ground-truth energy.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The total loss and its components.\n\n        Note:\n            Here is the description of the returned loss components:\n            `total_loss`: This is the total loss computed as the sum of all the other losses.\n            `mel_loss`: This is the mean absolute error (MAE) loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms.\n            `sc_mag_loss`: This is the spectral convergence loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n            `log_mag_loss`: This is the log STFT magnitude loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n            `ssim_loss`: This is the Structural Similarity Index (SSIM) loss between the predicted and target mel-spectrograms. It measures the similarity between the two mel-spectrograms in terms of their structure, contrast, and luminance.\n            `duration_loss`: This is the mean squared error (MSE) loss between the predicted and target log-durations. It measures how well the model predicts the durations of the phonemes.\n            `u_prosody_loss`: This is the MAE loss between the predicted and reference unvoiced prosody. It measures how well the model predicts the prosody (rhythm, stress, and intonation) of the unvoiced parts of the speech.\n            `p_prosody_loss`: This is the MAE loss between the predicted and reference voiced prosody. It measures how well the model predicts the prosody of the voiced parts of the speech.\n            `pitch_loss`: This is the MSE loss between the predicted and target pitch. It measures how well the model predicts the pitch of the speech.\n            `ctc_loss`: This is the Connectionist Temporal Classification (CTC) loss computed from the log-probability of attention and the lengths of the source sequences and mel-spectrograms. It measures how well the model aligns the input and output sequences.\n            `bin_loss`: This is the binarization loss computed from the hard and soft attention. It measures how well the model learns to attend to the correct parts of the input sequence.\n            `energy_loss`: This is the MSE loss between the predicted and target energy. It measures how well the model predicts the energy of the speech.\n        \"\"\"\n        log_duration_targets = torch.log(durations.float() + 1).to(src_masks.device)\n\n        log_duration_targets.requires_grad = False\n        mel_targets.requires_grad = False\n        p_targets.requires_grad = False\n        energy_target.requires_grad = False\n\n        log_duration_predictions = log_duration_predictions.masked_select(~src_masks)\n        log_duration_targets = log_duration_targets.masked_select(~src_masks)\n\n        mel_masks_expanded = mel_masks.unsqueeze(1)\n\n        mel_predictions_normalized = (\n            sample_wise_min_max(mel_predictions).float().to(mel_predictions.device)\n        )\n        mel_targets_normalized = (\n            sample_wise_min_max(mel_targets).float().to(mel_predictions.device)\n        )\n\n        ssim_loss: torch.Tensor = self.ssim_loss(\n            mel_predictions_normalized.unsqueeze(1),\n            mel_targets_normalized.unsqueeze(1),\n        )\n\n        if ssim_loss.item() &gt; 1.0 or ssim_loss.item() &lt; 0.0:\n            ssim_loss = torch.tensor([1.0], device=mel_predictions.device)\n\n        masked_mel_predictions = mel_predictions.masked_select(~mel_masks_expanded)\n\n        masked_mel_targets = mel_targets.masked_select(~mel_masks_expanded)\n\n        mel_loss: torch.Tensor = self.mae_loss(\n            masked_mel_predictions,\n            masked_mel_targets,\n        )\n\n        p_prosody_ref = p_prosody_ref.permute((0, 2, 1))\n        p_prosody_pred = p_prosody_pred.permute((0, 2, 1))\n\n        p_prosody_ref = p_prosody_ref.masked_fill(src_masks.unsqueeze(1), 0.0)\n        p_prosody_pred = p_prosody_pred.masked_fill(src_masks.unsqueeze(1), 0.0)\n\n        p_prosody_ref = p_prosody_ref.detach()\n\n        p_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n            p_prosody_ref.masked_select(~src_masks.unsqueeze(1)),\n            p_prosody_pred.masked_select(~src_masks.unsqueeze(1)),\n        )\n\n        u_prosody_ref = u_prosody_ref.detach()\n        u_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n            u_prosody_ref,\n            u_prosody_pred,\n        )\n\n        duration_loss: torch.Tensor = self.mse_loss(\n            log_duration_predictions,\n            log_duration_targets,\n        )\n\n        pitch_predictions = pitch_predictions.masked_select(~src_masks)\n        p_targets = p_targets.masked_select(~src_masks)\n\n        pitch_loss: torch.Tensor = self.mse_loss(pitch_predictions, p_targets)\n\n        ctc_loss: torch.Tensor = self.sum_loss(\n            attn_logprob=attn_logprob,\n            in_lens=src_lens,\n            out_lens=mel_lens,\n        )\n\n        if self.bin_warmup:\n            if step &lt; self.binarization_loss_enable_steps:\n                bin_loss_weight = 0.0\n            else:\n                bin_loss_weight = (\n                    min(\n                        (step - self.binarization_loss_enable_steps)\n                        / self.binarization_loss_warmup_steps,\n                        1.0,\n                    )\n                    * 1.0\n                )\n\n            bin_loss: torch.Tensor = (\n                self.bin_loss(hard_attention=attn_hard, soft_attention=attn_soft)\n                * bin_loss_weight\n            )\n        else:\n            bin_loss: torch.Tensor = self.bin_loss(\n                hard_attention=attn_hard,\n                soft_attention=attn_soft,\n            )\n\n        energy_loss: torch.Tensor = self.mse_loss(energy_pred, energy_target)\n\n        total_loss = (\n            mel_loss\n            + duration_loss\n            + u_prosody_loss\n            + p_prosody_loss\n            + ssim_loss\n            + pitch_loss\n            + ctc_loss\n            + bin_loss\n            + energy_loss\n        )\n\n        return (\n            total_loss,\n            mel_loss,\n            ssim_loss,\n            duration_loss,\n            u_prosody_loss,\n            p_prosody_loss,\n            pitch_loss,\n            ctc_loss,\n            bin_loss,\n            energy_loss,\n        )\n</code></pre>"},{"location":"training/loss/fast_speech_2_loss_gen/#training.loss.fast_speech_2_loss_gen.FastSpeech2LossGen.__init__","title":"<code>__init__(bin_warmup=True, binarization_loss_enable_steps=1260, binarization_loss_warmup_steps=700)</code>","text":"<p>Initializes the FastSpeech2LossGen module.</p> <p>Parameters:</p> Name Type Description Default <code>bin_warmup</code> <code>bool</code> <p>Whether to use binarization warmup. Defaults to True. NOTE: Switch this off if you preload the model with a checkpoint that has already passed the warmup phase.</p> <code>True</code> <code>binarization_loss_enable_steps</code> <code>int</code> <p>Number of steps to enable the binarization loss. Defaults to 1260.</p> <code>1260</code> <code>binarization_loss_warmup_steps</code> <code>int</code> <p>Number of warmup steps for the binarization loss. Defaults to 700.</p> <code>700</code> Source code in <code>training/loss/fast_speech_2_loss_gen.py</code> <pre><code>def __init__(\n    self,\n    bin_warmup: bool = True,\n    binarization_loss_enable_steps: int = 1260,\n    binarization_loss_warmup_steps: int = 700,\n):\n    r\"\"\"Initializes the FastSpeech2LossGen module.\n\n    Args:\n        bin_warmup (bool, optional): Whether to use binarization warmup. Defaults to True. NOTE: Switch this off if you preload the model with a checkpoint that has already passed the warmup phase.\n        binarization_loss_enable_steps (int, optional): Number of steps to enable the binarization loss. Defaults to 1260.\n        binarization_loss_warmup_steps (int, optional): Number of warmup steps for the binarization loss. Defaults to 700.\n    \"\"\"\n    super().__init__()\n\n    self.mse_loss = nn.MSELoss()\n    self.mae_loss = nn.L1Loss()\n    self.ssim_loss = SSIMLoss()\n    self.sum_loss = ForwardSumLoss()\n    self.bin_loss = BinLoss()\n\n    self.bin_warmup = bin_warmup\n    self.binarization_loss_enable_steps = binarization_loss_enable_steps\n    self.binarization_loss_warmup_steps = binarization_loss_warmup_steps\n</code></pre>"},{"location":"training/loss/fast_speech_2_loss_gen/#training.loss.fast_speech_2_loss_gen.FastSpeech2LossGen.forward","title":"<code>forward(src_masks, mel_masks, mel_targets, mel_predictions, log_duration_predictions, u_prosody_ref, u_prosody_pred, p_prosody_ref, p_prosody_pred, durations, pitch_predictions, p_targets, attn_logprob, attn_soft, attn_hard, step, src_lens, mel_lens, energy_pred, energy_target)</code>","text":"<p>Computes the loss for the FastSpeech2 model.</p> <p>Parameters:</p> Name Type Description Default <code>src_masks</code> <code>Tensor</code> <p>Mask for the source sequence.</p> required <code>mel_masks</code> <code>Tensor</code> <p>Mask for the mel-spectrogram.</p> required <code>mel_targets</code> <code>Tensor</code> <p>Target mel-spectrogram.</p> required <code>mel_predictions</code> <code>Tensor</code> <p>Predicted mel-spectrogram.</p> required <code>log_duration_predictions</code> <code>Tensor</code> <p>Predicted log-duration.</p> required <code>u_prosody_ref</code> <code>Tensor</code> <p>Reference unvoiced prosody.</p> required <code>u_prosody_pred</code> <code>Tensor</code> <p>Predicted unvoiced prosody.</p> required <code>p_prosody_ref</code> <code>Tensor</code> <p>Reference voiced prosody.</p> required <code>p_prosody_pred</code> <code>Tensor</code> <p>Predicted voiced prosody.</p> required <code>durations</code> <code>Tensor</code> <p>Ground-truth durations.</p> required <code>pitch_predictions</code> <code>Tensor</code> <p>Predicted pitch.</p> required <code>p_targets</code> <code>Tensor</code> <p>Ground-truth pitch.</p> required <code>attn_logprob</code> <code>Tensor</code> <p>Log-probability of attention.</p> required <code>attn_soft</code> <code>Tensor</code> <p>Soft attention.</p> required <code>attn_hard</code> <code>Tensor</code> <p>Hard attention.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>src_lens</code> <code>Tensor</code> <p>Lengths of the source sequences.</p> required <code>mel_lens</code> <code>Tensor</code> <p>Lengths of the mel-spectrograms.</p> required <code>energy_pred</code> <code>Tensor</code> <p>Predicted energy.</p> required <code>energy_target</code> <code>Tensor</code> <p>Ground-truth energy.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The total loss and its components.</p> Note <p>Here is the description of the returned loss components: <code>total_loss</code>: This is the total loss computed as the sum of all the other losses. <code>mel_loss</code>: This is the mean absolute error (MAE) loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms. <code>sc_mag_loss</code>: This is the spectral convergence loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure. <code>log_mag_loss</code>: This is the log STFT magnitude loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure. <code>ssim_loss</code>: This is the Structural Similarity Index (SSIM) loss between the predicted and target mel-spectrograms. It measures the similarity between the two mel-spectrograms in terms of their structure, contrast, and luminance. <code>duration_loss</code>: This is the mean squared error (MSE) loss between the predicted and target log-durations. It measures how well the model predicts the durations of the phonemes. <code>u_prosody_loss</code>: This is the MAE loss between the predicted and reference unvoiced prosody. It measures how well the model predicts the prosody (rhythm, stress, and intonation) of the unvoiced parts of the speech. <code>p_prosody_loss</code>: This is the MAE loss between the predicted and reference voiced prosody. It measures how well the model predicts the prosody of the voiced parts of the speech. <code>pitch_loss</code>: This is the MSE loss between the predicted and target pitch. It measures how well the model predicts the pitch of the speech. <code>ctc_loss</code>: This is the Connectionist Temporal Classification (CTC) loss computed from the log-probability of attention and the lengths of the source sequences and mel-spectrograms. It measures how well the model aligns the input and output sequences. <code>bin_loss</code>: This is the binarization loss computed from the hard and soft attention. It measures how well the model learns to attend to the correct parts of the input sequence. <code>energy_loss</code>: This is the MSE loss between the predicted and target energy. It measures how well the model predicts the energy of the speech.</p> Source code in <code>training/loss/fast_speech_2_loss_gen.py</code> <pre><code>def forward(\n    self,\n    src_masks: torch.Tensor,\n    mel_masks: torch.Tensor,\n    mel_targets: torch.Tensor,\n    mel_predictions: torch.Tensor,\n    log_duration_predictions: torch.Tensor,\n    u_prosody_ref: torch.Tensor,\n    u_prosody_pred: torch.Tensor,\n    p_prosody_ref: torch.Tensor,\n    p_prosody_pred: torch.Tensor,\n    durations: torch.Tensor,\n    pitch_predictions: torch.Tensor,\n    p_targets: torch.Tensor,\n    attn_logprob: torch.Tensor,\n    attn_soft: torch.Tensor,\n    attn_hard: torch.Tensor,\n    step: int,\n    src_lens: torch.Tensor,\n    mel_lens: torch.Tensor,\n    energy_pred: torch.Tensor,\n    energy_target: torch.Tensor,\n) -&gt; Tuple[\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n    torch.Tensor,\n]:\n    r\"\"\"Computes the loss for the FastSpeech2 model.\n\n    Args:\n        src_masks (torch.Tensor): Mask for the source sequence.\n        mel_masks (torch.Tensor): Mask for the mel-spectrogram.\n        mel_targets (torch.Tensor): Target mel-spectrogram.\n        mel_predictions (torch.Tensor): Predicted mel-spectrogram.\n        log_duration_predictions (torch.Tensor): Predicted log-duration.\n        u_prosody_ref (torch.Tensor): Reference unvoiced prosody.\n        u_prosody_pred (torch.Tensor): Predicted unvoiced prosody.\n        p_prosody_ref (torch.Tensor): Reference voiced prosody.\n        p_prosody_pred (torch.Tensor): Predicted voiced prosody.\n        durations (torch.Tensor): Ground-truth durations.\n        pitch_predictions (torch.Tensor): Predicted pitch.\n        p_targets (torch.Tensor): Ground-truth pitch.\n        attn_logprob (torch.Tensor): Log-probability of attention.\n        attn_soft (torch.Tensor): Soft attention.\n        attn_hard (torch.Tensor): Hard attention.\n        step (int): Current training step.\n        src_lens (torch.Tensor): Lengths of the source sequences.\n        mel_lens (torch.Tensor): Lengths of the mel-spectrograms.\n        energy_pred (torch.Tensor): Predicted energy.\n        energy_target (torch.Tensor): Ground-truth energy.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The total loss and its components.\n\n    Note:\n        Here is the description of the returned loss components:\n        `total_loss`: This is the total loss computed as the sum of all the other losses.\n        `mel_loss`: This is the mean absolute error (MAE) loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms.\n        `sc_mag_loss`: This is the spectral convergence loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n        `log_mag_loss`: This is the log STFT magnitude loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n        `ssim_loss`: This is the Structural Similarity Index (SSIM) loss between the predicted and target mel-spectrograms. It measures the similarity between the two mel-spectrograms in terms of their structure, contrast, and luminance.\n        `duration_loss`: This is the mean squared error (MSE) loss between the predicted and target log-durations. It measures how well the model predicts the durations of the phonemes.\n        `u_prosody_loss`: This is the MAE loss between the predicted and reference unvoiced prosody. It measures how well the model predicts the prosody (rhythm, stress, and intonation) of the unvoiced parts of the speech.\n        `p_prosody_loss`: This is the MAE loss between the predicted and reference voiced prosody. It measures how well the model predicts the prosody of the voiced parts of the speech.\n        `pitch_loss`: This is the MSE loss between the predicted and target pitch. It measures how well the model predicts the pitch of the speech.\n        `ctc_loss`: This is the Connectionist Temporal Classification (CTC) loss computed from the log-probability of attention and the lengths of the source sequences and mel-spectrograms. It measures how well the model aligns the input and output sequences.\n        `bin_loss`: This is the binarization loss computed from the hard and soft attention. It measures how well the model learns to attend to the correct parts of the input sequence.\n        `energy_loss`: This is the MSE loss between the predicted and target energy. It measures how well the model predicts the energy of the speech.\n    \"\"\"\n    log_duration_targets = torch.log(durations.float() + 1).to(src_masks.device)\n\n    log_duration_targets.requires_grad = False\n    mel_targets.requires_grad = False\n    p_targets.requires_grad = False\n    energy_target.requires_grad = False\n\n    log_duration_predictions = log_duration_predictions.masked_select(~src_masks)\n    log_duration_targets = log_duration_targets.masked_select(~src_masks)\n\n    mel_masks_expanded = mel_masks.unsqueeze(1)\n\n    mel_predictions_normalized = (\n        sample_wise_min_max(mel_predictions).float().to(mel_predictions.device)\n    )\n    mel_targets_normalized = (\n        sample_wise_min_max(mel_targets).float().to(mel_predictions.device)\n    )\n\n    ssim_loss: torch.Tensor = self.ssim_loss(\n        mel_predictions_normalized.unsqueeze(1),\n        mel_targets_normalized.unsqueeze(1),\n    )\n\n    if ssim_loss.item() &gt; 1.0 or ssim_loss.item() &lt; 0.0:\n        ssim_loss = torch.tensor([1.0], device=mel_predictions.device)\n\n    masked_mel_predictions = mel_predictions.masked_select(~mel_masks_expanded)\n\n    masked_mel_targets = mel_targets.masked_select(~mel_masks_expanded)\n\n    mel_loss: torch.Tensor = self.mae_loss(\n        masked_mel_predictions,\n        masked_mel_targets,\n    )\n\n    p_prosody_ref = p_prosody_ref.permute((0, 2, 1))\n    p_prosody_pred = p_prosody_pred.permute((0, 2, 1))\n\n    p_prosody_ref = p_prosody_ref.masked_fill(src_masks.unsqueeze(1), 0.0)\n    p_prosody_pred = p_prosody_pred.masked_fill(src_masks.unsqueeze(1), 0.0)\n\n    p_prosody_ref = p_prosody_ref.detach()\n\n    p_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n        p_prosody_ref.masked_select(~src_masks.unsqueeze(1)),\n        p_prosody_pred.masked_select(~src_masks.unsqueeze(1)),\n    )\n\n    u_prosody_ref = u_prosody_ref.detach()\n    u_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n        u_prosody_ref,\n        u_prosody_pred,\n    )\n\n    duration_loss: torch.Tensor = self.mse_loss(\n        log_duration_predictions,\n        log_duration_targets,\n    )\n\n    pitch_predictions = pitch_predictions.masked_select(~src_masks)\n    p_targets = p_targets.masked_select(~src_masks)\n\n    pitch_loss: torch.Tensor = self.mse_loss(pitch_predictions, p_targets)\n\n    ctc_loss: torch.Tensor = self.sum_loss(\n        attn_logprob=attn_logprob,\n        in_lens=src_lens,\n        out_lens=mel_lens,\n    )\n\n    if self.bin_warmup:\n        if step &lt; self.binarization_loss_enable_steps:\n            bin_loss_weight = 0.0\n        else:\n            bin_loss_weight = (\n                min(\n                    (step - self.binarization_loss_enable_steps)\n                    / self.binarization_loss_warmup_steps,\n                    1.0,\n                )\n                * 1.0\n            )\n\n        bin_loss: torch.Tensor = (\n            self.bin_loss(hard_attention=attn_hard, soft_attention=attn_soft)\n            * bin_loss_weight\n        )\n    else:\n        bin_loss: torch.Tensor = self.bin_loss(\n            hard_attention=attn_hard,\n            soft_attention=attn_soft,\n        )\n\n    energy_loss: torch.Tensor = self.mse_loss(energy_pred, energy_target)\n\n    total_loss = (\n        mel_loss\n        + duration_loss\n        + u_prosody_loss\n        + p_prosody_loss\n        + ssim_loss\n        + pitch_loss\n        + ctc_loss\n        + bin_loss\n        + energy_loss\n    )\n\n    return (\n        total_loss,\n        mel_loss,\n        ssim_loss,\n        duration_loss,\n        u_prosody_loss,\n        p_prosody_loss,\n        pitch_loss,\n        ctc_loss,\n        bin_loss,\n        energy_loss,\n    )\n</code></pre>"},{"location":"training/loss/forward_sum_loss/","title":"Forward Sum Loss","text":"<p>             Bases: <code>Module</code></p> <p>Computes the forward sum loss for sequence-to-sequence models with attention.</p> <p>Parameters:</p> Name Type Description Default <code>blank_logprob</code> <code>float</code> <p>The log probability of the blank symbol. Default: -1.</p> <code>-1</code> <p>Attributes:</p> Name Type Description <code>log_softmax</code> <code>LogSoftmax</code> <p>The log softmax function.</p> <code>ctc_loss</code> <code>CTCLoss</code> <p>The CTC loss function.</p> <code>blank_logprob</code> <code>float</code> <p>The log probability of the blank symbol.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the forward sum loss for sequence-to-sequence models with attention.</p> Source code in <code>training/loss/forward_sum_loss.py</code> <pre><code>class ForwardSumLoss(Module):\n    r\"\"\"Computes the forward sum loss for sequence-to-sequence models with attention.\n\n    Args:\n        blank_logprob (float): The log probability of the blank symbol. Default: -1.\n\n    Attributes:\n        log_softmax (nn.LogSoftmax): The log softmax function.\n        ctc_loss (nn.CTCLoss): The CTC loss function.\n        blank_logprob (float): The log probability of the blank symbol.\n\n    Methods:\n        forward: Computes the forward sum loss for sequence-to-sequence models with attention.\n\n    \"\"\"\n\n    def __init__(self, blank_logprob: float = -1):\n        super().__init__()\n        self.log_softmax = nn.LogSoftmax(dim=3)\n        self.ctc_loss = nn.CTCLoss(zero_infinity=True)\n        self.blank_logprob = blank_logprob\n\n    def forward(\n        self, attn_logprob: torch.Tensor, in_lens: torch.Tensor, out_lens: torch.Tensor,\n    ) -&gt; float:\n        r\"\"\"Computes the forward sum loss for sequence-to-sequence models with attention.\n\n        Args:\n            attn_logprob (torch.Tensor): The attention log probabilities of shape (batch_size, max_out_len, max_in_len).\n            in_lens (torch.Tensor): The input lengths of shape (batch_size,).\n            out_lens (torch.Tensor): The output lengths of shape (batch_size,).\n\n        Returns:\n            float: The forward sum loss.\n\n        \"\"\"\n        key_lens = in_lens\n        query_lens = out_lens\n        attn_logprob_padded = F.pad(\n            input=attn_logprob, pad=(1, 0), value=self.blank_logprob,\n        )\n\n        total_loss = 0.0\n        for bid in range(attn_logprob.shape[0]):\n            target_seq = torch.arange(1, int(key_lens[bid]) + 1).unsqueeze(0)\n            curr_logprob = attn_logprob_padded[bid].permute(1, 0, 2)[\n                : int(query_lens[bid]), :, : int(key_lens[bid]) + 1,\n            ]\n\n            curr_logprob = self.log_softmax(curr_logprob[None])[0]\n            loss = self.ctc_loss(\n                curr_logprob,\n                target_seq,\n                input_lengths=query_lens[bid : bid + 1],\n                target_lengths=key_lens[bid : bid + 1],\n            )\n            total_loss += loss\n\n        total_loss /= attn_logprob.shape[0]\n        return total_loss\n</code></pre>"},{"location":"training/loss/forward_sum_loss/#training.loss.ForwardSumLoss.forward","title":"<code>forward(attn_logprob, in_lens, out_lens)</code>","text":"<p>Computes the forward sum loss for sequence-to-sequence models with attention.</p> <p>Parameters:</p> Name Type Description Default <code>attn_logprob</code> <code>Tensor</code> <p>The attention log probabilities of shape (batch_size, max_out_len, max_in_len).</p> required <code>in_lens</code> <code>Tensor</code> <p>The input lengths of shape (batch_size,).</p> required <code>out_lens</code> <code>Tensor</code> <p>The output lengths of shape (batch_size,).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The forward sum loss.</p> Source code in <code>training/loss/forward_sum_loss.py</code> <pre><code>def forward(\n    self, attn_logprob: torch.Tensor, in_lens: torch.Tensor, out_lens: torch.Tensor,\n) -&gt; float:\n    r\"\"\"Computes the forward sum loss for sequence-to-sequence models with attention.\n\n    Args:\n        attn_logprob (torch.Tensor): The attention log probabilities of shape (batch_size, max_out_len, max_in_len).\n        in_lens (torch.Tensor): The input lengths of shape (batch_size,).\n        out_lens (torch.Tensor): The output lengths of shape (batch_size,).\n\n    Returns:\n        float: The forward sum loss.\n\n    \"\"\"\n    key_lens = in_lens\n    query_lens = out_lens\n    attn_logprob_padded = F.pad(\n        input=attn_logprob, pad=(1, 0), value=self.blank_logprob,\n    )\n\n    total_loss = 0.0\n    for bid in range(attn_logprob.shape[0]):\n        target_seq = torch.arange(1, int(key_lens[bid]) + 1).unsqueeze(0)\n        curr_logprob = attn_logprob_padded[bid].permute(1, 0, 2)[\n            : int(query_lens[bid]), :, : int(key_lens[bid]) + 1,\n        ]\n\n        curr_logprob = self.log_softmax(curr_logprob[None])[0]\n        loss = self.ctc_loss(\n            curr_logprob,\n            target_seq,\n            input_lengths=query_lens[bid : bid + 1],\n            target_lengths=key_lens[bid : bid + 1],\n        )\n        total_loss += loss\n\n    total_loss /= attn_logprob.shape[0]\n    return total_loss\n</code></pre>"},{"location":"training/loss/log_stft_magnitude_loss/","title":"Log STFT Magnitude Loss","text":""},{"location":"training/loss/log_stft_magnitude_loss/#training.loss.log_stft_magnitude_loss.LogSTFTMagnitudeLoss","title":"<code>LogSTFTMagnitudeLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Log STFT magnitude loss module. Log STFT magnitude loss is a loss function that is commonly used in speech and audio signal processing tasks, such as speech enhancement and source separation. It is a modification of the spectral convergence loss, which measures the similarity between two magnitude spectrograms.</p> <p>The log STFT magnitude loss is calculated as the mean squared error between the logarithm of the predicted and groundtruth magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale. The mean squared error is used to penalize large errors between the predicted and groundtruth spectrograms.</p> Source code in <code>training/loss/log_stft_magnitude_loss.py</code> <pre><code>class LogSTFTMagnitudeLoss(Module):\n    r\"\"\"Log STFT magnitude loss module.\n    Log STFT magnitude loss is a loss function that is commonly used in speech and audio signal processing tasks, such as speech enhancement and source separation. It is a modification of the spectral convergence loss, which measures the similarity between two magnitude spectrograms.\n\n    The log STFT magnitude loss is calculated as the mean squared error between the logarithm of the predicted and groundtruth magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale. The mean squared error is used to penalize large errors between the predicted and groundtruth spectrograms.\n    \"\"\"\n\n    def __init__(self):\n        r\"\"\"Initilize los STFT magnitude loss module.\"\"\"\n        super().__init__()\n\n    def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Log STFT magnitude loss value.\n        \"\"\"\n        # Ensure that x_mag and y_mag have the same size along dimension 1\n        min_len = min(x_mag.shape[1], y_mag.shape[1])\n        x_mag = x_mag[:, :min_len]\n        y_mag = y_mag[:, :min_len]\n\n        return F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n</code></pre>"},{"location":"training/loss/log_stft_magnitude_loss/#training.loss.log_stft_magnitude_loss.LogSTFTMagnitudeLoss.__init__","title":"<code>__init__()</code>","text":"<p>Initilize los STFT magnitude loss module.</p> Source code in <code>training/loss/log_stft_magnitude_loss.py</code> <pre><code>def __init__(self):\n    r\"\"\"Initilize los STFT magnitude loss module.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"training/loss/log_stft_magnitude_loss/#training.loss.log_stft_magnitude_loss.LogSTFTMagnitudeLoss.forward","title":"<code>forward(x_mag, y_mag)</code>","text":"<p>Calculate forward propagation.</p> <p>Parameters:</p> Name Type Description Default <code>x_mag</code> <code>Tensor</code> <p>Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).</p> required <code>y_mag</code> <code>Tensor</code> <p>Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Log STFT magnitude loss value.</p> Source code in <code>training/loss/log_stft_magnitude_loss.py</code> <pre><code>def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Calculate forward propagation.\n\n    Args:\n        x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n        y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n    Returns:\n        Tensor: Log STFT magnitude loss value.\n    \"\"\"\n    # Ensure that x_mag and y_mag have the same size along dimension 1\n    min_len = min(x_mag.shape[1], y_mag.shape[1])\n    x_mag = x_mag[:, :min_len]\n    y_mag = y_mag[:, :min_len]\n\n    return F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n</code></pre>"},{"location":"training/loss/multi_resolution_stft_loss/","title":"Multi Resolution STFT Loss","text":""},{"location":"training/loss/multi_resolution_stft_loss/#training.loss.multi_resolution_stft_loss.MultiResolutionSTFTLoss","title":"<code>MultiResolutionSTFTLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Multi resolution STFT loss module.</p> <p>The Multi resolution STFT loss module is a PyTorch module that computes the spectral convergence and log STFT magnitude losses for a predicted signal and a groundtruth signal at multiple resolutions. The module is designed for speech and audio signal processing tasks, such as speech enhancement and source separation.</p> <p>The module takes as input a list of tuples, where each tuple contains the FFT size, hop size, and window length for a particular resolution. For each resolution, the module computes the spectral convergence and log STFT magnitude losses using the STFTLoss module, which is a PyTorch module that computes the STFT of a signal and the corresponding magnitude spectrogram.</p> <p>The spectral convergence loss measures the similarity between two magnitude spectrograms, while the log STFT magnitude loss measures the similarity between two logarithmically-scaled magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale.</p> <p>The Multi resolution STFT loss module returns the average spectral convergence and log STFT magnitude losses across all resolutions. This allows the module to capture both fine-grained and coarse-grained spectral information in the predicted and groundtruth signals.</p> Source code in <code>training/loss/multi_resolution_stft_loss.py</code> <pre><code>class MultiResolutionSTFTLoss(Module):\n    r\"\"\"Multi resolution STFT loss module.\n\n    The Multi resolution STFT loss module is a PyTorch module that computes the spectral convergence and log STFT magnitude losses for a predicted signal and a groundtruth signal at multiple resolutions. The module is designed for speech and audio signal processing tasks, such as speech enhancement and source separation.\n\n    The module takes as input a list of tuples, where each tuple contains the FFT size, hop size, and window length for a particular resolution. For each resolution, the module computes the spectral convergence and log STFT magnitude losses using the STFTLoss module, which is a PyTorch module that computes the STFT of a signal and the corresponding magnitude spectrogram.\n\n    The spectral convergence loss measures the similarity between two magnitude spectrograms, while the log STFT magnitude loss measures the similarity between two logarithmically-scaled magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale.\n\n    The Multi resolution STFT loss module returns the average spectral convergence and log STFT magnitude losses across all resolutions. This allows the module to capture both fine-grained and coarse-grained spectral information in the predicted and groundtruth signals.\n    \"\"\"\n\n    def __init__(\n        self,\n        resolutions: list[tuple[int, int, int]],\n    ):\n        r\"\"\"Initialize Multi resolution STFT loss module.\n\n        Args:\n            resolutions (list): List of (FFT size, shift size, window length).\n        \"\"\"\n        super().__init__()\n\n        self.stft_losses = torch.nn.ModuleList(\n            [STFTLoss(fs, ss, wl) for fs, ss, wl in resolutions],\n        )\n\n    def forward(\n        self, x: torch.Tensor, y: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n\n        Returns:\n            Tensor: Multi resolution spectral convergence loss value.\n            Tensor: Multi resolution log STFT magnitude loss value.\n        \"\"\"\n        sc_loss = torch.tensor(0.0, device=x.device)\n        mag_loss = torch.tensor(0.0, device=x.device)\n\n        # Compute the spectral convergence and log STFT magnitude losses for each resolution\n        for f in self.stft_losses:\n            sc_l, mag_l = f(x, y)\n            sc_loss += sc_l\n            mag_loss += mag_l\n\n        # Average the losses across all resolutions\n        sc_loss /= len(self.stft_losses)\n        mag_loss /= len(self.stft_losses)\n\n        return sc_loss, mag_loss\n</code></pre>"},{"location":"training/loss/multi_resolution_stft_loss/#training.loss.multi_resolution_stft_loss.MultiResolutionSTFTLoss.__init__","title":"<code>__init__(resolutions)</code>","text":"<p>Initialize Multi resolution STFT loss module.</p> <p>Parameters:</p> Name Type Description Default <code>resolutions</code> <code>list</code> <p>List of (FFT size, shift size, window length).</p> required Source code in <code>training/loss/multi_resolution_stft_loss.py</code> <pre><code>def __init__(\n    self,\n    resolutions: list[tuple[int, int, int]],\n):\n    r\"\"\"Initialize Multi resolution STFT loss module.\n\n    Args:\n        resolutions (list): List of (FFT size, shift size, window length).\n    \"\"\"\n    super().__init__()\n\n    self.stft_losses = torch.nn.ModuleList(\n        [STFTLoss(fs, ss, wl) for fs, ss, wl in resolutions],\n    )\n</code></pre>"},{"location":"training/loss/multi_resolution_stft_loss/#training.loss.multi_resolution_stft_loss.MultiResolutionSTFTLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Calculate forward propagation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted signal (B, T).</p> required <code>y</code> <code>Tensor</code> <p>Groundtruth signal (B, T).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Multi resolution spectral convergence loss value.</p> <code>Tensor</code> <code>Tensor</code> <p>Multi resolution log STFT magnitude loss value.</p> Source code in <code>training/loss/multi_resolution_stft_loss.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, y: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Calculate forward propagation.\n\n    Args:\n        x (Tensor): Predicted signal (B, T).\n        y (Tensor): Groundtruth signal (B, T).\n\n    Returns:\n        Tensor: Multi resolution spectral convergence loss value.\n        Tensor: Multi resolution log STFT magnitude loss value.\n    \"\"\"\n    sc_loss = torch.tensor(0.0, device=x.device)\n    mag_loss = torch.tensor(0.0, device=x.device)\n\n    # Compute the spectral convergence and log STFT magnitude losses for each resolution\n    for f in self.stft_losses:\n        sc_l, mag_l = f(x, y)\n        sc_loss += sc_l\n        mag_loss += mag_l\n\n    # Average the losses across all resolutions\n    sc_loss /= len(self.stft_losses)\n    mag_loss /= len(self.stft_losses)\n\n    return sc_loss, mag_loss\n</code></pre>"},{"location":"training/loss/readme/","title":"References","text":""},{"location":"training/loss/readme/#references","title":"References","text":"<p>Here you can find docs for the loss functions.</p>"},{"location":"training/loss/readme/#acoustic-model-loss","title":"Acoustic model loss","text":""},{"location":"training/loss/readme/#bin-loss","title":"Bin loss","text":"<p>Binary cross-entropy loss for hard and soft attention.</p>"},{"location":"training/loss/readme/#forward-sum-loss","title":"Forward sum loss","text":"<p>Computes the forward sum loss for sequence-to-sequence models with attention.</p>"},{"location":"training/loss/readme/#fast-speech-2-loss","title":"Fast Speech 2 loss","text":"<p>FastSpeech 2 Loss module</p>"},{"location":"training/loss/readme/#voicoder-univnet-loss","title":"Voicoder Univnet loss","text":""},{"location":"training/loss/readme/#stft","title":"STFT","text":"<p>Perform STFT and convert to magnitude spectrogram. STFT stands for Short-Time Fourier Transform. It is a signal processing technique that is used to analyze the frequency content of a signal over time. The STFT is computed by dividing a long signal into shorter segments, and then computing the Fourier transform of each segment. This results in a time-frequency representation of the signal, where the frequency content of the signal is shown as a function of time.</p>"},{"location":"training/loss/readme/#spectral-convergence-loss","title":"Spectral Convergence Loss","text":"<p>Spectral convergence loss is a measure of the similarity between two magnitude spectrograms.</p> <p>The spectral convergence loss is calculated as the Frobenius norm of the difference between the predicted and groundtruth magnitude spectrograms, divided by the Frobenius norm of the groundtruth magnitude spectrogram. The Frobenius norm is a matrix norm that is equivalent to the square root of the sum of the squared elements of a matrix.</p> <p>The spectral convergence loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content. A lower spectral convergence loss indicates a better match between the predicted and groundtruth signals.</p>"},{"location":"training/loss/readme/#log-stft-magnitude-loss","title":"Log STFT Magnitude Loss","text":"<p>Log STFT magnitude loss is a loss function that is commonly used in speech and audio signal processing tasks, such as speech enhancement and source separation. It is a modification of the spectral convergence loss, which measures the similarity between two magnitude spectrograms.</p> <p>The log STFT magnitude loss is calculated as the mean squared error between the logarithm of the predicted and groundtruth magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale. The mean squared error is used to penalize large errors between the predicted and groundtruth spectrograms.</p>"},{"location":"training/loss/readme/#stft-loss","title":"STFT Loss","text":"<p>STFT loss is a combination of two loss functions: the spectral convergence loss and the log STFT magnitude loss.</p> <p>The spectral convergence loss measures the similarity between two magnitude spectrograms, while the log STFT magnitude loss measures the similarity between two logarithmically-scaled magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale.</p> <p>The STFT loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content on both a linear and decibel scale. A lower STFT loss indicates a better match between the predicted and groundtruth signals.</p>"},{"location":"training/loss/readme/#multi-resolution-stft-loss","title":"Multi Resolution STFT Loss","text":"<p>The Multi resolution STFT loss module is a PyTorch module that computes the spectral convergence and log STFT magnitude losses for a predicted signal and a groundtruth signal at multiple resolutions. The module is designed for speech and audio signal processing tasks, such as speech enhancement and source separation.</p>"},{"location":"training/loss/readme/#univnet-loss","title":"Univnet loss","text":"<p><code>UnivnetLoss</code> is a PyTorch Module that calculates the generator and <code>Discriminator</code> losses for <code>Univnet</code>.</p>"},{"location":"training/loss/spectral_convergence_loss/","title":"Spectral Convergence Loss","text":""},{"location":"training/loss/spectral_convergence_loss/#training.loss.spectral_convergence_loss.SpectralConvergengeLoss","title":"<code>SpectralConvergengeLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Spectral convergence loss module. Spectral convergence loss is a measure of the similarity between two magnitude spectrograms.</p> <p>The spectral convergence loss is calculated as the Frobenius norm of the difference between the predicted and groundtruth magnitude spectrograms, divided by the Frobenius norm of the groundtruth magnitude spectrogram. The Frobenius norm is a matrix norm that is equivalent to the square root of the sum of the squared elements of a matrix.</p> <p>The spectral convergence loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content. A lower spectral convergence loss indicates a better match between the predicted and groundtruth signals.</p> Source code in <code>training/loss/spectral_convergence_loss.py</code> <pre><code>class SpectralConvergengeLoss(Module):\n    r\"\"\"Spectral convergence loss module.\n    Spectral convergence loss is a measure of the similarity between two magnitude spectrograms.\n\n    The spectral convergence loss is calculated as the Frobenius norm of the difference between the predicted and groundtruth magnitude spectrograms, divided by the Frobenius norm of the groundtruth magnitude spectrogram. The Frobenius norm is a matrix norm that is equivalent to the square root of the sum of the squared elements of a matrix.\n\n    The spectral convergence loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content. A lower spectral convergence loss indicates a better match between the predicted and groundtruth signals.\n    \"\"\"\n\n    def __init__(self):\n        r\"\"\"Initilize spectral convergence loss module.\"\"\"\n        super().__init__()\n\n    def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n        \"\"\"\n        # Ensure that x_mag and y_mag have the same size along dimension 1\n        min_len = min(x_mag.shape[1], y_mag.shape[1])\n        x_mag = x_mag[:, :min_len]\n        y_mag = y_mag[:, :min_len]\n\n        return torch.norm(y_mag - x_mag, p=\"fro\") / torch.norm(y_mag, p=\"fro\")\n</code></pre>"},{"location":"training/loss/spectral_convergence_loss/#training.loss.spectral_convergence_loss.SpectralConvergengeLoss.__init__","title":"<code>__init__()</code>","text":"<p>Initilize spectral convergence loss module.</p> Source code in <code>training/loss/spectral_convergence_loss.py</code> <pre><code>def __init__(self):\n    r\"\"\"Initilize spectral convergence loss module.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"training/loss/spectral_convergence_loss/#training.loss.spectral_convergence_loss.SpectralConvergengeLoss.forward","title":"<code>forward(x_mag, y_mag)</code>","text":"<p>Calculate forward propagation.</p> <p>Parameters:</p> Name Type Description Default <code>x_mag</code> <code>Tensor</code> <p>Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).</p> required <code>y_mag</code> <code>Tensor</code> <p>Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Spectral convergence loss value.</p> Source code in <code>training/loss/spectral_convergence_loss.py</code> <pre><code>def forward(self, x_mag: torch.Tensor, y_mag: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Calculate forward propagation.\n\n    Args:\n        x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n        y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n    Returns:\n        Tensor: Spectral convergence loss value.\n    \"\"\"\n    # Ensure that x_mag and y_mag have the same size along dimension 1\n    min_len = min(x_mag.shape[1], y_mag.shape[1])\n    x_mag = x_mag[:, :min_len]\n    y_mag = y_mag[:, :min_len]\n\n    return torch.norm(y_mag - x_mag, p=\"fro\") / torch.norm(y_mag, p=\"fro\")\n</code></pre>"},{"location":"training/loss/stft/","title":"STFT","text":""},{"location":"training/loss/stft/#training.loss.stft.stft","title":"<code>stft(x, fft_size, hop_size, win_length, window)</code>","text":"<p>Perform STFT and convert to magnitude spectrogram. STFT stands for Short-Time Fourier Transform. It is a signal processing technique that is used to analyze the frequency content of a signal over time. The STFT is computed by dividing a long signal into shorter segments, and then computing the Fourier transform of each segment. This results in a time-frequency representation of the signal, where the frequency content of the signal is shown as a function of time.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input signal tensor (B, T).</p> required <code>fft_size</code> <code>int</code> <p>FFT size.</p> required <code>hop_size</code> <code>int</code> <p>Hop size.</p> required <code>win_length</code> <code>Tensor</code> <p>Window length.</p> required <code>window</code> <code>str</code> <p>Window function type.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Magnitude spectrogram (B, #frames, fft_size // 2 + 1).</p> Source code in <code>training/loss/stft.py</code> <pre><code>def stft(\n    x: torch.Tensor,\n    fft_size: int,\n    hop_size: int,\n    win_length: int,\n    window: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"Perform STFT and convert to magnitude spectrogram.\n    STFT stands for Short-Time Fourier Transform. It is a signal processing technique that is used to analyze the frequency content of a signal over time. The STFT is computed by dividing a long signal into shorter segments, and then computing the Fourier transform of each segment. This results in a time-frequency representation of the signal, where the frequency content of the signal is shown as a function of time.\n\n    Args:\n        x (Tensor): Input signal tensor (B, T).\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (torch.Tensor): Window length.\n        window (str): Window function type.\n\n    Returns:\n        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n    \"\"\"\n    x_stft = torch.stft(x, fft_size, hop_size, win_length, window, return_complex=True)\n    x_stft = torch.view_as_real(x_stft)\n\n    real = x_stft[..., 0]\n    imag = x_stft[..., 1]\n\n    # NOTE (kan-bayashi): clamp is needed to avoid nan or inf\n    return torch.sqrt(torch.clamp(real**2 + imag**2, min=1e-7)).transpose(2, 1)\n</code></pre>"},{"location":"training/loss/stft_loss/","title":"STFT Loss","text":""},{"location":"training/loss/stft_loss/#training.loss.stft_loss.STFTLoss","title":"<code>STFTLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>STFT loss module.</p> <p>STFT loss is a combination of two loss functions: the spectral convergence loss and the log STFT magnitude loss.</p> <p>The spectral convergence loss measures the similarity between two magnitude spectrograms, while the log STFT magnitude loss measures the similarity between two logarithmically-scaled magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale.</p> <p>The STFT loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content on both a linear and decibel scale. A lower STFT loss indicates a better match between the predicted and groundtruth signals.</p> <p>Parameters:</p> Name Type Description Default <code>fft_size</code> <code>int</code> <p>FFT size.</p> <code>1024</code> <code>shift_size</code> <code>int</code> <p>Shift size.</p> <code>120</code> <code>win_length</code> <code>int</code> <p>Window length.</p> <code>600</code> Source code in <code>training/loss/stft_loss.py</code> <pre><code>class STFTLoss(Module):\n    r\"\"\"STFT loss module.\n\n    STFT loss is a combination of two loss functions: the spectral convergence loss and the log STFT magnitude loss.\n\n    The spectral convergence loss measures the similarity between two magnitude spectrograms, while the log STFT magnitude loss measures the similarity between two logarithmically-scaled magnitude spectrograms. The logarithm is applied to the magnitude spectrograms to convert them to a decibel scale, which is more perceptually meaningful than the linear scale.\n\n    The STFT loss is a useful metric for evaluating the quality of a predicted signal, as it measures the degree to which the predicted signal matches the groundtruth signal in terms of its spectral content on both a linear and decibel scale. A lower STFT loss indicates a better match between the predicted and groundtruth signals.\n\n    Args:\n        fft_size (int): FFT size.\n        shift_size (int): Shift size.\n        win_length (int): Window length.\n    \"\"\"\n\n    def __init__(\n        self,\n        fft_size: int = 1024,\n        shift_size: int = 120,\n        win_length: int = 600,\n    ):\n        r\"\"\"Initialize STFT loss module.\"\"\"\n        super().__init__()\n\n        self.fft_size = fft_size\n        self.shift_size = shift_size\n        self.win_length = win_length\n\n        self.register_buffer(\"window\", torch.hann_window(win_length))\n\n        self.spectral_convergenge_loss = SpectralConvergengeLoss()\n        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n\n    def forward(\n        self, x: torch.Tensor, y: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n            Tensor: Log STFT magnitude loss value.\n        \"\"\"\n        x_mag = stft(x, self.fft_size, self.shift_size, self.win_length, self.window)\n        y_mag = stft(y, self.fft_size, self.shift_size, self.win_length, self.window)\n\n        sc_loss = self.spectral_convergenge_loss(x_mag, y_mag)\n        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\n        return sc_loss, mag_loss\n</code></pre>"},{"location":"training/loss/stft_loss/#training.loss.stft_loss.STFTLoss.__init__","title":"<code>__init__(fft_size=1024, shift_size=120, win_length=600)</code>","text":"<p>Initialize STFT loss module.</p> Source code in <code>training/loss/stft_loss.py</code> <pre><code>def __init__(\n    self,\n    fft_size: int = 1024,\n    shift_size: int = 120,\n    win_length: int = 600,\n):\n    r\"\"\"Initialize STFT loss module.\"\"\"\n    super().__init__()\n\n    self.fft_size = fft_size\n    self.shift_size = shift_size\n    self.win_length = win_length\n\n    self.register_buffer(\"window\", torch.hann_window(win_length))\n\n    self.spectral_convergenge_loss = SpectralConvergengeLoss()\n    self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n</code></pre>"},{"location":"training/loss/stft_loss/#training.loss.stft_loss.STFTLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Calculate forward propagation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted signal (B, T).</p> required <code>y</code> <code>Tensor</code> <p>Groundtruth signal (B, T).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Spectral convergence loss value.</p> <code>Tensor</code> <code>Tensor</code> <p>Log STFT magnitude loss value.</p> Source code in <code>training/loss/stft_loss.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, y: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Calculate forward propagation.\n\n    Args:\n        x (Tensor): Predicted signal (B, T).\n        y (Tensor): Groundtruth signal (B, T).\n\n    Returns:\n        Tensor: Spectral convergence loss value.\n        Tensor: Log STFT magnitude loss value.\n    \"\"\"\n    x_mag = stft(x, self.fft_size, self.shift_size, self.win_length, self.window)\n    y_mag = stft(y, self.fft_size, self.shift_size, self.win_length, self.window)\n\n    sc_loss = self.spectral_convergenge_loss(x_mag, y_mag)\n    mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\n    return sc_loss, mag_loss\n</code></pre>"},{"location":"training/loss/univnet_loss/","title":"Univnet loss","text":""},{"location":"training/loss/univnet_loss/#training.loss.univnet_loss.UnivnetLoss","title":"<code>UnivnetLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>UnivnetLoss is a PyTorch Module that calculates the generator and discriminator losses for Univnet.</p> Source code in <code>training/loss/univnet_loss.py</code> <pre><code>class UnivnetLoss(Module):\n    r\"\"\"UnivnetLoss is a PyTorch Module that calculates the generator and discriminator losses for Univnet.\"\"\"\n\n    def __init__(self):\n        r\"\"\"Initializes the UnivnetLoss module.\"\"\"\n        super().__init__()\n\n        train_config = VocoderBasicConfig()\n\n        self.stft_lamb = train_config.stft_lamb\n        self.model_config = VocoderModelConfig()\n\n        self.stft_criterion = MultiResolutionSTFTLoss(self.model_config.mrd.resolutions)\n        self.esr_loss = ESRLoss()\n        self.sisdr_loss = SISDRLoss()\n        self.snr_loss = SNRLoss()\n        self.sdsdr_loss = SDSDRLoss()\n\n    def forward(\n        self,\n        audio: Tensor,\n        fake_audio: Tensor,\n        res_fake: List[Tuple[Tensor, Tensor]],\n        period_fake: List[Tuple[Tensor, Tensor]],\n        res_real: List[Tuple[Tensor, Tensor]],\n        period_real: List[Tuple[Tensor, Tensor]],\n    ) -&gt; Tuple[\n        Tensor,\n        Tensor,\n        Tensor,\n        Tensor,\n        Tensor,\n        Tensor,\n    ]:\n        r\"\"\"Calculate the losses for the generator and discriminator.\n\n        Args:\n            audio (torch.Tensor): The real audio samples.\n            fake_audio (torch.Tensor): The generated audio samples.\n            res_fake (List[Tuple[Tensor, Tensor]]): The discriminator's output for the fake audio.\n            period_fake (List[Tuple[Tensor, Tensor]]): The discriminator's output for the fake audio in the period.\n            res_real (List[Tuple[Tensor, Tensor]]): The discriminator's output for the real audio.\n            period_real (List[Tuple[Tensor, Tensor]]): The discriminator's output for the real audio in the period.\n\n        Returns:\n            tuple: A tuple containing the univnet loss, discriminator loss, STFT loss, score loss, ESR, SISDR, SNR and SDSDR losses.\n        \"\"\"\n        # Calculate the STFT loss\n        sc_loss, mag_loss = self.stft_criterion(fake_audio.squeeze(1), audio.squeeze(1))\n        stft_loss = (sc_loss + mag_loss) * self.stft_lamb\n\n        # Pad the fake audio to match the length of the real audio\n        padding = audio.shape[2] - fake_audio.shape[2]\n        fake_audio_padded = torch.nn.functional.pad(fake_audio, (0, padding))\n\n        esr_loss = self.esr_loss.forward(fake_audio_padded, audio)\n        snr_loss = self.snr_loss.forward(fake_audio_padded, audio)\n\n        # Calculate the score loss\n        score_loss = torch.tensor(0.0, device=audio.device)\n        for _, score_fake in res_fake + period_fake:\n            score_loss += torch.mean(torch.pow(score_fake - 1.0, 2))\n\n        score_loss = score_loss / len(res_fake + period_fake)\n\n        # Calculate the total generator loss\n        total_loss_gen = score_loss + stft_loss + esr_loss + snr_loss\n\n        # Calculate the discriminator loss\n        total_loss_disc = torch.tensor(0.0, device=audio.device)\n        for (_, score_fake), (_, score_real) in zip(\n            res_fake + period_fake, res_real + period_real\n        ):\n            total_loss_disc += torch.mean(torch.pow(score_real - 1.0, 2)) + torch.mean(\n                torch.pow(score_fake, 2)\n            )\n\n        total_loss_disc = total_loss_disc / len(res_fake + period_fake)\n\n        return (\n            total_loss_gen,\n            total_loss_disc,\n            stft_loss,\n            score_loss,\n            esr_loss,\n            snr_loss,\n        )\n</code></pre>"},{"location":"training/loss/univnet_loss/#training.loss.univnet_loss.UnivnetLoss.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the UnivnetLoss module.</p> Source code in <code>training/loss/univnet_loss.py</code> <pre><code>def __init__(self):\n    r\"\"\"Initializes the UnivnetLoss module.\"\"\"\n    super().__init__()\n\n    train_config = VocoderBasicConfig()\n\n    self.stft_lamb = train_config.stft_lamb\n    self.model_config = VocoderModelConfig()\n\n    self.stft_criterion = MultiResolutionSTFTLoss(self.model_config.mrd.resolutions)\n    self.esr_loss = ESRLoss()\n    self.sisdr_loss = SISDRLoss()\n    self.snr_loss = SNRLoss()\n    self.sdsdr_loss = SDSDRLoss()\n</code></pre>"},{"location":"training/loss/univnet_loss/#training.loss.univnet_loss.UnivnetLoss.forward","title":"<code>forward(audio, fake_audio, res_fake, period_fake, res_real, period_real)</code>","text":"<p>Calculate the losses for the generator and discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Tensor</code> <p>The real audio samples.</p> required <code>fake_audio</code> <code>Tensor</code> <p>The generated audio samples.</p> required <code>res_fake</code> <code>List[Tuple[Tensor, Tensor]]</code> <p>The discriminator's output for the fake audio.</p> required <code>period_fake</code> <code>List[Tuple[Tensor, Tensor]]</code> <p>The discriminator's output for the fake audio in the period.</p> required <code>res_real</code> <code>List[Tuple[Tensor, Tensor]]</code> <p>The discriminator's output for the real audio.</p> required <code>period_real</code> <code>List[Tuple[Tensor, Tensor]]</code> <p>The discriminator's output for the real audio in the period.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple containing the univnet loss, discriminator loss, STFT loss, score loss, ESR, SISDR, SNR and SDSDR losses.</p> Source code in <code>training/loss/univnet_loss.py</code> <pre><code>def forward(\n    self,\n    audio: Tensor,\n    fake_audio: Tensor,\n    res_fake: List[Tuple[Tensor, Tensor]],\n    period_fake: List[Tuple[Tensor, Tensor]],\n    res_real: List[Tuple[Tensor, Tensor]],\n    period_real: List[Tuple[Tensor, Tensor]],\n) -&gt; Tuple[\n    Tensor,\n    Tensor,\n    Tensor,\n    Tensor,\n    Tensor,\n    Tensor,\n]:\n    r\"\"\"Calculate the losses for the generator and discriminator.\n\n    Args:\n        audio (torch.Tensor): The real audio samples.\n        fake_audio (torch.Tensor): The generated audio samples.\n        res_fake (List[Tuple[Tensor, Tensor]]): The discriminator's output for the fake audio.\n        period_fake (List[Tuple[Tensor, Tensor]]): The discriminator's output for the fake audio in the period.\n        res_real (List[Tuple[Tensor, Tensor]]): The discriminator's output for the real audio.\n        period_real (List[Tuple[Tensor, Tensor]]): The discriminator's output for the real audio in the period.\n\n    Returns:\n        tuple: A tuple containing the univnet loss, discriminator loss, STFT loss, score loss, ESR, SISDR, SNR and SDSDR losses.\n    \"\"\"\n    # Calculate the STFT loss\n    sc_loss, mag_loss = self.stft_criterion(fake_audio.squeeze(1), audio.squeeze(1))\n    stft_loss = (sc_loss + mag_loss) * self.stft_lamb\n\n    # Pad the fake audio to match the length of the real audio\n    padding = audio.shape[2] - fake_audio.shape[2]\n    fake_audio_padded = torch.nn.functional.pad(fake_audio, (0, padding))\n\n    esr_loss = self.esr_loss.forward(fake_audio_padded, audio)\n    snr_loss = self.snr_loss.forward(fake_audio_padded, audio)\n\n    # Calculate the score loss\n    score_loss = torch.tensor(0.0, device=audio.device)\n    for _, score_fake in res_fake + period_fake:\n        score_loss += torch.mean(torch.pow(score_fake - 1.0, 2))\n\n    score_loss = score_loss / len(res_fake + period_fake)\n\n    # Calculate the total generator loss\n    total_loss_gen = score_loss + stft_loss + esr_loss + snr_loss\n\n    # Calculate the discriminator loss\n    total_loss_disc = torch.tensor(0.0, device=audio.device)\n    for (_, score_fake), (_, score_real) in zip(\n        res_fake + period_fake, res_real + period_real\n    ):\n        total_loss_disc += torch.mean(torch.pow(score_real - 1.0, 2)) + torch.mean(\n            torch.pow(score_fake, 2)\n        )\n\n    total_loss_disc = total_loss_disc / len(res_fake + period_fake)\n\n    return (\n        total_loss_gen,\n        total_loss_disc,\n        stft_loss,\n        score_loss,\n        esr_loss,\n        snr_loss,\n    )\n</code></pre>"},{"location":"training/preprocess/audio/","title":"Audio","text":""},{"location":"training/preprocess/audio/#training.preprocess.audio.normalize_loudness","title":"<code>normalize_loudness(wav)</code>","text":"<p>Normalize the loudness of an audio waveform.</p> <p>Parameters:</p> Name Type Description Default <code>wav</code> <code>Tensor</code> <p>The input waveform.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The normalized waveform.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wav = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; normalize_loudness(wav)\ntensor([0.33333333, 0.66666667, 1.  ])\n</code></pre> Source code in <code>training/preprocess/audio.py</code> <pre><code>def normalize_loudness(wav: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Normalize the loudness of an audio waveform.\n\n    Args:\n        wav (torch.Tensor): The input waveform.\n\n    Returns:\n        torch.Tensor: The normalized waveform.\n\n    Examples:\n        &gt;&gt;&gt; wav = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; normalize_loudness(wav)\n        tensor([0.33333333, 0.66666667, 1.  ])\n    \"\"\"\n    return wav / torch.max(torch.abs(wav))\n</code></pre>"},{"location":"training/preprocess/audio/#training.preprocess.audio.preprocess_audio","title":"<code>preprocess_audio(audio, sr_actual, sr)</code>","text":"<p>Preprocesses audio by converting stereo to mono, resampling if necessary, and returning the audio tensor and sample rate.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Tensor</code> <p>The audio tensor to preprocess.</p> required <code>sr_actual</code> <code>int</code> <p>The actual sample rate of the audio.</p> required <code>sr</code> <code>Union[int, None]</code> <p>The target sample rate to resample the audio to, if necessary.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, int]</code> <p>Tuple[torch.Tensor, int]: The preprocessed audio tensor and sample rate.</p> Source code in <code>training/preprocess/audio.py</code> <pre><code>def preprocess_audio(\n    audio: torch.Tensor, sr_actual: int, sr: Union[int, None],\n) -&gt; Tuple[torch.Tensor, int]:\n    r\"\"\"Preprocesses audio by converting stereo to mono, resampling if necessary, and returning the audio tensor and sample rate.\n\n    Args:\n        audio (torch.Tensor): The audio tensor to preprocess.\n        sr_actual (int): The actual sample rate of the audio.\n        sr (Union[int, None]): The target sample rate to resample the audio to, if necessary.\n\n    Returns:\n        Tuple[torch.Tensor, int]: The preprocessed audio tensor and sample rate.\n    \"\"\"\n    try:\n        if audio.shape[0] &gt; 0:\n            audio = stereo_to_mono(audio)\n        audio = audio.squeeze(0)\n        if sr_actual != sr and sr is not None:\n            audio_np = resample(audio.numpy(), orig_sr=sr_actual, target_sr=sr)\n            # Convert back to torch tensor\n            audio = torch.from_numpy(audio_np)\n            sr_actual = sr\n    except Exception as e:\n        raise type(e)(\n            f\"The following error happened while processing the audio ... \\n {e!s}\",\n        ).with_traceback(sys.exc_info()[2])\n\n    return audio, sr_actual\n</code></pre>"},{"location":"training/preprocess/audio/#training.preprocess.audio.resample","title":"<code>resample(wav, orig_sr, target_sr)</code>","text":"<p>Resamples an audio waveform from the original sampling rate to the target sampling rate.</p> <p>Parameters:</p> Name Type Description Default <code>wav</code> <code>ndarray</code> <p>The audio waveform to be resampled.</p> required <code>orig_sr</code> <code>int</code> <p>The original sampling rate of the audio waveform.</p> required <code>target_sr</code> <code>int</code> <p>The target sampling rate to resample the audio waveform to.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The resampled audio waveform.</p> Source code in <code>training/preprocess/audio.py</code> <pre><code>def resample(wav: np.ndarray, orig_sr: int, target_sr: int) -&gt; np.ndarray:\n    r\"\"\"Resamples an audio waveform from the original sampling rate to the target sampling rate.\n\n    Args:\n        wav (np.ndarray): The audio waveform to be resampled.\n        orig_sr (int): The original sampling rate of the audio waveform.\n        target_sr (int): The target sampling rate to resample the audio waveform to.\n\n    Returns:\n        np.ndarray: The resampled audio waveform.\n    \"\"\"\n    return librosa.resample(wav, orig_sr=orig_sr, target_sr=target_sr)\n</code></pre>"},{"location":"training/preprocess/audio/#training.preprocess.audio.safe_load","title":"<code>safe_load(path, sr)</code>","text":"<p>Load an audio file from disk and return its content as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the audio file.</p> required <code>sr</code> <code>int or None</code> <p>The target sampling rate. If None, the original sampling rate is used.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, int]</code> <p>Tuple[np.ndarray, int]: A tuple containing the audio content as a numpy array and the actual sampling rate.</p> Source code in <code>training/preprocess/audio.py</code> <pre><code>def safe_load(path: str, sr: Union[int, None]) -&gt; Tuple[np.ndarray, int]:\n    r\"\"\"Load an audio file from disk and return its content as a numpy array.\n\n    Args:\n        path (str): The path to the audio file.\n        sr (int or None): The target sampling rate. If None, the original sampling rate is used.\n\n    Returns:\n        Tuple[np.ndarray, int]: A tuple containing the audio content as a numpy array and the actual sampling rate.\n    \"\"\"\n    try:\n        audio, sr_actual = torchaudio.load(path) # type: ignore\n        if audio.shape[0] &gt; 0:\n            audio = stereo_to_mono(audio)\n        audio = audio.squeeze(0)\n        if sr_actual != sr and sr is not None:\n            audio = resample(audio.numpy(), orig_sr=sr_actual, target_sr=sr)\n            sr_actual = sr\n        else:\n            audio = audio.numpy()\n    except Exception as e:\n        raise type(e)(\n            f\"The following error happened loading the file {path} ... \\n\" + str(e),\n        ).with_traceback(sys.exc_info()[2])\n\n    return audio, sr_actual\n</code></pre>"},{"location":"training/preprocess/audio/#training.preprocess.audio.stereo_to_mono","title":"<code>stereo_to_mono(audio)</code>","text":"<p>Converts a stereo audio tensor to mono by taking the mean across channels.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Tensor</code> <p>Input audio tensor of shape (channels, samples).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Mono audio tensor of shape (1, samples).</p> Source code in <code>training/preprocess/audio.py</code> <pre><code>def stereo_to_mono(audio: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Converts a stereo audio tensor to mono by taking the mean across channels.\n\n    Args:\n        audio (torch.Tensor): Input audio tensor of shape (channels, samples).\n\n    Returns:\n        torch.Tensor: Mono audio tensor of shape (1, samples).\n    \"\"\"\n    return torch.mean(audio, 0, True)\n</code></pre>"},{"location":"training/preprocess/audio_processor/","title":"Audio Processor","text":""},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor","title":"<code>AudioProcessor</code>","text":"<p>A class used to process audio signals and convert them into different representations.</p> <p>Attributes:</p> Name Type Description <code>hann_window</code> <code>dict</code> <p>A dictionary to store the Hann window for different configurations.</p> <code>mel_basis</code> <code>dict</code> <p>A dictionary to store the Mel basis for different configurations.</p> <p>Methods:</p> Name Description <code>name_mel_basis</code> <p>Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.</p> <code>amp_to_db</code> <p>Convert amplitude to decibels (dB).</p> <code>db_to_amp</code> <p>Convert decibels (dB) to amplitude.</p> <code>wav_to_spec</code> <p>Convert a waveform to a spectrogram and compute the magnitude.</p> <code>wav_to_energy</code> <p>Convert a waveform to a spectrogram and compute the energy.</p> <code>spec_to_mel</code> <p>Convert a spectrogram to a Mel spectrogram.</p> <code>wav_to_mel</code> <p>Convert a waveform to a Mel spectrogram.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>class AudioProcessor:\n    r\"\"\"A class used to process audio signals and convert them into different representations.\n\n    Attributes:\n        hann_window (dict): A dictionary to store the Hann window for different configurations.\n        mel_basis (dict): A dictionary to store the Mel basis for different configurations.\n\n    Methods:\n        name_mel_basis(spec, n_fft, fmax): Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n        amp_to_db(magnitudes, C=1, clip_val=1e-5): Convert amplitude to decibels (dB).\n        db_to_amp(magnitudes, C=1): Convert decibels (dB) to amplitude.\n        wav_to_spec(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the magnitude.\n        wav_to_energy(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the energy.\n        spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax): Convert a spectrogram to a Mel spectrogram.\n        wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False): Convert a waveform to a Mel spectrogram.\n    \"\"\"\n\n    def __init__(self):\n        self.hann_window = {}\n        self.mel_basis = {}\n\n    @staticmethod\n    def name_mel_basis(spec: torch.Tensor, n_fft: int, fmax: int) -&gt; str:\n        \"\"\"Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n\n        Args:\n            spec (torch.Tensor): The spectrogram tensor.\n            n_fft (int): The FFT size.\n            fmax (int): The maximum frequency.\n\n        Returns:\n            str: The generated name for the Mel basis.\n        \"\"\"\n        n_fft_len = f\"{n_fft}_{fmax}_{spec.dtype}_{spec.device}\"\n        return n_fft_len\n\n    @staticmethod\n    def amp_to_db(magnitudes: torch.Tensor, C: int = 1, clip_val: float = 1e-5) -&gt; torch.Tensor:\n        r\"\"\"Convert amplitude to decibels (dB).\n\n        Args:\n            magnitudes (Tensor): The amplitude magnitudes to convert.\n            C (int, optional): A constant value used in the conversion. Defaults to 1.\n            clip_val (float, optional): A value to clamp the magnitudes to avoid taking the log of zero. Defaults to 1e-5.\n\n        Returns:\n            Tensor: The converted magnitudes in dB.\n        \"\"\"\n        return torch.log(torch.clamp(magnitudes, min=clip_val) * C)\n\n    @staticmethod\n    def db_to_amp(magnitudes: torch.Tensor, C: int = 1) -&gt; torch.Tensor:\n        r\"\"\"Convert decibels (dB) to amplitude.\n\n        Args:\n            magnitudes (Tensor): The dB magnitudes to convert.\n            C (int, optional): A constant value used in the conversion. Defaults to 1.\n\n        Returns:\n            Tensor: The converted magnitudes in amplitude.\n        \"\"\"\n        return torch.exp(magnitudes) / C\n\n    def wav_to_spec(\n        self,\n        y: torch.Tensor,\n        n_fft: int,\n        hop_length: int,\n        win_length: int,\n        center: bool = False,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Convert a waveform to a spectrogram and compute the magnitude.\n\n        Args:\n            y (Tensor): The input waveform.\n            n_fft (int): The FFT size.\n            hop_length (int): The hop (stride) size.\n            win_length (int): The window size.\n            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n        Returns:\n            Tensor: The magnitude of the computed spectrogram.\n        \"\"\"\n        y = y.squeeze(1)\n\n        dtype_device = str(y.dtype) + \"_\" + str(y.device)\n        wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n        if wnsize_dtype_device not in self.hann_window:\n            self.hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n        y = torch.nn.functional.pad(\n            y.unsqueeze(1),\n            (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n            mode=\"reflect\",\n        )\n        y = y.squeeze(1)\n\n        spec = torch.stft(\n            y,\n            n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            window=self.hann_window[wnsize_dtype_device],\n            center=center,\n            pad_mode=\"reflect\",\n            normalized=False,\n            onesided=True,\n            return_complex=True,\n        )\n\n        spec = torch.view_as_real(spec)\n\n        # Compute the magnitude\n        spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n        return spec\n\n    def wav_to_energy(\n        self,\n        y: torch.Tensor,\n        n_fft: int,\n        hop_length: int,\n        win_length: int,\n        center: bool = False,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Convert a waveform to a spectrogram and compute the energy.\n\n        Args:\n            y (Tensor): The input waveform.\n            n_fft (int): The FFT size.\n            hop_length (int): The hop (stride) size.\n            win_length (int): The window size.\n            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n        Returns:\n            Tensor: The energy of the computed spectrogram.\n        \"\"\"\n        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n        spec = torch.norm(spec, dim=1, keepdim=True).squeeze(0)\n\n        # Normalize the energy\n        return (spec - spec.mean()) / spec.std()\n\n    def spec_to_mel(\n            self,\n            spec: torch.Tensor,\n            n_fft: int,\n            num_mels: int,\n            sample_rate: int,\n            fmin: int,\n            fmax: int,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Convert a spectrogram to a Mel spectrogram.\n\n        Args:\n            spec (torch.Tensor): The input spectrogram of shape [B, C, T].\n            n_fft (int): The FFT size.\n            num_mels (int): The number of Mel bands.\n            sample_rate (int): The sample rate of the audio.\n            fmin (int): The minimum frequency.\n            fmax (int): The maximum frequency.\n\n        Returns:\n            torch.Tensor: The computed Mel spectrogram of shape [B, C, T].\n        \"\"\"\n        mel_basis_key = self.name_mel_basis(spec, n_fft, fmax)\n\n        if mel_basis_key not in self.mel_basis:\n            mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n            self.mel_basis[mel_basis_key] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n\n        mel = torch.matmul(self.mel_basis[mel_basis_key], spec)\n        mel = self.amp_to_db(mel)\n\n        return mel\n\n    def wav_to_mel(\n        self,\n        y: torch.Tensor,\n        n_fft: int,\n        num_mels: int,\n        sample_rate: int,\n        hop_length: int,\n        win_length: int,\n        fmin: int,\n        fmax: int,\n        center: bool = False,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Convert a waveform to a Mel spectrogram.\n\n        Args:\n            y (torch.Tensor): The input waveform.\n            n_fft (int): The FFT size.\n            num_mels (int): The number of Mel bands.\n            sample_rate (int): The sample rate of the audio.\n            hop_length (int): The hop (stride) size.\n            win_length (int): The window size.\n            fmin (int): The minimum frequency.\n            fmax (int): The maximum frequency.\n            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n        Returns:\n            torch.Tensor: The computed Mel spectrogram.\n        \"\"\"\n        # Convert the waveform to a spectrogram\n        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n\n        # Convert the spectrogram to a Mel spectrogram\n        mel = self.spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax)\n\n        return mel\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.amp_to_db","title":"<code>amp_to_db(magnitudes, C=1, clip_val=1e-05)</code>  <code>staticmethod</code>","text":"<p>Convert amplitude to decibels (dB).</p> <p>Parameters:</p> Name Type Description Default <code>magnitudes</code> <code>Tensor</code> <p>The amplitude magnitudes to convert.</p> required <code>C</code> <code>int</code> <p>A constant value used in the conversion. Defaults to 1.</p> <code>1</code> <code>clip_val</code> <code>float</code> <p>A value to clamp the magnitudes to avoid taking the log of zero. Defaults to 1e-5.</p> <code>1e-05</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The converted magnitudes in dB.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>@staticmethod\ndef amp_to_db(magnitudes: torch.Tensor, C: int = 1, clip_val: float = 1e-5) -&gt; torch.Tensor:\n    r\"\"\"Convert amplitude to decibels (dB).\n\n    Args:\n        magnitudes (Tensor): The amplitude magnitudes to convert.\n        C (int, optional): A constant value used in the conversion. Defaults to 1.\n        clip_val (float, optional): A value to clamp the magnitudes to avoid taking the log of zero. Defaults to 1e-5.\n\n    Returns:\n        Tensor: The converted magnitudes in dB.\n    \"\"\"\n    return torch.log(torch.clamp(magnitudes, min=clip_val) * C)\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.db_to_amp","title":"<code>db_to_amp(magnitudes, C=1)</code>  <code>staticmethod</code>","text":"<p>Convert decibels (dB) to amplitude.</p> <p>Parameters:</p> Name Type Description Default <code>magnitudes</code> <code>Tensor</code> <p>The dB magnitudes to convert.</p> required <code>C</code> <code>int</code> <p>A constant value used in the conversion. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The converted magnitudes in amplitude.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>@staticmethod\ndef db_to_amp(magnitudes: torch.Tensor, C: int = 1) -&gt; torch.Tensor:\n    r\"\"\"Convert decibels (dB) to amplitude.\n\n    Args:\n        magnitudes (Tensor): The dB magnitudes to convert.\n        C (int, optional): A constant value used in the conversion. Defaults to 1.\n\n    Returns:\n        Tensor: The converted magnitudes in amplitude.\n    \"\"\"\n    return torch.exp(magnitudes) / C\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.name_mel_basis","title":"<code>name_mel_basis(spec, n_fft, fmax)</code>  <code>staticmethod</code>","text":"<p>Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>Tensor</code> <p>The spectrogram tensor.</p> required <code>n_fft</code> <code>int</code> <p>The FFT size.</p> required <code>fmax</code> <code>int</code> <p>The maximum frequency.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated name for the Mel basis.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>@staticmethod\ndef name_mel_basis(spec: torch.Tensor, n_fft: int, fmax: int) -&gt; str:\n    \"\"\"Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n\n    Args:\n        spec (torch.Tensor): The spectrogram tensor.\n        n_fft (int): The FFT size.\n        fmax (int): The maximum frequency.\n\n    Returns:\n        str: The generated name for the Mel basis.\n    \"\"\"\n    n_fft_len = f\"{n_fft}_{fmax}_{spec.dtype}_{spec.device}\"\n    return n_fft_len\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.spec_to_mel","title":"<code>spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax)</code>","text":"<p>Convert a spectrogram to a Mel spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>Tensor</code> <p>The input spectrogram of shape [B, C, T].</p> required <code>n_fft</code> <code>int</code> <p>The FFT size.</p> required <code>num_mels</code> <code>int</code> <p>The number of Mel bands.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>fmin</code> <code>int</code> <p>The minimum frequency.</p> required <code>fmax</code> <code>int</code> <p>The maximum frequency.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed Mel spectrogram of shape [B, C, T].</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>def spec_to_mel(\n        self,\n        spec: torch.Tensor,\n        n_fft: int,\n        num_mels: int,\n        sample_rate: int,\n        fmin: int,\n        fmax: int,\n) -&gt; torch.Tensor:\n    r\"\"\"Convert a spectrogram to a Mel spectrogram.\n\n    Args:\n        spec (torch.Tensor): The input spectrogram of shape [B, C, T].\n        n_fft (int): The FFT size.\n        num_mels (int): The number of Mel bands.\n        sample_rate (int): The sample rate of the audio.\n        fmin (int): The minimum frequency.\n        fmax (int): The maximum frequency.\n\n    Returns:\n        torch.Tensor: The computed Mel spectrogram of shape [B, C, T].\n    \"\"\"\n    mel_basis_key = self.name_mel_basis(spec, n_fft, fmax)\n\n    if mel_basis_key not in self.mel_basis:\n        mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        self.mel_basis[mel_basis_key] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n\n    mel = torch.matmul(self.mel_basis[mel_basis_key], spec)\n    mel = self.amp_to_db(mel)\n\n    return mel\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.wav_to_energy","title":"<code>wav_to_energy(y, n_fft, hop_length, win_length, center=False)</code>","text":"<p>Convert a waveform to a spectrogram and compute the energy.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>The input waveform.</p> required <code>n_fft</code> <code>int</code> <p>The FFT size.</p> required <code>hop_length</code> <code>int</code> <p>The hop (stride) size.</p> required <code>win_length</code> <code>int</code> <p>The window size.</p> required <code>center</code> <code>bool</code> <p>Whether to pad <code>y</code> such that frames are centered. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The energy of the computed spectrogram.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>def wav_to_energy(\n    self,\n    y: torch.Tensor,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    center: bool = False,\n) -&gt; torch.Tensor:\n    r\"\"\"Convert a waveform to a spectrogram and compute the energy.\n\n    Args:\n        y (Tensor): The input waveform.\n        n_fft (int): The FFT size.\n        hop_length (int): The hop (stride) size.\n        win_length (int): The window size.\n        center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n    Returns:\n        Tensor: The energy of the computed spectrogram.\n    \"\"\"\n    spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n    spec = torch.norm(spec, dim=1, keepdim=True).squeeze(0)\n\n    # Normalize the energy\n    return (spec - spec.mean()) / spec.std()\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.wav_to_mel","title":"<code>wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False)</code>","text":"<p>Convert a waveform to a Mel spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>The input waveform.</p> required <code>n_fft</code> <code>int</code> <p>The FFT size.</p> required <code>num_mels</code> <code>int</code> <p>The number of Mel bands.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>hop_length</code> <code>int</code> <p>The hop (stride) size.</p> required <code>win_length</code> <code>int</code> <p>The window size.</p> required <code>fmin</code> <code>int</code> <p>The minimum frequency.</p> required <code>fmax</code> <code>int</code> <p>The maximum frequency.</p> required <code>center</code> <code>bool</code> <p>Whether to pad <code>y</code> such that frames are centered. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed Mel spectrogram.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>def wav_to_mel(\n    self,\n    y: torch.Tensor,\n    n_fft: int,\n    num_mels: int,\n    sample_rate: int,\n    hop_length: int,\n    win_length: int,\n    fmin: int,\n    fmax: int,\n    center: bool = False,\n) -&gt; torch.Tensor:\n    r\"\"\"Convert a waveform to a Mel spectrogram.\n\n    Args:\n        y (torch.Tensor): The input waveform.\n        n_fft (int): The FFT size.\n        num_mels (int): The number of Mel bands.\n        sample_rate (int): The sample rate of the audio.\n        hop_length (int): The hop (stride) size.\n        win_length (int): The window size.\n        fmin (int): The minimum frequency.\n        fmax (int): The maximum frequency.\n        center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n    Returns:\n        torch.Tensor: The computed Mel spectrogram.\n    \"\"\"\n    # Convert the waveform to a spectrogram\n    spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n\n    # Convert the spectrogram to a Mel spectrogram\n    mel = self.spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax)\n\n    return mel\n</code></pre>"},{"location":"training/preprocess/audio_processor/#training.preprocess.audio_processor.AudioProcessor.wav_to_spec","title":"<code>wav_to_spec(y, n_fft, hop_length, win_length, center=False)</code>","text":"<p>Convert a waveform to a spectrogram and compute the magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>The input waveform.</p> required <code>n_fft</code> <code>int</code> <p>The FFT size.</p> required <code>hop_length</code> <code>int</code> <p>The hop (stride) size.</p> required <code>win_length</code> <code>int</code> <p>The window size.</p> required <code>center</code> <code>bool</code> <p>Whether to pad <code>y</code> such that frames are centered. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The magnitude of the computed spectrogram.</p> Source code in <code>training/preprocess/audio_processor.py</code> <pre><code>def wav_to_spec(\n    self,\n    y: torch.Tensor,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    center: bool = False,\n) -&gt; torch.Tensor:\n    r\"\"\"Convert a waveform to a spectrogram and compute the magnitude.\n\n    Args:\n        y (Tensor): The input waveform.\n        n_fft (int): The FFT size.\n        hop_length (int): The hop (stride) size.\n        win_length (int): The window size.\n        center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n\n    Returns:\n        Tensor: The magnitude of the computed spectrogram.\n    \"\"\"\n    y = y.squeeze(1)\n\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n    if wnsize_dtype_device not in self.hann_window:\n        self.hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    spec = torch.stft(\n        y,\n        n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=self.hann_window[wnsize_dtype_device],\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=True,\n    )\n\n    spec = torch.view_as_real(spec)\n\n    # Compute the magnitude\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    return spec\n</code></pre>"},{"location":"training/preprocess/compute_yin/","title":"compute_yin","text":""},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.compute_pitch","title":"<code>compute_pitch(sig_torch, sr, w_len=1024, w_step=256, f0_min=50, f0_max=1000, harmo_thresh=0.25)</code>","text":"<p>Compute the pitch of an audio signal using the Yin algorithm.</p> <p>The Yin algorithm is a widely used method for pitch detection in speech and music signals. This function uses the Yin algorithm to compute the pitch of the input audio signal, and then normalizes and interpolates the pitch values. Returns the normalized and interpolated pitch values.</p> <p>Parameters:</p> Name Type Description Default <code>sig_torch</code> <code>Tensor</code> <p>The audio signal as a 1D numpy array of floats.</p> required <code>sr</code> <code>int</code> <p>The sampling rate of the signal.</p> required <code>w_len</code> <code>int</code> <p>The size of the analysis window in samples.</p> <code>1024</code> <code>w_step</code> <code>int</code> <p>The size of the lag between two consecutive windows in samples.</p> <code>256</code> <code>f0_min</code> <code>int</code> <p>The minimum fundamental frequency that can be detected in Hz.</p> <code>50</code> <code>f0_max</code> <code>int</code> <p>The maximum fundamental frequency that can be detected in Hz.</p> <code>1000</code> <code>harmo_thresh</code> <code>float</code> <p>The threshold of detection. The algorithm returns the first minimum of the CMND function below this threshold.</p> <code>0.25</code> <p>Returns:</p> Type Description <p>np.ndarray: The normalized and interpolated pitch values of the audio signal.</p> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def compute_pitch(\n    sig_torch: torch.Tensor,\n    sr: int,\n    w_len: int = 1024,\n    w_step: int = 256,\n    f0_min: int = 50,\n    f0_max: int = 1000,\n    harmo_thresh: float = 0.25,\n):\n    r\"\"\"Compute the pitch of an audio signal using the Yin algorithm.\n\n    The Yin algorithm is a widely used method for pitch detection in speech and music signals. This function uses the\n    Yin algorithm to compute the pitch of the input audio signal, and then normalizes and interpolates the pitch values.\n    Returns the normalized and interpolated pitch values.\n\n    Args:\n        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n        sr (int): The sampling rate of the signal.\n        w_len (int, optional): The size of the analysis window in samples.\n        w_step (int, optional): The size of the lag between two consecutive windows in samples.\n        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz.\n        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz.\n        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND function below this threshold.\n\n    Returns:\n        np.ndarray: The normalized and interpolated pitch values of the audio signal.\n    \"\"\"\n    pitch, _, _, _ = compute_yin(\n        sig_torch,\n        sr=sr,\n        w_len=w_len,\n        w_step=w_step,\n        f0_min=f0_min,\n        f0_max=f0_max,\n        harmo_thresh=harmo_thresh,\n    )\n\n    pitch, _ = norm_interp_f0(pitch)\n\n    return pitch\n</code></pre>"},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.compute_yin","title":"<code>compute_yin(sig_torch, sr, w_len=512, w_step=256, f0_min=100, f0_max=500, harmo_thresh=0.1)</code>","text":"<p>Compute the Yin Algorithm for pitch detection on an audio signal.</p> <p>The Yin Algorithm is a widely used method for pitch detection in speech and music signals. It works by computing the Cumulative Mean Normalized Difference function (CMND) of the difference function of the signal, and finding the first minimum of the CMND below a given threshold. The fundamental period of the signal is then estimated as the inverse of the lag corresponding to this minimum.</p> <p>Parameters:</p> Name Type Description Default <code>sig_torch</code> <code>Tensor</code> <p>The audio signal as a 1D numpy array of floats.</p> required <code>sr</code> <code>int</code> <p>The sampling rate of the signal.</p> required <code>w_len</code> <code>int</code> <p>The size of the analysis window in samples. Defaults to 512.</p> <code>512</code> <code>w_step</code> <code>int</code> <p>The size of the lag between two consecutive windows in samples. Defaults to 256.</p> <code>256</code> <code>f0_min</code> <code>int</code> <p>The minimum fundamental frequency that can be detected in Hz. Defaults to 100.</p> <code>100</code> <code>f0_max</code> <code>int</code> <p>The maximum fundamental frequency that can be detected in Hz. Defaults to 500.</p> <code>500</code> <code>harmo_thresh</code> <code>float</code> <p>The threshold of detection. The algorithm returns the first minimum of the CMND function below this threshold. Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, List[float], List[float], List[float]]</code> <p>Tuple[np.ndarray, List[float], List[float], List[float]]: A tuple containing the following elements: * pitches (np.ndarray): A 1D numpy array of fundamental frequencies estimated for each analysis window. * harmonic_rates (List[float]): A list of harmonic rate values for each fundamental frequency value, which   can be interpreted as a confidence value. * argmins (List[float]): A list of the minimums of the Cumulative Mean Normalized Difference Function for   each analysis window. * times (List[float]): A list of the time of each estimation, in seconds.</p> References <p>[1] A. K. Jain, Fundamentals of Digital Image Processing, Prentice Hall, 1989. [2] A. de Cheveign\u00e9 and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal     of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.</p> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def compute_yin(\n    sig_torch: torch.Tensor,\n    sr: int,\n    w_len: int = 512,\n    w_step: int = 256,\n    f0_min: int = 100,\n    f0_max: int = 500,\n    harmo_thresh: float = 0.1,\n) -&gt; Tuple[np.ndarray, List[float], List[float], List[float]]:\n    r\"\"\"Compute the Yin Algorithm for pitch detection on an audio signal.\n\n    The Yin Algorithm is a widely used method for pitch detection in speech and music signals. It works by computing the\n    Cumulative Mean Normalized Difference function (CMND) of the difference function of the signal, and finding the first\n    minimum of the CMND below a given threshold. The fundamental period of the signal is then estimated as the inverse of\n    the lag corresponding to this minimum.\n\n    Args:\n        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n        sr (int): The sampling rate of the signal.\n        w_len (int, optional): The size of the analysis window in samples. Defaults to 512.\n        w_step (int, optional): The size of the lag between two consecutive windows in samples. Defaults to 256.\n        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz. Defaults to 100.\n        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz. Defaults to 500.\n        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND\n            function below this threshold. Defaults to 0.1.\n\n    Returns:\n        Tuple[np.ndarray, List[float], List[float], List[float]]: A tuple containing the following elements:\n            * pitches (np.ndarray): A 1D numpy array of fundamental frequencies estimated for each analysis window.\n            * harmonic_rates (List[float]): A list of harmonic rate values for each fundamental frequency value, which\n              can be interpreted as a confidence value.\n            * argmins (List[float]): A list of the minimums of the Cumulative Mean Normalized Difference Function for\n              each analysis window.\n            * times (List[float]): A list of the time of each estimation, in seconds.\n\n    References:\n        [1] A. K. Jain, Fundamentals of Digital Image Processing, Prentice Hall, 1989.\n        [2] A. de Cheveign\u00e9 and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal\n            of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n    \"\"\"\n    sig_torch = sig_torch.view(1, 1, -1)\n    sig_torch = F.pad(\n        sig_torch.unsqueeze(1),\n        (int((w_len - w_step) / 2), int((w_len - w_step) / 2), 0, 0),\n        mode=\"reflect\",\n    )\n    sig_torch_n: np.ndarray = sig_torch.view(-1).numpy()\n\n    tau_min = int(sr / f0_max)\n    tau_max = int(sr / f0_min)\n\n    timeScale = range(\n        0, len(sig_torch_n) - w_len, w_step,\n    )  # time values for each analysis window\n    times = [t / float(sr) for t in timeScale]\n    frames = [sig_torch_n[t : t + w_len] for t in timeScale]\n\n    pitches = [0.0] * len(timeScale)\n    harmonic_rates = [0.0] * len(timeScale)\n    argmins = [0.0] * len(timeScale)\n\n    for i, frame in enumerate(frames):\n        # Compute YIN\n        df = differenceFunction(frame, w_len, tau_max)\n        cmdf = cumulativeMeanNormalizedDifferenceFunction(df, tau_max)\n        p = getPitch(cmdf, tau_min, tau_max, harmo_thresh)\n\n        # Get results\n        if np.argmin(cmdf) &gt; tau_min:\n            argmins[i] = float(sr / np.argmin(cmdf))\n        if p != 0:  # A pitch was found\n            pitches[i] = float(sr / p)\n            harmonic_rates[i] = cmdf[p]\n        else:  # No pitch, but we compute a value of the harmonic rate\n            harmonic_rates[i] = min(cmdf)\n\n    return np.array(pitches), harmonic_rates, argmins, times\n</code></pre>"},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.cumulativeMeanNormalizedDifferenceFunction","title":"<code>cumulativeMeanNormalizedDifferenceFunction(df, N)</code>","text":"<p>Compute the cumulative mean normalized difference function (CMND) of a difference function.</p> <p>The CMND is defined as the element-wise product of the difference function with a range of values from 1 to N-1, divided by the cumulative sum of the difference function up to that point, plus a small epsilon value to avoid division by zero. The first element of the CMND is set to 1.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>ndarray</code> <p>The difference function.</p> required <code>N</code> <code>int</code> <p>The length of the data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The cumulative mean normalized difference function.</p> References <p>[1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"     Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.</p> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def cumulativeMeanNormalizedDifferenceFunction(df: np.ndarray, N: int) -&gt; np.ndarray:\n    r\"\"\"Compute the cumulative mean normalized difference function (CMND) of a difference function.\n\n    The CMND is defined as the element-wise product of the difference function with a range of values from 1 to N-1,\n    divided by the cumulative sum of the difference function up to that point, plus a small epsilon value to avoid\n    division by zero. The first element of the CMND is set to 1.\n\n    Args:\n        df (np.ndarray): The difference function.\n        N (int): The length of the data.\n\n    Returns:\n        np.ndarray: The cumulative mean normalized difference function.\n\n    References:\n        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n    \"\"\"\n    cmndf = (\n        df[1:] * range(1, N) / (np.cumsum(df[1:]).astype(float) + 1e-8)\n    )  # scipy method\n    return np.insert(cmndf, 0, 1)\n</code></pre>"},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.differenceFunction","title":"<code>differenceFunction(x, N, tau_max)</code>","text":"<p>Compute the difference function of an audio signal.</p> <p>This function computes the difference function of an audio signal <code>x</code> using the algorithm described in equation (6) of [1]. The difference function is a measure of the similarity between the signal and a time-shifted version of itself, and is commonly used in pitch detection algorithms.</p> <p>This implementation uses the NumPy FFT functions to compute the difference function efficiently.</p> <p>Parameters     x (np.ndarray): The audio signal to compute the difference function for.     N (int): The length of the audio signal.     tau_max (int): The maximum integration window size to use.</p> <p>Returns     np.ndarray: The difference function of the audio signal.</p> <p>References     [1] A. de Cheveign\u00e9 and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.</p> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def differenceFunction(x: np.ndarray, N: int, tau_max: int) -&gt; np.ndarray:\n    r\"\"\"Compute the difference function of an audio signal.\n\n    This function computes the difference function of an audio signal `x` using the algorithm described in equation (6) of [1]. The difference function is a measure of the similarity between the signal and a time-shifted version of itself, and is commonly used in pitch detection algorithms.\n\n    This implementation uses the NumPy FFT functions to compute the difference function efficiently.\n\n    Parameters\n        x (np.ndarray): The audio signal to compute the difference function for.\n        N (int): The length of the audio signal.\n        tau_max (int): The maximum integration window size to use.\n\n    Returns\n        np.ndarray: The difference function of the audio signal.\n\n    References\n        [1] A. de Cheveign\u00e9 and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n    \"\"\"\n    x = np.array(x, np.float64)\n    w = x.size\n    tau_max = min(tau_max, w)\n    x_cumsum = np.concatenate((np.array([0.0]), (x * x).cumsum()))\n    size = w + tau_max\n    p2 = (size // 32).bit_length()\n    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n    size_pad = min(x * 2**p2 for x in nice_numbers if x * 2**p2 &gt;= size)\n    fc = np.fft.rfft(x, size_pad)\n    conv = np.fft.irfft(fc * fc.conjugate())[:tau_max]\n    return x_cumsum[w : w - tau_max : -1] + x_cumsum[w] - x_cumsum[:tau_max] - 2 * conv\n</code></pre>"},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.getPitch","title":"<code>getPitch(cmdf, tau_min, tau_max, harmo_th=0.1)</code>","text":"<p>Compute the fundamental period of a frame based on the Cumulative Mean Normalized Difference function (CMND).</p> <p>The CMND is a measure of the periodicity of a signal, and is computed as the cumulative mean normalized difference function of the difference function of the signal. The fundamental period is the first value of the index <code>tau</code> between <code>tau_min</code> and <code>tau_max</code> where the CMND is below the <code>harmo_th</code> threshold. If there are no such values, the function returns 0 to indicate that the signal is unvoiced.</p> <p>Parameters:</p> Name Type Description Default <code>cmdf</code> <code>ndarray</code> <p>The Cumulative Mean Normalized Difference function of the signal.</p> required <code>tau_min</code> <code>int</code> <p>The minimum period for speech.</p> required <code>tau_max</code> <code>int</code> <p>The maximum period for speech.</p> required <code>harmo_th</code> <code>float</code> <p>The harmonicity threshold to determine if it is necessary to compute pitch frequency. Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The fundamental period of the signal, or 0 if the signal is unvoiced.</p> References <p>[1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"     Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.</p> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def getPitch(cmdf: np.ndarray, tau_min: int, tau_max: int, harmo_th: float=0.1) -&gt; int:\n    r\"\"\"Compute the fundamental period of a frame based on the Cumulative Mean Normalized Difference function (CMND).\n\n    The CMND is a measure of the periodicity of a signal, and is computed as the cumulative mean normalized difference\n    function of the difference function of the signal. The fundamental period is the first value of the index `tau`\n    between `tau_min` and `tau_max` where the CMND is below the `harmo_th` threshold. If there are no such values, the\n    function returns 0 to indicate that the signal is unvoiced.\n\n    Args:\n        cmdf (np.ndarray): The Cumulative Mean Normalized Difference function of the signal.\n        tau_min (int): The minimum period for speech.\n        tau_max (int): The maximum period for speech.\n        harmo_th (float, optional): The harmonicity threshold to determine if it is necessary to compute pitch\n            frequency. Defaults to 0.1.\n\n    Returns:\n        int: The fundamental period of the signal, or 0 if the signal is unvoiced.\n\n    References:\n        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n    \"\"\"\n    tau = tau_min\n    while tau &lt; tau_max:\n        if cmdf[tau] &lt; harmo_th:\n            while tau + 1 &lt; tau_max and cmdf[tau + 1] &lt; cmdf[tau]:\n                tau += 1\n            return tau\n        tau += 1\n\n    return 0  # if unvoiced\n</code></pre>"},{"location":"training/preprocess/compute_yin/#training.preprocess.compute_yin.norm_interp_f0","title":"<code>norm_interp_f0(f0)</code>","text":"<p>Normalize and interpolate the fundamental frequency (f0) values.</p> <p>Parameters:</p> Name Type Description Default <code>f0</code> <code>ndarray</code> <p>The input f0 values.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: A tuple containing the normalized f0 values and a boolean array indicating which values were interpolated.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; f0 = np.array([0, 100, 0, 200, 0])\n&gt;&gt;&gt; norm_interp_f0(f0)\n(\n    np.array([100, 100, 150, 200, 200]),\n    np.array([True, False, True, False, True]),\n)\n</code></pre> Source code in <code>training/preprocess/compute_yin.py</code> <pre><code>def norm_interp_f0(f0: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    r\"\"\"Normalize and interpolate the fundamental frequency (f0) values.\n\n    Args:\n        f0 (np.ndarray): The input f0 values.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing the normalized f0 values and a boolean array indicating which values were interpolated.\n\n    Examples:\n        &gt;&gt;&gt; f0 = np.array([0, 100, 0, 200, 0])\n        &gt;&gt;&gt; norm_interp_f0(f0)\n        (\n            np.array([100, 100, 150, 200, 200]),\n            np.array([True, False, True, False, True]),\n        )\n    \"\"\"\n    uv: np.ndarray = f0 == 0\n    if sum(uv) == len(f0):\n        f0[uv] = 0\n    elif sum(uv) &gt; 0:\n        f0[uv] = np.interp(np.where(uv)[0], np.where(~uv)[0], f0[~uv])\n    return f0, uv\n</code></pre>"},{"location":"training/preprocess/normalize_text/","title":"Normalize Text","text":""},{"location":"training/preprocess/normalize_text/#training.preprocess.normalize_text.NormalizeText","title":"<code>NormalizeText</code>","text":"<p>NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.</p> <p>This class normalize the characters in the input text and normalize the input text with the <code>nemo_text_processing</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language code to use for normalization. Defaults to \"en\".</p> <code>'en'</code> <p>Attributes:</p> Name Type Description <code>lang</code> <code>str</code> <p>The language code to use for normalization. Defaults to \"en\".</p> <code>model</code> <code>Normalizer</code> <p>The <code>nemo_text_processing</code> Normalizer model.</p> <p>Methods:</p> Name Description <code>byte_encode</code> <p>str) -&gt; list: Encode a word as a list of bytes.</p> <code>normalize_chars</code> <p>str) -&gt; str: Normalize the characters in the input text.</p> <code>__call__</code> <p>str) -&gt; str: Normalize the input text with the <code>nemo_text_processing</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from training.preprocess.normilize_text import NormalizeText\n&gt;&gt;&gt; normilize_text = NormalizeText()\n&gt;&gt;&gt; normilize_text(\"It\u2019s a beautiful day\u2026\")\n\"It's a beautiful day.\"\n</code></pre> Source code in <code>training/preprocess/normalize_text.py</code> <pre><code>class NormalizeText:\n    r\"\"\"NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.\n\n    This class normalize the characters in the input text and normalize the input text with the `nemo_text_processing`.\n\n    Args:\n        lang (str): The language code to use for normalization. Defaults to \"en\".\n\n    Attributes:\n        lang (str): The language code to use for normalization. Defaults to \"en\".\n        model (Normalizer): The `nemo_text_processing` Normalizer model.\n\n    Methods:\n        byte_encode(word: str) -&gt; list: Encode a word as a list of bytes.\n        normalize_chars(text: str) -&gt; str: Normalize the characters in the input text.\n        __call__(text: str) -&gt; str: Normalize the input text with the `nemo_text_processing`.\n\n    Examples:\n        &gt;&gt;&gt; from training.preprocess.normilize_text import NormalizeText\n        &gt;&gt;&gt; normilize_text = NormalizeText()\n        &gt;&gt;&gt; normilize_text(\"It\u2019s a beautiful day\u2026\")\n        \"It's a beautiful day.\"\n    \"\"\"\n\n    def __init__(self, lang: str = \"en\"):\n        r\"\"\"Initialize a new instance of the NormalizeText class.\n\n        Args:\n            lang (str): The language code to use for normalization. Defaults to \"en\".\n\n        \"\"\"\n        self.lang = lang\n\n        self.model = Normalizer(input_case=\"cased\", lang=lang)\n\n    def byte_encode(self, word: str) -&gt; list:\n        r\"\"\"Encode a word as a list of bytes.\n\n        Args:\n            word (str): The word to encode.\n\n        Returns:\n            list: A list of bytes representing the encoded word.\n\n        \"\"\"\n        text = word.strip()\n        return list(text.encode(\"utf-8\"))\n\n    def normalize_chars(self, text: str) -&gt; str:\n        r\"\"\"Normalize the characters in the input text.\n\n        Args:\n            text (str): The input text to normalize.\n\n        Returns:\n            str: The normalized text.\n\n        Examples:\n            &gt;&gt;&gt; normalize_chars(\"It\u2019s a beautiful day\u2026\")\n            \"It's a beautiful day.\"\n\n        \"\"\"\n        # Define the character mapping\n        char_mapping = {\n            ord(\"\u2019\"): ord(\"'\"),\n            ord(\"\u201d\"): ord(\"'\"),\n            ord(\"\u2026\"): ord(\".\"),\n            ord(\"\u201e\"): ord(\"'\"),\n            ord(\"\u201c\"): ord(\"'\"),\n            ord('\"'): ord(\"'\"),\n            ord(\"\u2013\"): ord(\"-\"),\n            ord(\"\u2014\"): ord(\"-\"),\n            ord(\"\u00ab\"): ord(\"'\"),\n            ord(\"\u00bb\"): ord(\"'\"),\n        }\n\n        # Add unicode normalization as additional garanty and normalize the characters using translate() method\n        normalized_string = unidecode(text).translate(char_mapping)\n\n        # Remove redundant multiple characters\n        # TODO: Maybe there is some effect on duplication?\n        return re.sub(r\"(\\.|\\!|\\?|\\-)\\1+\", r\"\\1\", normalized_string)\n\n    def __call__(self, text: str) -&gt; str:\n        r\"\"\"Normalize the input text with the `nemo_text_processing`.\n\n        Args:\n            text (str): The input text to normalize.\n\n        Returns:\n            str: The normalized text.\n\n        \"\"\"\n        text = self.normalize_chars(text)\n        # return self.model.normalize(text)\n\n        # Split the text into lines\n        lines = text.split(\"\\n\")\n        normalized_lines = self.model.normalize_list(lines)\n\n        # TODO: check this!\n        # Join the normalized lines, replace \\n with . and return the result\n        result = \". \".join(normalized_lines)\n        return result\n</code></pre>"},{"location":"training/preprocess/normalize_text/#training.preprocess.normalize_text.NormalizeText.__call__","title":"<code>__call__(text)</code>","text":"<p>Normalize the input text with the <code>nemo_text_processing</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> Source code in <code>training/preprocess/normalize_text.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    r\"\"\"Normalize the input text with the `nemo_text_processing`.\n\n    Args:\n        text (str): The input text to normalize.\n\n    Returns:\n        str: The normalized text.\n\n    \"\"\"\n    text = self.normalize_chars(text)\n    # return self.model.normalize(text)\n\n    # Split the text into lines\n    lines = text.split(\"\\n\")\n    normalized_lines = self.model.normalize_list(lines)\n\n    # TODO: check this!\n    # Join the normalized lines, replace \\n with . and return the result\n    result = \". \".join(normalized_lines)\n    return result\n</code></pre>"},{"location":"training/preprocess/normalize_text/#training.preprocess.normalize_text.NormalizeText.__init__","title":"<code>__init__(lang='en')</code>","text":"<p>Initialize a new instance of the NormalizeText class.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language code to use for normalization. Defaults to \"en\".</p> <code>'en'</code> Source code in <code>training/preprocess/normalize_text.py</code> <pre><code>def __init__(self, lang: str = \"en\"):\n    r\"\"\"Initialize a new instance of the NormalizeText class.\n\n    Args:\n        lang (str): The language code to use for normalization. Defaults to \"en\".\n\n    \"\"\"\n    self.lang = lang\n\n    self.model = Normalizer(input_case=\"cased\", lang=lang)\n</code></pre>"},{"location":"training/preprocess/normalize_text/#training.preprocess.normalize_text.NormalizeText.byte_encode","title":"<code>byte_encode(word)</code>","text":"<p>Encode a word as a list of bytes.</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The word to encode.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of bytes representing the encoded word.</p> Source code in <code>training/preprocess/normalize_text.py</code> <pre><code>def byte_encode(self, word: str) -&gt; list:\n    r\"\"\"Encode a word as a list of bytes.\n\n    Args:\n        word (str): The word to encode.\n\n    Returns:\n        list: A list of bytes representing the encoded word.\n\n    \"\"\"\n    text = word.strip()\n    return list(text.encode(\"utf-8\"))\n</code></pre>"},{"location":"training/preprocess/normalize_text/#training.preprocess.normalize_text.NormalizeText.normalize_chars","title":"<code>normalize_chars(text)</code>","text":"<p>Normalize the characters in the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalize_chars(\"It\u2019s a beautiful day\u2026\")\n\"It's a beautiful day.\"\n</code></pre> Source code in <code>training/preprocess/normalize_text.py</code> <pre><code>def normalize_chars(self, text: str) -&gt; str:\n    r\"\"\"Normalize the characters in the input text.\n\n    Args:\n        text (str): The input text to normalize.\n\n    Returns:\n        str: The normalized text.\n\n    Examples:\n        &gt;&gt;&gt; normalize_chars(\"It\u2019s a beautiful day\u2026\")\n        \"It's a beautiful day.\"\n\n    \"\"\"\n    # Define the character mapping\n    char_mapping = {\n        ord(\"\u2019\"): ord(\"'\"),\n        ord(\"\u201d\"): ord(\"'\"),\n        ord(\"\u2026\"): ord(\".\"),\n        ord(\"\u201e\"): ord(\"'\"),\n        ord(\"\u201c\"): ord(\"'\"),\n        ord('\"'): ord(\"'\"),\n        ord(\"\u2013\"): ord(\"-\"),\n        ord(\"\u2014\"): ord(\"-\"),\n        ord(\"\u00ab\"): ord(\"'\"),\n        ord(\"\u00bb\"): ord(\"'\"),\n    }\n\n    # Add unicode normalization as additional garanty and normalize the characters using translate() method\n    normalized_string = unidecode(text).translate(char_mapping)\n\n    # Remove redundant multiple characters\n    # TODO: Maybe there is some effect on duplication?\n    return re.sub(r\"(\\.|\\!|\\?|\\-)\\1+\", r\"\\1\", normalized_string)\n</code></pre>"},{"location":"training/preprocess/preprocess_libritts/","title":"Preprocess LibriTTS","text":""},{"location":"training/preprocess/preprocess_libritts/#training.preprocess.preprocess_libritts.PreprocessLibriTTS","title":"<code>PreprocessLibriTTS</code>","text":"<p>Preprocessing PreprocessLibriTTS audio and text data for use with a TacotronSTFT model.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_config</code> <code>PreprocessingConfig</code> <p>The preprocessing configuration.</p> required <code>lang</code> <code>str</code> <p>The language of the input text.</p> <code>'en'</code> <p>Attributes:</p> Name Type Description <code>min_seconds</code> <code>float</code> <p>The minimum duration of audio clips in seconds.</p> <code>max_seconds</code> <code>float</code> <p>The maximum duration of audio clips in seconds.</p> <code>hop_length</code> <code>int</code> <p>The hop length of the STFT.</p> <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the audio.</p> <code>use_audio_normalization</code> <code>bool</code> <p>Whether to normalize the loudness of the audio.</p> <code>tacotronSTFT</code> <code>TacotronSTFT</code> <p>The TacotronSTFT object used for computing mel spectrograms.</p> <code>min_samples</code> <code>int</code> <p>The minimum number of audio samples in a clip.</p> <code>max_samples</code> <code>int</code> <p>The maximum number of audio samples in a clip.</p> Source code in <code>training/preprocess/preprocess_libritts.py</code> <pre><code>class PreprocessLibriTTS:\n    r\"\"\"Preprocessing PreprocessLibriTTS audio and text data for use with a TacotronSTFT model.\n\n    Args:\n        preprocess_config (PreprocessingConfig): The preprocessing configuration.\n        lang (str): The language of the input text.\n\n    Attributes:\n        min_seconds (float): The minimum duration of audio clips in seconds.\n        max_seconds (float): The maximum duration of audio clips in seconds.\n        hop_length (int): The hop length of the STFT.\n        sampling_rate (int): The sampling rate of the audio.\n        use_audio_normalization (bool): Whether to normalize the loudness of the audio.\n        tacotronSTFT (TacotronSTFT): The TacotronSTFT object used for computing mel spectrograms.\n        min_samples (int): The minimum number of audio samples in a clip.\n        max_samples (int): The maximum number of audio samples in a clip.\n    \"\"\"\n\n    def __init__(\n        self,\n        preprocess_config: PreprocessingConfig,\n        lang: str = \"en\",\n    ):\n        super().__init__()\n\n        lang_map = get_lang_map(lang)\n\n        self.phonemizer_lang = lang_map.phonemizer\n        normilize_text_lang = lang_map.nemo\n\n        self.normilize_text = NormalizeText(normilize_text_lang)\n        self.tokenizer = TokenizerIPA(lang)\n        self.vocoder_train_config = VocoderBasicConfig()\n\n        self.preprocess_config = preprocess_config\n\n        self.sampling_rate = self.preprocess_config.sampling_rate\n        self.use_audio_normalization = self.preprocess_config.use_audio_normalization\n\n        self.hop_length = self.preprocess_config.stft.hop_length\n        self.filter_length = self.preprocess_config.stft.filter_length\n        self.mel_fmin = self.preprocess_config.stft.mel_fmin\n        self.win_length = self.preprocess_config.stft.win_length\n\n        self.tacotronSTFT = TacotronSTFT(\n            filter_length=self.filter_length,\n            hop_length=self.hop_length,\n            win_length=self.preprocess_config.stft.win_length,\n            n_mel_channels=self.preprocess_config.stft.n_mel_channels,\n            sampling_rate=self.sampling_rate,\n            mel_fmin=self.mel_fmin,\n            mel_fmax=self.preprocess_config.stft.mel_fmax,\n            center=False,\n        )\n\n        min_seconds, max_seconds = (\n            self.preprocess_config.min_seconds,\n            self.preprocess_config.max_seconds,\n        )\n\n        self.min_samples = int(self.sampling_rate * min_seconds)\n        self.max_samples = int(self.sampling_rate * max_seconds)\n\n        self.audio_processor = AudioProcessor()\n\n    def beta_binomial_prior_distribution(\n        self,\n        phoneme_count: int,\n        mel_count: int,\n        scaling_factor: float = 1.0,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Computes the beta-binomial prior distribution for the attention mechanism.\n\n        Args:\n            phoneme_count (int): Number of phonemes in the input text.\n            mel_count (int): Number of mel frames in the input mel-spectrogram.\n            scaling_factor (float, optional): Scaling factor for the beta distribution. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor: A 2D tensor containing the prior distribution.\n        \"\"\"\n        P, M = phoneme_count, mel_count\n        x = np.arange(0, P)\n        mel_text_probs = []\n        for i in range(1, M + 1):\n            a, b = scaling_factor * i, scaling_factor * (M + 1 - i)\n            rv: Any = betabinom(P, a, b)\n            mel_i_prob = rv.pmf(x)\n            mel_text_probs.append(mel_i_prob)\n        return torch.from_numpy(np.array(mel_text_probs))\n\n    def acoustic(\n        self,\n        row: Tuple[torch.Tensor, int, str, str, int, str | int, str],\n    ) -&gt; Union[None, PreprocessForAcousticResult]:\n        r\"\"\"Preprocesses audio and text data for use with a TacotronSTFT model.\n\n        Args:\n            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n\n        Returns:\n            dict: A dictionary containing the preprocessed audio and text data.\n\n        Examples:\n            &gt;&gt;&gt; preprocess_audio = PreprocessAudio(\"english_only\")\n            &gt;&gt;&gt; audio = torch.randn(1, 44100)\n            &gt;&gt;&gt; sr_actual = 44100\n            &gt;&gt;&gt; raw_text = \"Hello, world!\"\n            &gt;&gt;&gt; output = preprocess_audio(audio, sr_actual, raw_text)\n            &gt;&gt;&gt; output.keys()\n            dict_keys(['wav', 'mel', 'pitch', 'phones', 'raw_text', 'normalized_text', 'speaker_id', 'chapter_id', 'utterance_id', 'pitch_is_normalized'])\n        \"\"\"\n        (\n            audio,\n            sr_actual,\n            raw_text,\n            normalized_text,\n            speaker_id,\n            chapter_id,\n            utterance_id,\n        ) = row\n\n        wav, sampling_rate = preprocess_audio(audio, sr_actual, self.sampling_rate)\n\n        # TODO: check this, maybe you need to move it to some other place\n        # TODO: maybe we can increate the max_samples ?\n        # if wav.shape[0] &lt; self.min_samples or wav.shape[0] &gt; self.max_samples:\n        #     return None\n\n        if self.use_audio_normalization:\n            wav = normalize_loudness(wav)\n\n        normalized_text = self.normilize_text(normalized_text)\n\n        # NOTE: fixed version of tokenizer with punctuation\n        phones_ipa, phones = self.tokenizer(normalized_text)\n\n        # Convert to tensor\n        phones = torch.Tensor(phones)\n\n        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n\n        # Skipping small sample due to the mel-spectrogram containing less than self.mel_fmin frames\n        # if mel_spectrogram.shape[1] &lt; self.mel_fmin:\n        #     return None\n\n        # Text is longer than mel, will be skipped due to monotonic alignment search\n        if phones.shape[0] &gt;= mel_spectrogram.shape[1]:\n            return None\n\n        pitch, _, _, _ = compute_yin(\n            wav,\n            sr=sampling_rate,\n            w_len=self.filter_length,\n            w_step=self.hop_length,\n            f0_min=50,\n            f0_max=1000,\n            harmo_thresh=0.25,\n        )\n\n        pitch, _ = norm_interp_f0(pitch)\n\n        if np.sum(pitch != 0) &lt;= 1:\n            return None\n\n        pitch = torch.from_numpy(pitch)\n\n        # TODO this shouldnt be necessary, currently pitch sometimes has 1 less frame than spectrogram,\n        # We should find out why\n        mel_spectrogram = mel_spectrogram[:, : pitch.shape[0]]\n\n        attn_prior = self.beta_binomial_prior_distribution(\n            phones.shape[0],\n            mel_spectrogram.shape[1],\n        ).T\n\n        assert pitch.shape[0] == mel_spectrogram.shape[1], (\n            pitch.shape,\n            mel_spectrogram.shape[1],\n        )\n\n        energy = self.audio_processor.wav_to_energy(\n            wav.unsqueeze(0),\n            self.filter_length,\n            self.hop_length,\n            self.win_length,\n        )\n\n        return PreprocessForAcousticResult(\n            wav=wav,\n            mel=mel_spectrogram,\n            pitch=pitch,\n            attn_prior=attn_prior,\n            energy=energy,\n            phones_ipa=phones_ipa,\n            phones=phones,\n            raw_text=raw_text,\n            normalized_text=normalized_text,\n            speaker_id=speaker_id,\n            chapter_id=chapter_id,\n            utterance_id=utterance_id,\n            # TODO: check the pitch normalization process\n            pitch_is_normalized=False,\n        )\n\n    def univnet(self, row: Tuple[torch.Tensor, int, str, str, int, str | int, str]):\n        r\"\"\"Preprocesses audio data for use with a UnivNet model.\n\n        This method takes a row of data, extracts the audio and preprocesses it.\n        It then selects a random segment from the preprocessed audio and its corresponding mel spectrogram.\n\n        Args:\n            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, int]: A tuple containing the selected segment of the mel spectrogram, the corresponding audio segment, and the speaker ID.\n\n        Examples:\n            &gt;&gt;&gt; preprocess = PreprocessLibriTTS()\n            &gt;&gt;&gt; audio = torch.randn(1, 44100)\n            &gt;&gt;&gt; sr_actual = 44100\n            &gt;&gt;&gt; speaker_id = 0\n            &gt;&gt;&gt; mel, audio_segment, speaker_id = preprocess.preprocess_univnet((audio, sr_actual, \"\", \"\", speaker_id, 0, \"\"))\n        \"\"\"\n        (\n            audio,\n            sr_actual,\n            _,\n            _,\n            speaker_id,\n            _,\n            _,\n        ) = row\n\n        segment_size = self.vocoder_train_config.segment_size\n        frames_per_seg = math.ceil(segment_size / self.hop_length)\n\n        wav, _ = preprocess_audio(audio, sr_actual, self.sampling_rate)\n\n        if self.use_audio_normalization:\n            wav = normalize_loudness(wav)\n\n        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n\n        if wav.shape[0] &lt; segment_size:\n            wav = F.pad(\n                wav,\n                (0, segment_size - wav.shape[0]),\n                \"constant\",\n            )\n\n        if mel_spectrogram.shape[1] &lt; frames_per_seg:\n            mel_spectrogram = F.pad(\n                mel_spectrogram,\n                (0, frames_per_seg - mel_spectrogram.shape[1]),\n                \"constant\",\n            )\n\n        from_frame = random.randint(0, mel_spectrogram.shape[1] - frames_per_seg)\n\n        # Skip last frame, otherwise errors are thrown, find out why\n        if from_frame &gt; 0:\n            from_frame -= 1\n\n        till_frame = from_frame + frames_per_seg\n\n        mel_spectrogram = mel_spectrogram[:, from_frame:till_frame]\n        wav = wav[from_frame * self.hop_length : till_frame * self.hop_length]\n\n        return mel_spectrogram, wav, speaker_id\n</code></pre>"},{"location":"training/preprocess/preprocess_libritts/#training.preprocess.preprocess_libritts.PreprocessLibriTTS.acoustic","title":"<code>acoustic(row)</code>","text":"<p>Preprocesses audio and text data for use with a TacotronSTFT model.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Tuple[FloatTensor, int, str, str, int, str | int, str]</code> <p>The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[None, PreprocessForAcousticResult]</code> <p>A dictionary containing the preprocessed audio and text data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; preprocess_audio = PreprocessAudio(\"english_only\")\n&gt;&gt;&gt; audio = torch.randn(1, 44100)\n&gt;&gt;&gt; sr_actual = 44100\n&gt;&gt;&gt; raw_text = \"Hello, world!\"\n&gt;&gt;&gt; output = preprocess_audio(audio, sr_actual, raw_text)\n&gt;&gt;&gt; output.keys()\ndict_keys(['wav', 'mel', 'pitch', 'phones', 'raw_text', 'normalized_text', 'speaker_id', 'chapter_id', 'utterance_id', 'pitch_is_normalized'])\n</code></pre> Source code in <code>training/preprocess/preprocess_libritts.py</code> <pre><code>def acoustic(\n    self,\n    row: Tuple[torch.Tensor, int, str, str, int, str | int, str],\n) -&gt; Union[None, PreprocessForAcousticResult]:\n    r\"\"\"Preprocesses audio and text data for use with a TacotronSTFT model.\n\n    Args:\n        row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n\n    Returns:\n        dict: A dictionary containing the preprocessed audio and text data.\n\n    Examples:\n        &gt;&gt;&gt; preprocess_audio = PreprocessAudio(\"english_only\")\n        &gt;&gt;&gt; audio = torch.randn(1, 44100)\n        &gt;&gt;&gt; sr_actual = 44100\n        &gt;&gt;&gt; raw_text = \"Hello, world!\"\n        &gt;&gt;&gt; output = preprocess_audio(audio, sr_actual, raw_text)\n        &gt;&gt;&gt; output.keys()\n        dict_keys(['wav', 'mel', 'pitch', 'phones', 'raw_text', 'normalized_text', 'speaker_id', 'chapter_id', 'utterance_id', 'pitch_is_normalized'])\n    \"\"\"\n    (\n        audio,\n        sr_actual,\n        raw_text,\n        normalized_text,\n        speaker_id,\n        chapter_id,\n        utterance_id,\n    ) = row\n\n    wav, sampling_rate = preprocess_audio(audio, sr_actual, self.sampling_rate)\n\n    # TODO: check this, maybe you need to move it to some other place\n    # TODO: maybe we can increate the max_samples ?\n    # if wav.shape[0] &lt; self.min_samples or wav.shape[0] &gt; self.max_samples:\n    #     return None\n\n    if self.use_audio_normalization:\n        wav = normalize_loudness(wav)\n\n    normalized_text = self.normilize_text(normalized_text)\n\n    # NOTE: fixed version of tokenizer with punctuation\n    phones_ipa, phones = self.tokenizer(normalized_text)\n\n    # Convert to tensor\n    phones = torch.Tensor(phones)\n\n    mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n\n    # Skipping small sample due to the mel-spectrogram containing less than self.mel_fmin frames\n    # if mel_spectrogram.shape[1] &lt; self.mel_fmin:\n    #     return None\n\n    # Text is longer than mel, will be skipped due to monotonic alignment search\n    if phones.shape[0] &gt;= mel_spectrogram.shape[1]:\n        return None\n\n    pitch, _, _, _ = compute_yin(\n        wav,\n        sr=sampling_rate,\n        w_len=self.filter_length,\n        w_step=self.hop_length,\n        f0_min=50,\n        f0_max=1000,\n        harmo_thresh=0.25,\n    )\n\n    pitch, _ = norm_interp_f0(pitch)\n\n    if np.sum(pitch != 0) &lt;= 1:\n        return None\n\n    pitch = torch.from_numpy(pitch)\n\n    # TODO this shouldnt be necessary, currently pitch sometimes has 1 less frame than spectrogram,\n    # We should find out why\n    mel_spectrogram = mel_spectrogram[:, : pitch.shape[0]]\n\n    attn_prior = self.beta_binomial_prior_distribution(\n        phones.shape[0],\n        mel_spectrogram.shape[1],\n    ).T\n\n    assert pitch.shape[0] == mel_spectrogram.shape[1], (\n        pitch.shape,\n        mel_spectrogram.shape[1],\n    )\n\n    energy = self.audio_processor.wav_to_energy(\n        wav.unsqueeze(0),\n        self.filter_length,\n        self.hop_length,\n        self.win_length,\n    )\n\n    return PreprocessForAcousticResult(\n        wav=wav,\n        mel=mel_spectrogram,\n        pitch=pitch,\n        attn_prior=attn_prior,\n        energy=energy,\n        phones_ipa=phones_ipa,\n        phones=phones,\n        raw_text=raw_text,\n        normalized_text=normalized_text,\n        speaker_id=speaker_id,\n        chapter_id=chapter_id,\n        utterance_id=utterance_id,\n        # TODO: check the pitch normalization process\n        pitch_is_normalized=False,\n    )\n</code></pre>"},{"location":"training/preprocess/preprocess_libritts/#training.preprocess.preprocess_libritts.PreprocessLibriTTS.beta_binomial_prior_distribution","title":"<code>beta_binomial_prior_distribution(phoneme_count, mel_count, scaling_factor=1.0)</code>","text":"<p>Computes the beta-binomial prior distribution for the attention mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>phoneme_count</code> <code>int</code> <p>Number of phonemes in the input text.</p> required <code>mel_count</code> <code>int</code> <p>Number of mel frames in the input mel-spectrogram.</p> required <code>scaling_factor</code> <code>float</code> <p>Scaling factor for the beta distribution. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor containing the prior distribution.</p> Source code in <code>training/preprocess/preprocess_libritts.py</code> <pre><code>def beta_binomial_prior_distribution(\n    self,\n    phoneme_count: int,\n    mel_count: int,\n    scaling_factor: float = 1.0,\n) -&gt; torch.Tensor:\n    r\"\"\"Computes the beta-binomial prior distribution for the attention mechanism.\n\n    Args:\n        phoneme_count (int): Number of phonemes in the input text.\n        mel_count (int): Number of mel frames in the input mel-spectrogram.\n        scaling_factor (float, optional): Scaling factor for the beta distribution. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: A 2D tensor containing the prior distribution.\n    \"\"\"\n    P, M = phoneme_count, mel_count\n    x = np.arange(0, P)\n    mel_text_probs = []\n    for i in range(1, M + 1):\n        a, b = scaling_factor * i, scaling_factor * (M + 1 - i)\n        rv: Any = betabinom(P, a, b)\n        mel_i_prob = rv.pmf(x)\n        mel_text_probs.append(mel_i_prob)\n    return torch.from_numpy(np.array(mel_text_probs))\n</code></pre>"},{"location":"training/preprocess/preprocess_libritts/#training.preprocess.preprocess_libritts.PreprocessLibriTTS.univnet","title":"<code>univnet(row)</code>","text":"<p>Preprocesses audio data for use with a UnivNet model.</p> <p>This method takes a row of data, extracts the audio and preprocesses it. It then selects a random segment from the preprocessed audio and its corresponding mel spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Tuple[FloatTensor, int, str, str, int, str | int, str]</code> <p>The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).</p> required <p>Returns:</p> Type Description <p>Tuple[torch.Tensor, torch.Tensor, int]: A tuple containing the selected segment of the mel spectrogram, the corresponding audio segment, and the speaker ID.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; preprocess = PreprocessLibriTTS()\n&gt;&gt;&gt; audio = torch.randn(1, 44100)\n&gt;&gt;&gt; sr_actual = 44100\n&gt;&gt;&gt; speaker_id = 0\n&gt;&gt;&gt; mel, audio_segment, speaker_id = preprocess.preprocess_univnet((audio, sr_actual, \"\", \"\", speaker_id, 0, \"\"))\n</code></pre> Source code in <code>training/preprocess/preprocess_libritts.py</code> <pre><code>def univnet(self, row: Tuple[torch.Tensor, int, str, str, int, str | int, str]):\n    r\"\"\"Preprocesses audio data for use with a UnivNet model.\n\n    This method takes a row of data, extracts the audio and preprocesses it.\n    It then selects a random segment from the preprocessed audio and its corresponding mel spectrogram.\n\n    Args:\n        row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, int]: A tuple containing the selected segment of the mel spectrogram, the corresponding audio segment, and the speaker ID.\n\n    Examples:\n        &gt;&gt;&gt; preprocess = PreprocessLibriTTS()\n        &gt;&gt;&gt; audio = torch.randn(1, 44100)\n        &gt;&gt;&gt; sr_actual = 44100\n        &gt;&gt;&gt; speaker_id = 0\n        &gt;&gt;&gt; mel, audio_segment, speaker_id = preprocess.preprocess_univnet((audio, sr_actual, \"\", \"\", speaker_id, 0, \"\"))\n    \"\"\"\n    (\n        audio,\n        sr_actual,\n        _,\n        _,\n        speaker_id,\n        _,\n        _,\n    ) = row\n\n    segment_size = self.vocoder_train_config.segment_size\n    frames_per_seg = math.ceil(segment_size / self.hop_length)\n\n    wav, _ = preprocess_audio(audio, sr_actual, self.sampling_rate)\n\n    if self.use_audio_normalization:\n        wav = normalize_loudness(wav)\n\n    mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n\n    if wav.shape[0] &lt; segment_size:\n        wav = F.pad(\n            wav,\n            (0, segment_size - wav.shape[0]),\n            \"constant\",\n        )\n\n    if mel_spectrogram.shape[1] &lt; frames_per_seg:\n        mel_spectrogram = F.pad(\n            mel_spectrogram,\n            (0, frames_per_seg - mel_spectrogram.shape[1]),\n            \"constant\",\n        )\n\n    from_frame = random.randint(0, mel_spectrogram.shape[1] - frames_per_seg)\n\n    # Skip last frame, otherwise errors are thrown, find out why\n    if from_frame &gt; 0:\n        from_frame -= 1\n\n    till_frame = from_frame + frames_per_seg\n\n    mel_spectrogram = mel_spectrogram[:, from_frame:till_frame]\n    wav = wav[from_frame * self.hop_length : till_frame * self.hop_length]\n\n    return mel_spectrogram, wav, speaker_id\n</code></pre>"},{"location":"training/preprocess/readme/","title":"References","text":""},{"location":"training/preprocess/readme/#references","title":"References","text":"<p>Here you can find preprocessing code.</p>"},{"location":"training/preprocess/readme/#audio","title":"Audio","text":"<p>Audio processing functions.</p>"},{"location":"training/preprocess/readme/#audio-processor","title":"Audio processor","text":"<p>A class used to process audio signals and convert them into different representations.</p>"},{"location":"training/preprocess/readme/#compute_yin","title":"Compute_YIN","text":"<p>Original implementation</p>"},{"location":"training/preprocess/readme/#normalize-text","title":"Normalize Text","text":"<p>This class normalize the characters in the input text and normalize the input text with the <code>nemo_text_processing</code>.</p>"},{"location":"training/preprocess/readme/#preprocess-libritts","title":"Preprocess LibriTTS","text":"<p>Preprocessing <code>PreprocessLibriTTS</code> audio and text data for use with a <code>TacotronSTFT</code> model.</p>"},{"location":"training/preprocess/readme/#tacotronstft","title":"TacotronSTFT","text":"<p><code>TacotronSTFT</code> module that computes mel-spectrograms from a batch of waves.</p>"},{"location":"training/preprocess/readme/#wav2vec-aligner","title":"wav2vec aligner","text":"<p>The Wav2VecAligner model is designed for aligning audio data with text data. This class handles the training and validation of the Wav2VecAligner model.</p>"},{"location":"training/preprocess/readme/#tokenizeripa","title":"TokenizerIPA","text":"<p>The tokenizer of IPA tokens with punctuation</p>"},{"location":"training/preprocess/tacotron_stft/","title":"TacotronSTFT","text":""},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT","title":"<code>TacotronSTFT</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>class TacotronSTFT(Module):\n    def __init__(\n        self,\n        filter_length: int,\n        hop_length: int,\n        win_length: int,\n        n_mel_channels: int,\n        sampling_rate: int,\n        center: bool,\n        mel_fmax: Optional[int],\n        mel_fmin: float = 0.0,\n    ):\n        r\"\"\"TacotronSTFT module that computes mel-spectrograms from a batch of waves.\n\n        Args:\n            filter_length (int): Length of the filter window.\n            hop_length (int): Number of samples between successive frames.\n            win_length (int): Size of the STFT window.\n            n_mel_channels (int): Number of mel bins.\n            sampling_rate (int): Sampling rate of the input waveforms.\n            mel_fmin (int or None): Minimum frequency for the mel filter bank.\n            mel_fmax (int or None): Maximum frequency for the mel filter bank.\n            center (bool): Whether to pad the input signal on both sides.\n        \"\"\"\n        super().__init__()\n\n        self.n_mel_channels = n_mel_channels\n        self.sampling_rate = sampling_rate\n        self.n_fft = filter_length\n        self.hop_size = hop_length\n        self.win_size = win_length\n        self.fmin = mel_fmin\n        self.fmax = mel_fmax\n        self.center = center\n\n        # Define the mel filterbank\n        mel = librosa.filters.mel(\n            sr=sampling_rate,\n            n_fft=filter_length,\n            n_mels=n_mel_channels,\n            fmin=mel_fmin,\n            fmax=mel_fmax,\n        )\n\n        mel_basis = torch.from_numpy(mel).float()\n\n        # Define the Hann window\n        hann_window = torch.hann_window(win_length)\n\n        self.register_buffer(\"mel_basis\", mel_basis)\n        self.register_buffer(\"hann_window\", hann_window)\n\n    def _spectrogram(self, y: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the linear spectrogram of a batch of waves.\n\n        Args:\n            y (torch.Tensor): Input waveforms.\n\n        Returns:\n            torch.Tensor: Linear spectrogram.\n        \"\"\"\n        assert torch.min(y.data) &gt;= -1\n        assert torch.max(y.data) &lt;= 1\n\n        y = torch.nn.functional.pad(\n            y.unsqueeze(1),\n            (\n                int((self.n_fft - self.hop_size) / 2),\n                int((self.n_fft - self.hop_size) / 2),\n            ),\n            mode=\"reflect\",\n        )\n        y = y.squeeze(1)\n        spec = torch.stft(\n            y,\n            self.n_fft,\n            hop_length=self.hop_size,\n            win_length=self.win_size,\n            window=self.hann_window,  # type: ignore\n            center=self.center,\n            pad_mode=\"reflect\",\n            normalized=False,\n            onesided=True,\n            return_complex=True,\n        )\n        return torch.view_as_real(spec)\n\n    def linear_spectrogram(self, y: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Computes the linear spectrogram of a batch of waves.\n\n        Args:\n            y (torch.Tensor): Input waveforms.\n\n        Returns:\n            torch.Tensor: Linear spectrogram.\n        \"\"\"\n        spec = self._spectrogram(y)\n        return torch.norm(spec, p=2, dim=-1)\n\n    def forward(self, y: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"Computes mel-spectrograms from a batch of waves.\n\n        Args:\n            y (torch.FloatTensor): Input waveforms with shape (B, T) in range [-1, 1]\n\n        Returns:\n            torch.FloatTensor: Spectrogram of shape (B, n_spech_channels, T)\n            torch.FloatTensor: Mel-spectrogram of shape (B, n_mel_channels, T)\n        \"\"\"\n        spec = self._spectrogram(y)\n\n        spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n\n        mel = torch.matmul(self.mel_basis, spec)  # type: ignore\n        mel = self.spectral_normalize_torch(mel)\n\n        return spec, mel\n\n    def spectral_normalize_torch(self, magnitudes: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"Applies dynamic range compression to magnitudes.\n\n        Args:\n            magnitudes (torch.Tensor): Input magnitudes.\n\n        Returns:\n            torch.Tensor: Output magnitudes.\n        \"\"\"\n        return self.dynamic_range_compression_torch(magnitudes)\n\n    def dynamic_range_compression_torch(\n        self,\n        x: torch.Tensor,\n        C: int = 1,\n        clip_val: float = 1e-5,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Applies dynamic range compression to x.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            C (float): Compression factor.\n            clip_val (float): Clipping value.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        return torch.log(torch.clamp(x, min=clip_val) * C)\n\n    # NOTE: audio np.ndarray changed to torch.FloatTensor!\n    def get_mel_from_wav(self, audio: torch.Tensor) -&gt; torch.Tensor:\n        audio_tensor = audio.unsqueeze(0)\n        with torch.no_grad():\n            _, melspec = self.forward(audio_tensor)\n        return melspec.squeeze(0)\n</code></pre>"},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT.__init__","title":"<code>__init__(filter_length, hop_length, win_length, n_mel_channels, sampling_rate, center, mel_fmax, mel_fmin=0.0)</code>","text":"<p>TacotronSTFT module that computes mel-spectrograms from a batch of waves.</p> <p>Parameters:</p> Name Type Description Default <code>filter_length</code> <code>int</code> <p>Length of the filter window.</p> required <code>hop_length</code> <code>int</code> <p>Number of samples between successive frames.</p> required <code>win_length</code> <code>int</code> <p>Size of the STFT window.</p> required <code>n_mel_channels</code> <code>int</code> <p>Number of mel bins.</p> required <code>sampling_rate</code> <code>int</code> <p>Sampling rate of the input waveforms.</p> required <code>mel_fmin</code> <code>int or None</code> <p>Minimum frequency for the mel filter bank.</p> <code>0.0</code> <code>mel_fmax</code> <code>int or None</code> <p>Maximum frequency for the mel filter bank.</p> required <code>center</code> <code>bool</code> <p>Whether to pad the input signal on both sides.</p> required Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>def __init__(\n    self,\n    filter_length: int,\n    hop_length: int,\n    win_length: int,\n    n_mel_channels: int,\n    sampling_rate: int,\n    center: bool,\n    mel_fmax: Optional[int],\n    mel_fmin: float = 0.0,\n):\n    r\"\"\"TacotronSTFT module that computes mel-spectrograms from a batch of waves.\n\n    Args:\n        filter_length (int): Length of the filter window.\n        hop_length (int): Number of samples between successive frames.\n        win_length (int): Size of the STFT window.\n        n_mel_channels (int): Number of mel bins.\n        sampling_rate (int): Sampling rate of the input waveforms.\n        mel_fmin (int or None): Minimum frequency for the mel filter bank.\n        mel_fmax (int or None): Maximum frequency for the mel filter bank.\n        center (bool): Whether to pad the input signal on both sides.\n    \"\"\"\n    super().__init__()\n\n    self.n_mel_channels = n_mel_channels\n    self.sampling_rate = sampling_rate\n    self.n_fft = filter_length\n    self.hop_size = hop_length\n    self.win_size = win_length\n    self.fmin = mel_fmin\n    self.fmax = mel_fmax\n    self.center = center\n\n    # Define the mel filterbank\n    mel = librosa.filters.mel(\n        sr=sampling_rate,\n        n_fft=filter_length,\n        n_mels=n_mel_channels,\n        fmin=mel_fmin,\n        fmax=mel_fmax,\n    )\n\n    mel_basis = torch.from_numpy(mel).float()\n\n    # Define the Hann window\n    hann_window = torch.hann_window(win_length)\n\n    self.register_buffer(\"mel_basis\", mel_basis)\n    self.register_buffer(\"hann_window\", hann_window)\n</code></pre>"},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT.dynamic_range_compression_torch","title":"<code>dynamic_range_compression_torch(x, C=1, clip_val=1e-05)</code>","text":"<p>Applies dynamic range compression to x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>C</code> <code>float</code> <p>Compression factor.</p> <code>1</code> <code>clip_val</code> <code>float</code> <p>Clipping value.</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>def dynamic_range_compression_torch(\n    self,\n    x: torch.Tensor,\n    C: int = 1,\n    clip_val: float = 1e-5,\n) -&gt; torch.Tensor:\n    r\"\"\"Applies dynamic range compression to x.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n        C (float): Compression factor.\n        clip_val (float): Clipping value.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n</code></pre>"},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT.forward","title":"<code>forward(y)</code>","text":"<p>Computes mel-spectrograms from a batch of waves.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>FloatTensor</code> <p>Input waveforms with shape (B, T) in range [-1, 1]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.FloatTensor: Spectrogram of shape (B, n_spech_channels, T)</p> <code>Tensor</code> <p>torch.FloatTensor: Mel-spectrogram of shape (B, n_mel_channels, T)</p> Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>def forward(self, y: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Computes mel-spectrograms from a batch of waves.\n\n    Args:\n        y (torch.FloatTensor): Input waveforms with shape (B, T) in range [-1, 1]\n\n    Returns:\n        torch.FloatTensor: Spectrogram of shape (B, n_spech_channels, T)\n        torch.FloatTensor: Mel-spectrogram of shape (B, n_mel_channels, T)\n    \"\"\"\n    spec = self._spectrogram(y)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n\n    mel = torch.matmul(self.mel_basis, spec)  # type: ignore\n    mel = self.spectral_normalize_torch(mel)\n\n    return spec, mel\n</code></pre>"},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT.linear_spectrogram","title":"<code>linear_spectrogram(y)</code>","text":"<p>Computes the linear spectrogram of a batch of waves.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>Input waveforms.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Linear spectrogram.</p> Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>def linear_spectrogram(self, y: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Computes the linear spectrogram of a batch of waves.\n\n    Args:\n        y (torch.Tensor): Input waveforms.\n\n    Returns:\n        torch.Tensor: Linear spectrogram.\n    \"\"\"\n    spec = self._spectrogram(y)\n    return torch.norm(spec, p=2, dim=-1)\n</code></pre>"},{"location":"training/preprocess/tacotron_stft/#training.preprocess.tacotron_stft.TacotronSTFT.spectral_normalize_torch","title":"<code>spectral_normalize_torch(magnitudes)</code>","text":"<p>Applies dynamic range compression to magnitudes.</p> <p>Parameters:</p> Name Type Description Default <code>magnitudes</code> <code>Tensor</code> <p>Input magnitudes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output magnitudes.</p> Source code in <code>training/preprocess/tacotron_stft.py</code> <pre><code>def spectral_normalize_torch(self, magnitudes: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"Applies dynamic range compression to magnitudes.\n\n    Args:\n        magnitudes (torch.Tensor): Input magnitudes.\n\n    Returns:\n        torch.Tensor: Output magnitudes.\n    \"\"\"\n    return self.dynamic_range_compression_torch(magnitudes)\n</code></pre>"},{"location":"training/preprocess/tokenizer_ipa/","title":"Tokenizer ipa","text":""},{"location":"training/preprocess/tokenizer_ipa/#training.preprocess.tokenizer_ipa.TokenizerIPA","title":"<code>TokenizerIPA</code>","text":"<p>DEPRECATED: TokenizerIPA is a class for tokenizing International Phonetic Alphabet (IPA) phonemes.</p> <p>Attributes:</p> Name Type Description <code>lang</code> <code>str</code> <p>Language to be used. Default is \"en\".</p> <code>phonemizer_checkpoint</code> <code>str</code> <p>Path to the phonemizer checkpoint file.</p> <code>phonemizer</code> <code>Phonemizer</code> <p>Phonemizer object for converting text to phonemes.</p> <code>tokenizer</code> <code>SequenceTokenizer</code> <p>SequenceTokenizer object for tokenizing the phonemes.</p> Source code in <code>training/preprocess/tokenizer_ipa.py</code> <pre><code>class TokenizerIPA:\n    r\"\"\"DEPRECATED: TokenizerIPA is a class for tokenizing International Phonetic Alphabet (IPA) phonemes.\n\n    Attributes:\n        lang (str): Language to be used. Default is \"en\".\n        phonemizer_checkpoint (str): Path to the phonemizer checkpoint file.\n        phonemizer (Phonemizer): Phonemizer object for converting text to phonemes.\n        tokenizer (SequenceTokenizer): SequenceTokenizer object for tokenizing the phonemes.\n    \"\"\"\n\n    def __init__(\n        self,\n        lang: str = \"en\",\n        phonemizer_checkpoint: str = \"checkpoints/en_us_cmudict_ipa_forward.pt\",\n    ):\n        r\"\"\"Initializes TokenizerIPA with the given language and phonemizer checkpoint.\n\n        Args:\n            lang (str): The language to be used. Default is \"en\".\n            phonemizer_checkpoint (str): The path to the phonemizer checkpoint file.\n        \"\"\"\n        lang_map = get_lang_map(lang)\n        self.lang = lang_map.phonemizer\n\n        self.phonemizer = Phonemizer.from_checkpoint(phonemizer_checkpoint)\n\n        phoneme_symbols = [\n            # IPA symbols\n            \"a\",\n            \"b\",\n            \"d\",\n            \"e\",\n            \"f\",\n            \"g\",\n            \"h\",\n            \"i\",\n            \"j\",\n            \"k\",\n            \"l\",\n            \"m\",\n            \"n\",\n            \"o\",\n            \"p\",\n            \"r\",\n            \"s\",\n            \"t\",\n            \"u\",\n            \"v\",\n            \"w\",\n            \"x\",\n            \"y\",\n            \"z\",\n            \"\u00e6\",\n            \"\u00e7\",\n            \"\u00f0\",\n            \"\u00f8\",\n            \"\u014b\",\n            \"\u0153\",\n            \"\u0250\",\n            \"\u0251\",\n            \"\u0254\",\n            \"\u0259\",\n            \"\u025b\",\n            \"\u025d\",\n            \"\u0279\",\n            \"\u0261\",\n            \"\u026a\",\n            \"\u0281\",\n            \"\u0283\",\n            \"\u028a\",\n            \"\u028c\",\n            \"\u028f\",\n            \"\u0292\",\n            \"\u0294\",\n            \"\u02c8\",\n            \"\u02cc\",\n            \"\u02d0\",\n            \"\u0303\",\n            \"\u030d\",\n            \"\u0325\",\n            \"\u0329\",\n            \"\u032f\",\n            \"\u0361\",\n            \"\u03b8\",\n            # Punctuation\n            \"!\",\n            \"?\",\n            \",\",\n            \".\",\n            \"-\",\n            \":\",\n            \";\",\n            '\"',\n            \"'\",\n            \"(\",\n            \")\",\n            \" \",\n        ]\n\n        self.tokenizer = SequenceTokenizer(\n            phoneme_symbols,\n            languages=[\"de\", \"en_us\"],\n            lowercase=True,\n            char_repeats=1,\n            append_start_end=True,\n        )\n\n        # test_text = \"Hello, World!\"\n        # print(\"Initializing TokenizerIPA, check on: \", test_text)\n\n        # phones_ipa = self.phonemizer(test_text, lang=self.lang)\n        # tokens = self.tokenizer(phones_ipa, language=self.lang)\n\n        # print(\"phones_ipa: \", phones_ipa)\n        # print(\"tokens: \", tokens)\n        # print(\"decoded: \", self.tokenizer.decode(tokens))\n\n    def __call__(self, text: str) -&gt; Tuple[Union[str, List[str]], List[int]]:\n        r\"\"\"Converts the input text to phonemes and tokenizes them.\n\n        Args:\n            text (str): The input text to be tokenized.\n\n        Returns:\n            list: The tokenized phonemes.\n\n        \"\"\"\n        phones_ipa = self.phonemizer(text, lang=self.lang)\n        tokens = self.tokenizer(phones_ipa, language=self.lang)\n\n        return phones_ipa, tokens\n</code></pre>"},{"location":"training/preprocess/tokenizer_ipa/#training.preprocess.tokenizer_ipa.TokenizerIPA.__call__","title":"<code>__call__(text)</code>","text":"<p>Converts the input text to phonemes and tokenizes them.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be tokenized.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>Tuple[Union[str, List[str]], List[int]]</code> <p>The tokenized phonemes.</p> Source code in <code>training/preprocess/tokenizer_ipa.py</code> <pre><code>def __call__(self, text: str) -&gt; Tuple[Union[str, List[str]], List[int]]:\n    r\"\"\"Converts the input text to phonemes and tokenizes them.\n\n    Args:\n        text (str): The input text to be tokenized.\n\n    Returns:\n        list: The tokenized phonemes.\n\n    \"\"\"\n    phones_ipa = self.phonemizer(text, lang=self.lang)\n    tokens = self.tokenizer(phones_ipa, language=self.lang)\n\n    return phones_ipa, tokens\n</code></pre>"},{"location":"training/preprocess/tokenizer_ipa/#training.preprocess.tokenizer_ipa.TokenizerIPA.__init__","title":"<code>__init__(lang='en', phonemizer_checkpoint='checkpoints/en_us_cmudict_ipa_forward.pt')</code>","text":"<p>Initializes TokenizerIPA with the given language and phonemizer checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language to be used. Default is \"en\".</p> <code>'en'</code> <code>phonemizer_checkpoint</code> <code>str</code> <p>The path to the phonemizer checkpoint file.</p> <code>'checkpoints/en_us_cmudict_ipa_forward.pt'</code> Source code in <code>training/preprocess/tokenizer_ipa.py</code> <pre><code>def __init__(\n    self,\n    lang: str = \"en\",\n    phonemizer_checkpoint: str = \"checkpoints/en_us_cmudict_ipa_forward.pt\",\n):\n    r\"\"\"Initializes TokenizerIPA with the given language and phonemizer checkpoint.\n\n    Args:\n        lang (str): The language to be used. Default is \"en\".\n        phonemizer_checkpoint (str): The path to the phonemizer checkpoint file.\n    \"\"\"\n    lang_map = get_lang_map(lang)\n    self.lang = lang_map.phonemizer\n\n    self.phonemizer = Phonemizer.from_checkpoint(phonemizer_checkpoint)\n\n    phoneme_symbols = [\n        # IPA symbols\n        \"a\",\n        \"b\",\n        \"d\",\n        \"e\",\n        \"f\",\n        \"g\",\n        \"h\",\n        \"i\",\n        \"j\",\n        \"k\",\n        \"l\",\n        \"m\",\n        \"n\",\n        \"o\",\n        \"p\",\n        \"r\",\n        \"s\",\n        \"t\",\n        \"u\",\n        \"v\",\n        \"w\",\n        \"x\",\n        \"y\",\n        \"z\",\n        \"\u00e6\",\n        \"\u00e7\",\n        \"\u00f0\",\n        \"\u00f8\",\n        \"\u014b\",\n        \"\u0153\",\n        \"\u0250\",\n        \"\u0251\",\n        \"\u0254\",\n        \"\u0259\",\n        \"\u025b\",\n        \"\u025d\",\n        \"\u0279\",\n        \"\u0261\",\n        \"\u026a\",\n        \"\u0281\",\n        \"\u0283\",\n        \"\u028a\",\n        \"\u028c\",\n        \"\u028f\",\n        \"\u0292\",\n        \"\u0294\",\n        \"\u02c8\",\n        \"\u02cc\",\n        \"\u02d0\",\n        \"\u0303\",\n        \"\u030d\",\n        \"\u0325\",\n        \"\u0329\",\n        \"\u032f\",\n        \"\u0361\",\n        \"\u03b8\",\n        # Punctuation\n        \"!\",\n        \"?\",\n        \",\",\n        \".\",\n        \"-\",\n        \":\",\n        \";\",\n        '\"',\n        \"'\",\n        \"(\",\n        \")\",\n        \" \",\n    ]\n\n    self.tokenizer = SequenceTokenizer(\n        phoneme_symbols,\n        languages=[\"de\", \"en_us\"],\n        lowercase=True,\n        char_repeats=1,\n        append_start_end=True,\n    )\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/","title":"Wav2Vec Aligner","text":""},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Item","title":"<code>Item</code>  <code>dataclass</code>","text":"<p>A data class that represents an item with a sentence, a path to a wav file, and an output path.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>@dataclass\nclass Item:\n    r\"\"\"A data class that represents an item with a sentence,\n    a path to a wav file, and an output path.\n    \"\"\"\n\n    sent: str\n    wav_path: str\n    out_path: str\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Point","title":"<code>Point</code>  <code>dataclass</code>","text":"<p>A data class that represents a point with a token index, a time index, and a score.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>@dataclass\nclass Point:\n    r\"\"\"A data class that represents a point with a token index,\n    a time index, and a score.\n    \"\"\"\n\n    token_index: int\n    time_index: int\n    score: float\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Segment","title":"<code>Segment</code>  <code>dataclass</code>","text":"<p>A data class that represents a segment with a label, a start time, an end time, a duration, and a score.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>@dataclass\nclass Segment:\n    r\"\"\"A data class that represents a segment with a label,\n    a start time, an end time, a duration, and a score.\n    \"\"\"\n\n    label: str\n    start: int\n    end: int\n    # TODO: check that the scale of duration is correct...\n    duration: float\n    score: float\n\n    @property\n    def length(self):\n        return self.end - self.start\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner","title":"<code>Wav2VecAligner</code>","text":"<p>             Bases: <code>Module</code></p> <p>Wav2VecAligner model.</p> <p>The Wav2VecAligner model is designed for aligning audio data with text data. This class handles the training and validation of the Wav2VecAligner model.</p> <p>Attributes     config (AutoConfig): The configuration for the pre-trained model.     model (AutoModelForCTC): The pre-trained model.     processor (AutoProcessor): The processor for the pre-trained model.     labels (List): The labels from the vocabulary of the tokenizer.     blank_id (int): The ID of the blank token.</p> <p>Methods     load_audio: Load an audio file from the specified path.     encode: Encode the labels.     decode: Decode the tokens.     align_single_sample: Align a single sample of audio data with the corresponding text.     get_trellis: Build a trellis matrix that represents the probabilities of each source token being at a certain time step.     backtrack: Walk backwards from the last     merge_repeats: Merge repeated tokens into a single segment.     merge_words: Merge words in the given path.     forward: Perform the forward pass of the model, which involves loading the audio data, aligning the audio with the text, building the trellis, backtracking to find the optimal path, merging repeated tokens, and finally merging words.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>class Wav2VecAligner(Module):\n    r\"\"\"Wav2VecAligner model.\n\n    The Wav2VecAligner model is designed for aligning audio data with text data.\n    This class handles the training and validation of the Wav2VecAligner model.\n\n    Attributes\n        config (AutoConfig): The configuration for the pre-trained model.\n        model (AutoModelForCTC): The pre-trained model.\n        processor (AutoProcessor): The processor for the pre-trained model.\n        labels (List): The labels from the vocabulary of the tokenizer.\n        blank_id (int): The ID of the blank token.\n\n    Methods\n        load_audio: Load an audio file from the specified path.\n        encode: Encode the labels.\n        decode: Decode the tokens.\n        align_single_sample: Align a single sample of audio data with the corresponding text.\n        get_trellis: Build a trellis matrix that represents the probabilities of each source token being at a certain time step.\n        backtrack: Walk backwards from the last\n        merge_repeats: Merge repeated tokens into a single segment.\n        merge_words: Merge words in the given path.\n        forward: Perform the forward pass of the model, which involves loading the audio data, aligning the audio with the text, building the trellis, backtracking to find the optimal path, merging repeated tokens, and finally merging words.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"facebook/wav2vec2-base-960h\",\n    ):\n        r\"\"\"Initialize a new instance of the Wav2VecAligner class.\n\n        Args:\n            model_name (str): The name of the pre-trained model to use. Defaults to \"facebook/wav2vec2-base-960h\".\n        \"\"\"\n        super().__init__()\n\n        # Load the config\n        self.config = AutoConfig.from_pretrained(model_name)\n\n        self.model = AutoModelForCTC.from_pretrained(model_name)\n        self.model.eval()\n\n        self.processor = AutoProcessor.from_pretrained(model_name)\n\n        # get labels from vocab\n        self.labels = list(self.processor.tokenizer.get_vocab().keys())\n\n        for i in range(len(self.labels)):\n            if self.labels[i] == \"[PAD]\" or self.labels[i] == \"&lt;pad&gt;\":\n                self.blank_id = i\n\n        print(\"Blank Token id [PAD]/&lt;pad&gt;\", self.blank_id)\n\n    def load_audio(self, wav_path: str) -&gt; Tuple[torch.Tensor, int]:\n        r\"\"\"Load an audio file from the specified path.\n\n        Args:\n            wav_path (str): The path to the audio file.\n\n        Returns:\n            Tuple[torch.Tensor, int]: A tuple containing the loaded audio data and the sample rate, or a FileNotFoundError if the file does not exist.\n        \"\"\"\n        if not os.path.isfile(wav_path):\n            raise FileNotFoundError(wav_path, \"Not found in wavs directory\")\n\n        speech_array, sampling_rate = torchaudio.load(wav_path) # type: ignore\n        return speech_array, sampling_rate\n\n    def encode(self, text: str) -&gt; List:\n        # encode labels\n        with self.processor.as_target_processor():\n            return self.processor(text, return_tensors=\"pt\").input_ids\n\n    def decode(self, tokens: List):\n        # Decode tokens\n        decoded = self.processor.batch_decode(tokens)\n        return decoded[0]\n\n    def align_single_sample(\n        self, audio_input: torch.Tensor, text: str,\n    ) -&gt; Tuple[torch.Tensor, List, str]:\n        r\"\"\"Align a single sample of audio data with the corresponding text.\n\n        Args:\n            audio_input (torch.Tensor): The audio data.\n            text (str): The corresponding text.\n\n        Returns:\n            Tuple[torch.Tensor, List, str]: A tuple containing the emissions, the tokens, and the transcript.\n        \"\"\"\n        transcript = \"|\".join(text.split(\" \"))\n        transcript = f\"|{transcript}|\"\n\n        with torch.inference_mode():\n            logits = self.model(audio_input).logits\n\n        # Get the emission probability at frame level\n        # Compute the probability in log-domain to avoid numerical instability\n        # For this purpose, we normalize the emission with `torch.log_softmax()`\n        emissions = torch.log_softmax(logits, dim=-1)\n        emissions = emissions[0]\n\n        tokens = self.encode(transcript)[0]\n\n        return emissions, tokens, transcript\n\n    def get_trellis(\n        self,\n        emission: torch.Tensor,\n        tokens: List,\n    ) -&gt; torch.Tensor:\n        r\"\"\"Build a trellis matrix of shape (num_frames + 1, num_tokens + 1)\n        that represents the probabilities of each source token being at a certain time step.\n\n        Since we are looking for the most likely transitions, we take the more likely path for the value of $k_{(t+1,j+1)}$, that is:\n\n        $k_{t+1, j+1} = \\max(k_{t, j} p_{t+1, c_{j+1}}, k_{t, j+1} p_{t+1, \\text{repeat}})$\n\n        Args:\n            emission (torch.Tensor): The emission tensor.\n            tokens (List): The list of tokens.\n\n        Returns:\n            torch.Tensor: The trellis matrix.\n        \"\"\"\n        num_frames = emission.size(0)\n        num_tokens = len(tokens)\n\n        # Trellis has extra diemsions for both time axis and tokens.\n        # The extra dim for tokens represents &lt;SoS&gt; (start-of-sentence)\n        # The extra dim for time axis is for simplification of the code.\n\n        trellis = torch.zeros((num_frames, num_tokens))\n        trellis[1:, 0] = torch.cumsum(emission[1:, self.blank_id], 0)\n        trellis[0, 1:] = -float(\"inf\")\n        trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n\n        for t in range(num_frames - 1):\n            trellis[t + 1, 1:] = torch.maximum(\n                # Score for staying at the same token\n                trellis[t, 1:] + emission[t, self.blank_id],\n                # Score for changing to the next token\n                trellis[t, :-1] + emission[t, tokens[1:]],\n            )\n        return trellis\n\n    def backtrack(\n        self,\n        trellis: torch.Tensor,\n        emission: torch.Tensor,\n        tokens: List,\n    ) -&gt; List[Point]:\n        r\"\"\"Walk backwards from the last (sentence_token, time_step) pair to build the optimal sequence alignment path.\n\n        Args:\n            trellis (torch.Tensor): The trellis matrix.\n            emission (torch.Tensor): The emission tensor.\n            tokens (List): The list of tokens.\n\n        Returns:\n            List[Point]: The optimal sequence alignment path.\n        \"\"\"\n        # Note:\n        # j and t are indices for trellis, which has extra dimensions\n        # for time and tokens at the beginning.\n        # When referring to time frame index `T` in trellis,\n        # the corresponding index in emission is `T-1`.\n        # Similarly, when referring to token index `J` in trellis,\n        # the corresponding index in transcript is `J-1`.\n        t, j = trellis.size(0) - 1, trellis.size(1) - 1\n\n        path = [Point(j, t, emission[t, self.blank_id].exp().item())]\n        while j &gt; 0:\n            # Should not happen but just in case\n            assert t &gt; 0\n\n            # 1. Figure out if the current position was stay or change\n            # Frame-wise score of stay vs change\n            p_stay = emission[t - 1, self.blank_id]\n            p_change = emission[t - 1, tokens[j]]\n\n            # Context-aware score for stay vs change\n            stayed = trellis[t - 1, j] + p_stay\n            changed = trellis[t - 1, j - 1] + p_change\n\n            # Update position\n            t -= 1\n            if changed &gt; stayed:\n                j -= 1\n\n            # Store the path with frame-wise probability.\n            prob = (p_change if changed &gt; stayed else p_stay).exp().item()\n            path.append(Point(j, t, prob))\n\n        # Now j == 0, which means, it reached the SoS.\n        # Fill up the rest for the sake of visualization\n        while t &gt; 0:\n            prob = emission[t - 1, self.blank_id].exp().item()\n            path.append(Point(j, t - 1, prob))\n            t -= 1\n\n        return path[::-1]\n\n    def merge_repeats(self, path: List[Point], transcript: str) -&gt; List[Segment]:\n        r\"\"\"Merge repeated tokens into a single segment.\n\n        Args:\n            path (List[Point]): The sequence alignment path.\n            transcript (str): The transcript.\n\n        Returns:\n            List[Segment]: The list of segments.\n\n        Note: this shouldn't affect repeated characters from the\n        original sentences (e.g. `ll` in `hello`)\n        \"\"\"\n        i1, i2 = 0, 0\n        segments = []\n        while i1 &lt; len(path):\n            while i2 &lt; len(path) and path[i1].token_index == path[i2].token_index:\n                i2 += 1\n            score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n\n            x0, x1 = path[i1].time_index, path[i2 - 1].time_index + 1\n            duration = x1 - x0\n\n            segments.append(\n                Segment(\n                    transcript[path[i1].token_index],\n                    x0,\n                    x1,\n                    duration,\n                    score,\n                ),\n            )\n            i1 = i2\n        return segments\n\n    # Merge words\n    def merge_words(\n        self, segments: List[Segment], separator: str = \"|\",\n    ) -&gt; List[Segment]:\n        r\"\"\"Merge words in the given path.\n\n        Args:\n            segments (List[Segment]): The list of segments.\n            separator (str): The separator character. Defaults to \"|\".\n\n        Returns:\n            List[Segment]: The list of merged words.\n        \"\"\"\n        words = []\n        i1, i2 = 0, 0\n        while i1 &lt; len(segments):\n            if i2 &gt;= len(segments) or segments[i2].label == separator:\n                if i1 != i2:\n                    segs = segments[i1:i2]\n                    word = \"\".join([seg.label for seg in segs])\n                    score = sum(seg.score * seg.length for seg in segs) / sum(\n                        seg.length for seg in segs\n                    )\n\n                    x0, x1 = segments[i1].start, segments[i2 - 1].end\n                    duration = x1 - x0\n\n                    words.append(Segment(word, x0, x1, duration, score))\n                i1 = i2 + 1\n                i2 = i1\n            else:\n                i2 += 1\n        return words\n\n    def forward(self, wav_path: str, text: str) -&gt; List[Segment]:\n        r\"\"\"Perform the forward pass of the model, which involves loading the audio data, aligning the audio with the text,\n        building the trellis, backtracking to find the optimal path, merging repeated tokens, and finally merging words.\n\n        Args:\n            wav_path (str): The path to the audio file.\n            text (str): The corresponding text.\n\n        Returns:\n            List[Segment]: The list of segments representing the alignment of the audio data with the text.\n        \"\"\"\n        audio_input, _ = self.load_audio(wav_path)\n\n        emissions, tokens, transcript = self.align_single_sample(audio_input, text)\n\n        trellis = self.get_trellis(emissions, tokens)\n\n        path = self.backtrack(trellis, emissions, tokens)\n\n        merged_path = self.merge_repeats(path, transcript)\n\n        return self.merge_words(merged_path)\n\n    def save_segments(self, wav_path: str, text: str, save_dir: str):\n        r\"\"\"Perform the forward pass of the model to get the segments and save each segment to a file.\n        Used for debugging purposes.\n\n        Args:\n            wav_path (str): The path to the audio file.\n            text (str): The corresponding text.\n            save_dir (str): The directory where the audio files should be saved.\n\n        Returns:\n            None\n        \"\"\"\n        word_segments = self.forward(wav_path, text)\n\n        waveform, sampling_rate = self.load_audio(wav_path)\n\n        emissions, tokens, _ = self.align_single_sample(waveform, text)\n\n        trellis = self.get_trellis(emissions, tokens)\n\n        ratio = waveform.size(1) / trellis.size(0)\n\n        for i, word in enumerate(word_segments):\n            x0 = int(ratio * word.start)\n            x1 = int(ratio * word.end)\n\n            print(\n                f\"{word.label} ({word.score:.2f}): {x0 / sampling_rate:.3f} - {x1 / sampling_rate:.3f} sec\",\n            )\n\n            segment_waveform = waveform[:, x0:x1]\n\n            # Save the segment waveform to a file\n            filename = f\"{i}_{word.label}.wav\"\n            torchaudio.save( # type: ignore\n                os.path.join(save_dir, filename), segment_waveform, sampling_rate,\n            )\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.__init__","title":"<code>__init__(model_name='facebook/wav2vec2-base-960h')</code>","text":"<p>Initialize a new instance of the Wav2VecAligner class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained model to use. Defaults to \"facebook/wav2vec2-base-960h\".</p> <code>'facebook/wav2vec2-base-960h'</code> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"facebook/wav2vec2-base-960h\",\n):\n    r\"\"\"Initialize a new instance of the Wav2VecAligner class.\n\n    Args:\n        model_name (str): The name of the pre-trained model to use. Defaults to \"facebook/wav2vec2-base-960h\".\n    \"\"\"\n    super().__init__()\n\n    # Load the config\n    self.config = AutoConfig.from_pretrained(model_name)\n\n    self.model = AutoModelForCTC.from_pretrained(model_name)\n    self.model.eval()\n\n    self.processor = AutoProcessor.from_pretrained(model_name)\n\n    # get labels from vocab\n    self.labels = list(self.processor.tokenizer.get_vocab().keys())\n\n    for i in range(len(self.labels)):\n        if self.labels[i] == \"[PAD]\" or self.labels[i] == \"&lt;pad&gt;\":\n            self.blank_id = i\n\n    print(\"Blank Token id [PAD]/&lt;pad&gt;\", self.blank_id)\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.align_single_sample","title":"<code>align_single_sample(audio_input, text)</code>","text":"<p>Align a single sample of audio data with the corresponding text.</p> <p>Parameters:</p> Name Type Description Default <code>audio_input</code> <code>Tensor</code> <p>The audio data.</p> required <code>text</code> <code>str</code> <p>The corresponding text.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, List, str]</code> <p>Tuple[torch.Tensor, List, str]: A tuple containing the emissions, the tokens, and the transcript.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def align_single_sample(\n    self, audio_input: torch.Tensor, text: str,\n) -&gt; Tuple[torch.Tensor, List, str]:\n    r\"\"\"Align a single sample of audio data with the corresponding text.\n\n    Args:\n        audio_input (torch.Tensor): The audio data.\n        text (str): The corresponding text.\n\n    Returns:\n        Tuple[torch.Tensor, List, str]: A tuple containing the emissions, the tokens, and the transcript.\n    \"\"\"\n    transcript = \"|\".join(text.split(\" \"))\n    transcript = f\"|{transcript}|\"\n\n    with torch.inference_mode():\n        logits = self.model(audio_input).logits\n\n    # Get the emission probability at frame level\n    # Compute the probability in log-domain to avoid numerical instability\n    # For this purpose, we normalize the emission with `torch.log_softmax()`\n    emissions = torch.log_softmax(logits, dim=-1)\n    emissions = emissions[0]\n\n    tokens = self.encode(transcript)[0]\n\n    return emissions, tokens, transcript\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.backtrack","title":"<code>backtrack(trellis, emission, tokens)</code>","text":"<p>Walk backwards from the last (sentence_token, time_step) pair to build the optimal sequence alignment path.</p> <p>Parameters:</p> Name Type Description Default <code>trellis</code> <code>Tensor</code> <p>The trellis matrix.</p> required <code>emission</code> <code>Tensor</code> <p>The emission tensor.</p> required <code>tokens</code> <code>List</code> <p>The list of tokens.</p> required <p>Returns:</p> Type Description <code>List[Point]</code> <p>List[Point]: The optimal sequence alignment path.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def backtrack(\n    self,\n    trellis: torch.Tensor,\n    emission: torch.Tensor,\n    tokens: List,\n) -&gt; List[Point]:\n    r\"\"\"Walk backwards from the last (sentence_token, time_step) pair to build the optimal sequence alignment path.\n\n    Args:\n        trellis (torch.Tensor): The trellis matrix.\n        emission (torch.Tensor): The emission tensor.\n        tokens (List): The list of tokens.\n\n    Returns:\n        List[Point]: The optimal sequence alignment path.\n    \"\"\"\n    # Note:\n    # j and t are indices for trellis, which has extra dimensions\n    # for time and tokens at the beginning.\n    # When referring to time frame index `T` in trellis,\n    # the corresponding index in emission is `T-1`.\n    # Similarly, when referring to token index `J` in trellis,\n    # the corresponding index in transcript is `J-1`.\n    t, j = trellis.size(0) - 1, trellis.size(1) - 1\n\n    path = [Point(j, t, emission[t, self.blank_id].exp().item())]\n    while j &gt; 0:\n        # Should not happen but just in case\n        assert t &gt; 0\n\n        # 1. Figure out if the current position was stay or change\n        # Frame-wise score of stay vs change\n        p_stay = emission[t - 1, self.blank_id]\n        p_change = emission[t - 1, tokens[j]]\n\n        # Context-aware score for stay vs change\n        stayed = trellis[t - 1, j] + p_stay\n        changed = trellis[t - 1, j - 1] + p_change\n\n        # Update position\n        t -= 1\n        if changed &gt; stayed:\n            j -= 1\n\n        # Store the path with frame-wise probability.\n        prob = (p_change if changed &gt; stayed else p_stay).exp().item()\n        path.append(Point(j, t, prob))\n\n    # Now j == 0, which means, it reached the SoS.\n    # Fill up the rest for the sake of visualization\n    while t &gt; 0:\n        prob = emission[t - 1, self.blank_id].exp().item()\n        path.append(Point(j, t - 1, prob))\n        t -= 1\n\n    return path[::-1]\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.forward","title":"<code>forward(wav_path, text)</code>","text":"<p>Perform the forward pass of the model, which involves loading the audio data, aligning the audio with the text, building the trellis, backtracking to find the optimal path, merging repeated tokens, and finally merging words.</p> <p>Parameters:</p> Name Type Description Default <code>wav_path</code> <code>str</code> <p>The path to the audio file.</p> required <code>text</code> <code>str</code> <p>The corresponding text.</p> required <p>Returns:</p> Type Description <code>List[Segment]</code> <p>List[Segment]: The list of segments representing the alignment of the audio data with the text.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def forward(self, wav_path: str, text: str) -&gt; List[Segment]:\n    r\"\"\"Perform the forward pass of the model, which involves loading the audio data, aligning the audio with the text,\n    building the trellis, backtracking to find the optimal path, merging repeated tokens, and finally merging words.\n\n    Args:\n        wav_path (str): The path to the audio file.\n        text (str): The corresponding text.\n\n    Returns:\n        List[Segment]: The list of segments representing the alignment of the audio data with the text.\n    \"\"\"\n    audio_input, _ = self.load_audio(wav_path)\n\n    emissions, tokens, transcript = self.align_single_sample(audio_input, text)\n\n    trellis = self.get_trellis(emissions, tokens)\n\n    path = self.backtrack(trellis, emissions, tokens)\n\n    merged_path = self.merge_repeats(path, transcript)\n\n    return self.merge_words(merged_path)\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.get_trellis","title":"<code>get_trellis(emission, tokens)</code>","text":"<p>Build a trellis matrix of shape (num_frames + 1, num_tokens + 1) that represents the probabilities of each source token being at a certain time step.</p> <p>Since we are looking for the most likely transitions, we take the more likely path for the value of \\(k_{(t+1,j+1)}\\), that is:</p> <p>\\(k_{t+1, j+1} = \\max(k_{t, j} p_{t+1, c_{j+1}}, k_{t, j+1} p_{t+1, \\text{repeat}})\\)</p> <p>Parameters:</p> Name Type Description Default <code>emission</code> <code>Tensor</code> <p>The emission tensor.</p> required <code>tokens</code> <code>List</code> <p>The list of tokens.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The trellis matrix.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def get_trellis(\n    self,\n    emission: torch.Tensor,\n    tokens: List,\n) -&gt; torch.Tensor:\n    r\"\"\"Build a trellis matrix of shape (num_frames + 1, num_tokens + 1)\n    that represents the probabilities of each source token being at a certain time step.\n\n    Since we are looking for the most likely transitions, we take the more likely path for the value of $k_{(t+1,j+1)}$, that is:\n\n    $k_{t+1, j+1} = \\max(k_{t, j} p_{t+1, c_{j+1}}, k_{t, j+1} p_{t+1, \\text{repeat}})$\n\n    Args:\n        emission (torch.Tensor): The emission tensor.\n        tokens (List): The list of tokens.\n\n    Returns:\n        torch.Tensor: The trellis matrix.\n    \"\"\"\n    num_frames = emission.size(0)\n    num_tokens = len(tokens)\n\n    # Trellis has extra diemsions for both time axis and tokens.\n    # The extra dim for tokens represents &lt;SoS&gt; (start-of-sentence)\n    # The extra dim for time axis is for simplification of the code.\n\n    trellis = torch.zeros((num_frames, num_tokens))\n    trellis[1:, 0] = torch.cumsum(emission[1:, self.blank_id], 0)\n    trellis[0, 1:] = -float(\"inf\")\n    trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n\n    for t in range(num_frames - 1):\n        trellis[t + 1, 1:] = torch.maximum(\n            # Score for staying at the same token\n            trellis[t, 1:] + emission[t, self.blank_id],\n            # Score for changing to the next token\n            trellis[t, :-1] + emission[t, tokens[1:]],\n        )\n    return trellis\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.load_audio","title":"<code>load_audio(wav_path)</code>","text":"<p>Load an audio file from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>wav_path</code> <code>str</code> <p>The path to the audio file.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, int]</code> <p>Tuple[torch.Tensor, int]: A tuple containing the loaded audio data and the sample rate, or a FileNotFoundError if the file does not exist.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def load_audio(self, wav_path: str) -&gt; Tuple[torch.Tensor, int]:\n    r\"\"\"Load an audio file from the specified path.\n\n    Args:\n        wav_path (str): The path to the audio file.\n\n    Returns:\n        Tuple[torch.Tensor, int]: A tuple containing the loaded audio data and the sample rate, or a FileNotFoundError if the file does not exist.\n    \"\"\"\n    if not os.path.isfile(wav_path):\n        raise FileNotFoundError(wav_path, \"Not found in wavs directory\")\n\n    speech_array, sampling_rate = torchaudio.load(wav_path) # type: ignore\n    return speech_array, sampling_rate\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.merge_repeats","title":"<code>merge_repeats(path, transcript)</code>","text":"<p>Merge repeated tokens into a single segment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>List[Point]</code> <p>The sequence alignment path.</p> required <code>transcript</code> <code>str</code> <p>The transcript.</p> required <p>Returns:</p> Type Description <code>List[Segment]</code> <p>List[Segment]: The list of segments.</p> <p>Note: this shouldn't affect repeated characters from the original sentences (e.g. <code>ll</code> in <code>hello</code>)</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def merge_repeats(self, path: List[Point], transcript: str) -&gt; List[Segment]:\n    r\"\"\"Merge repeated tokens into a single segment.\n\n    Args:\n        path (List[Point]): The sequence alignment path.\n        transcript (str): The transcript.\n\n    Returns:\n        List[Segment]: The list of segments.\n\n    Note: this shouldn't affect repeated characters from the\n    original sentences (e.g. `ll` in `hello`)\n    \"\"\"\n    i1, i2 = 0, 0\n    segments = []\n    while i1 &lt; len(path):\n        while i2 &lt; len(path) and path[i1].token_index == path[i2].token_index:\n            i2 += 1\n        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n\n        x0, x1 = path[i1].time_index, path[i2 - 1].time_index + 1\n        duration = x1 - x0\n\n        segments.append(\n            Segment(\n                transcript[path[i1].token_index],\n                x0,\n                x1,\n                duration,\n                score,\n            ),\n        )\n        i1 = i2\n    return segments\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.merge_words","title":"<code>merge_words(segments, separator='|')</code>","text":"<p>Merge words in the given path.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[Segment]</code> <p>The list of segments.</p> required <code>separator</code> <code>str</code> <p>The separator character. Defaults to \"|\".</p> <code>'|'</code> <p>Returns:</p> Type Description <code>List[Segment]</code> <p>List[Segment]: The list of merged words.</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def merge_words(\n    self, segments: List[Segment], separator: str = \"|\",\n) -&gt; List[Segment]:\n    r\"\"\"Merge words in the given path.\n\n    Args:\n        segments (List[Segment]): The list of segments.\n        separator (str): The separator character. Defaults to \"|\".\n\n    Returns:\n        List[Segment]: The list of merged words.\n    \"\"\"\n    words = []\n    i1, i2 = 0, 0\n    while i1 &lt; len(segments):\n        if i2 &gt;= len(segments) or segments[i2].label == separator:\n            if i1 != i2:\n                segs = segments[i1:i2]\n                word = \"\".join([seg.label for seg in segs])\n                score = sum(seg.score * seg.length for seg in segs) / sum(\n                    seg.length for seg in segs\n                )\n\n                x0, x1 = segments[i1].start, segments[i2 - 1].end\n                duration = x1 - x0\n\n                words.append(Segment(word, x0, x1, duration, score))\n            i1 = i2 + 1\n            i2 = i1\n        else:\n            i2 += 1\n    return words\n</code></pre>"},{"location":"training/preprocess/wav2vec_aligner/#training.preprocess.wav2vec_aligner.Wav2VecAligner.save_segments","title":"<code>save_segments(wav_path, text, save_dir)</code>","text":"<p>Perform the forward pass of the model to get the segments and save each segment to a file. Used for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>wav_path</code> <code>str</code> <p>The path to the audio file.</p> required <code>text</code> <code>str</code> <p>The corresponding text.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the audio files should be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>training/preprocess/wav2vec_aligner.py</code> <pre><code>def save_segments(self, wav_path: str, text: str, save_dir: str):\n    r\"\"\"Perform the forward pass of the model to get the segments and save each segment to a file.\n    Used for debugging purposes.\n\n    Args:\n        wav_path (str): The path to the audio file.\n        text (str): The corresponding text.\n        save_dir (str): The directory where the audio files should be saved.\n\n    Returns:\n        None\n    \"\"\"\n    word_segments = self.forward(wav_path, text)\n\n    waveform, sampling_rate = self.load_audio(wav_path)\n\n    emissions, tokens, _ = self.align_single_sample(waveform, text)\n\n    trellis = self.get_trellis(emissions, tokens)\n\n    ratio = waveform.size(1) / trellis.size(0)\n\n    for i, word in enumerate(word_segments):\n        x0 = int(ratio * word.start)\n        x1 = int(ratio * word.end)\n\n        print(\n            f\"{word.label} ({word.score:.2f}): {x0 / sampling_rate:.3f} - {x1 / sampling_rate:.3f} sec\",\n        )\n\n        segment_waveform = waveform[:, x0:x1]\n\n        # Save the segment waveform to a file\n        filename = f\"{i}_{word.label}.wav\"\n        torchaudio.save( # type: ignore\n            os.path.join(save_dir, filename), segment_waveform, sampling_rate,\n        )\n</code></pre>"}]}